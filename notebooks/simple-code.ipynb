{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fDOzpVnIuY-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: C:\\Users\\notin\\Bahar\\llm\\AIED\\data\\trans_df.csv\n"
     ]
    }
   ],
   "source": [
    "# change data path as you need\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"C:\\\\Users\\\\notin\\\\Bahar\\\\llm\\\\AIED\\\\data\"\n",
    "RESULTS_PATH = \"C:\\\\Users\\\\notin\\\\Bahar\\\\llm\\\\AIED\\\\results\"\n",
    "\n",
    "data_file_name = \"trans_df.csv\"\n",
    "data_path = os.path.join(DATA_PATH, data_file_name)\n",
    "print(f\"Loading data from: {data_path}\")\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_2022 = df[df['year'] == 2022]\n",
    "df_2022.drop(columns=['year', \"audio_file_path\", \"session\", \"audio_file_name\"], inplace=True)\n",
    "print(\"Number of records in 2022:\", len(df_2022))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do for later runs\n",
    "* [ ] fix logic: if all agents agree do not go to the next round\n",
    "* [ ] fix prompt:\n",
    "    - CODEBOOK needs more cotext, agents seems to be confused about whta other categoy means\n",
    "    - Reduce emphasis on short setences \n",
    "    - Emphasis on returning None only when needed\n",
    "* [ ] Currently if llm resp is not formatted correctly after <max_tries>, code returns the heuristic label instead of the llm resp. The problem is that it happens alot. Possible fix is  to return the raw resp as well with heuristic label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RETRIES = 2\n",
    "BATCH_NUM = 10\n",
    "MAX_NEW_TOKENS = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4w-rod8UV-u"
   },
   "source": [
    "# Run this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3otIZ5BaCZla"
   },
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1768977189693,
     "user": {
      "displayName": "Bahar Shahrokhian",
      "userId": "02701353148338583964"
     },
     "user_tz": 420
    },
    "id": "ti7iJ7XODP5i",
    "outputId": "7fc22386-1066-4092-f3fd-12b79241718c"
   },
   "outputs": [],
   "source": [
    "import os, re, json, time, ast\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7QVvwPoH6NC"
   },
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "# del model   # if available\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDZJAvhvs9Pb"
   },
   "outputs": [],
   "source": [
    "CAD_CODEBOOK_DICT = {\n",
    "    \"WCT\": \"The teacher is addressing the whole class.\",\n",
    "    \"GT\":  \"The teacher is addressing a group or a student in a group. It also includes any talk: student level\",\n",
    "    \"Other\": \"The teacher isnâ€™t talking to the whole class or any groups or students. Either sheâ€™s silent or talking to herself or a visitor in a non-distracting way: \"\n",
    "}\n",
    "output_example = { \"code\": \"WCT\", \"reasoning\": \"it is more likly that the teacher is addressing the whole class\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1768977190198,
     "user": {
      "displayName": "Bahar Shahrokhian",
      "userId": "02701353148338583964"
     },
     "user_tz": 420
    },
    "id": "aXvcOzC0PzEH",
    "outputId": "60248857-fdc5-4c00-eb33-8b0aaf38de54"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Get the root logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Remove any existing handlers to prevent duplicate messages or conflicting configurations\n",
    "# This is important if basicConfig was called before or if the cell is run multiple times\n",
    "for handler in logger.handlers[:]: # Iterate over a copy of the list\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "# Create a StreamHandler that prints to standard output\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "\n",
    "# Define a formatter for the log messages\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "# Add the handler to the logger\n",
    "logger.addHandler(handler)\n",
    "\n",
    "logger.info(\"This is an info message from the explicitly configured logger.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BvwholfAQ3Xb"
   },
   "outputs": [],
   "source": [
    "class AnnotationResult:\n",
    "    \"\"\"Result from annotating a single text.\"\"\"\n",
    "    success: bool\n",
    "    code: Optional[str] = None\n",
    "    rationale: Optional[str] = None\n",
    "    raw_output: Optional[str] = None\n",
    "    error: Optional[str] = None\n",
    "    text: Optional[str] = None\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        if self.success:\n",
    "            return f\"âœ“ {self.code}: {self.rationale}\"\n",
    "        return f\"âœ— Error: {self.error}\"\n",
    "\n",
    "    @property\n",
    "    def parsed(self) -> Dict[str, str]:\n",
    "        \"\"\"Get parsed result in legacy format.\"\"\"\n",
    "        return {\n",
    "            \"CAD-code\": self.code or \"NONE\",\n",
    "            \"rationale\": self.rationale or \"\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lbo5AF4GJLmf"
   },
   "source": [
    "## Model/gpu manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NEW_TOKENS = 512\n",
    "TEMPERATURE = 0.4\n",
    "TOP_K = 40\n",
    "CPU_MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "GPU_MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "\n",
    "# ============================================================================\n",
    "# MEMORY UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "@staticmethod\n",
    "def clear_cache():\n",
    "    \"\"\"Clears GPU cache and runs garbage collection.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    gc.collect()\n",
    "\n",
    "@staticmethod\n",
    "def get_memory_info() -> Dict[str, float]:\n",
    "    \"\"\"Returns current GPU memory usage in GB.\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return {}\n",
    "\n",
    "    device = torch.cuda.current_device()\n",
    "    allocated = torch.cuda.memory_allocated(device) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(device) / 1024**3\n",
    "    total = torch.cuda.get_device_properties(device).total_memory / 1024**3\n",
    "    free = total - allocated\n",
    "\n",
    "    return {\n",
    "        \"allocated_gb\": allocated,\n",
    "        \"reserved_gb\": reserved,\n",
    "        \"total_gb\": total,\n",
    "        \"free_gb\": free\n",
    "    }\n",
    "\n",
    "@staticmethod\n",
    "def log_memory_usage():\n",
    "    \"\"\"Logs current memory usage.\"\"\"\n",
    "    info = get_memory_info()\n",
    "    if info:\n",
    "        logger.debug(\n",
    "            f\"GPU Memory - Allocated: {info['allocated_gb']:.2f}GB, \"\n",
    "            f\"Free: {info['free_gb']:.2f}GB, Total: {info['total_gb']:.2f}GB\"\n",
    "        )\n",
    "\n",
    "@staticmethod\n",
    "def setup_memory_optimization():\n",
    "    \"\"\"Sets up environment variables for better memory management.\"\"\"\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFyS2ftqJLX6"
   },
   "outputs": [],
   "source": [
    "class ModelManager:\n",
    "    \"\"\"Handles model loading and inference.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: Optional[str] = None,\n",
    "        device: Optional[str] = None,\n",
    "        temperature: float = 0.7,\n",
    "        top_k: int = 40,\n",
    "        use_8bit: bool = False,\n",
    "        use_4bit: bool = True, # Essential for 7B on 8GB GPU\n",
    "        max_memory_gb: Optional[float] = 7.5 # Leave some headroom\n",
    "    ):\n",
    "        self.device = self._resolve_device(device)\n",
    "        self.model_id = model_id or self._get_default_model_id()\n",
    "        self.temperature = temperature\n",
    "        self.top_k = top_k\n",
    "        self.use_8bit = use_8bit\n",
    "        self.use_4bit = use_4bit\n",
    "        self.max_memory_gb = max_memory_gb\n",
    "\n",
    "\n",
    "        self._tokenizer = None\n",
    "        self._model = None\n",
    "        # Setup memory optimization\n",
    "        setup_memory_optimization()\n",
    "\n",
    "\n",
    "    def _resolve_device(self, device: Optional[str]) -> torch.device:\n",
    "        \"\"\"Determines the appropriate device for model execution.\"\"\"\n",
    "        if device:\n",
    "            return torch.device(device)\n",
    "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def _get_default_model_id(self) -> str:\n",
    "        \"\"\"Selects default model based on available hardware.\"\"\"\n",
    "        if self.device.type == \"cuda\":\n",
    "            return CPU_MODEL_ID\n",
    "        return CPU_MODEL_ID\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Loads tokenizer and model if not already loaded.\"\"\"\n",
    "        if self._model and self._tokenizer:\n",
    "            return\n",
    "        logging.info(f\"Loading model: {self.model_id}\")\n",
    "        clear_cache()\n",
    "        try:\n",
    "          self._tokenizer = AutoTokenizer.from_pretrained(self.model_id, use_fast=True)\n",
    "          \n",
    "          # Build model loading kwargs\n",
    "          model_kwargs = {\"device_map\": \"auto\"}\n",
    "\n",
    "          # ########## Add quantization for GPU ##########\n",
    "          #  Apply 4-bit or 8-bit quantization if specified\n",
    "          if self.use_4bit:\n",
    "              from transformers import BitsAndBytesConfig\n",
    "              model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "                  load_in_4bit=True,\n",
    "                  bnb_4bit_compute_dtype=torch.float16,\n",
    "                  bnb_4bit_quant_type=\"nf4\",\n",
    "                  bnb_4bit_use_double_quant=True,\n",
    "              )\n",
    "              logging.info(\"Loading with 4-bit quantization\")\n",
    "          elif self.use_8bit:\n",
    "                from transformers import BitsAndBytesConfig\n",
    "                model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "                    load_in_8bit=True\n",
    "                )\n",
    "                logging.info(\"Loading with 8-bit quantization\")\n",
    "          else:\n",
    "              # Only set dtype if NOT using quantization\n",
    "              model_kwargs[\"torch_dtype\"] = torch.float16 if self.device.type == \"cuda\" else torch.float32\n",
    "            \n",
    "          if self.max_memory_gb and self.device.type == \"cuda\":\n",
    "            device_index = self.device.index if self.device.index is not None else 0\n",
    "            model_kwargs[\"max_memory\"] = {device_index: f\"{self.max_memory_gb}GB\"}\n",
    "          # ########## quantization ##########\n",
    "\n",
    "          self._model = AutoModelForCausalLM.from_pretrained(\n",
    "              self.model_id,\n",
    "              **model_kwargs\n",
    "          )\n",
    "          self._model.eval()\n",
    "\n",
    "          # Log memory after loading if in debug\n",
    "          logging.info(\"Model loaded successfully\")\n",
    "          log_memory_usage()\n",
    "        except torch.cuda.OutOfMemoryError as e:\n",
    "            logging.error(f\"CUDA OOM while loading model: {e}\")\n",
    "            clear_cache()\n",
    "            raise RuntimeError(\n",
    "                \"Out of GPU memory. Try: \\n\"\n",
    "                \"1. Use smaller model (1.5B instead of 7B)\\n\"\n",
    "                \"2. Enable quantization: use_8bit=True or use_4bit=True\\n\"\n",
    "                \"3. Set max_memory_gb to limit memory per GPU\\n\"\n",
    "                \"4. Close other GPU processes\"\n",
    "            ) from e\n",
    "\n",
    "\n",
    "    # def _clean_deepseek_output(self, text: str) -> str:\n",
    "    #     \"\"\"\n",
    "    #     Cleans DeepSeek-R1 model output by removing reasoning tokens.\n",
    "    #     DeepSeek-R1 models wrap reasoning in <think></think> or similar tags.\n",
    "    #     \"\"\"\n",
    "    #     # Remove content between think tags\n",
    "    #     text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n",
    "    #     text = re.sub(r'<sub>.*?</sub>', '', text, flags=re.DOTALL)\n",
    "\n",
    "    #     # Try to extract JSON object if present\n",
    "    #     json_match = re.search(r'\\{[^{}]*\"CAD-code\"[^{}]*\\}', text, flags=re.DOTALL)\n",
    "    #     if json_match:\n",
    "    #         return json_match.group(0)\n",
    "\n",
    "    #     return text.strip()\n",
    "\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        temperature: Optional[float] = None,\n",
    "        top_k: Optional[int] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Generates text from the model given a prompt.\"\"\"\n",
    "        self.load_model()\n",
    "        temp = temperature if temperature is not None else self.temperature\n",
    "        tk = top_k if top_k is not None else self.top_k\n",
    "\n",
    "        try:\n",
    "          # Clear cache before generation\n",
    "          clear_cache()\n",
    "\n",
    "          inputs = self._tokenizer(\n",
    "              prompt,\n",
    "              return_tensors=\"pt\",\n",
    "              truncation=True\n",
    "          ).to(self._model.device)\n",
    "          logging.debug(f\" Calling model with these inputs: {inputs}\")\n",
    "\n",
    "          generate_kwargs = {\n",
    "            \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "            \"pad_token_id\": self._tokenizer.eos_token_id,\n",
    "            \"eos_token_id\": self._tokenizer.eos_token_id,\n",
    "          }\n",
    "          generate_kwargs.update({\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": float(temp),\n",
    "            \"top_k\": int(tk),\n",
    "        })\n",
    "\n",
    "\n",
    "          with torch.no_grad():\n",
    "              outputs = self._model.generate(**inputs, **generate_kwargs)\n",
    "              \n",
    "          logging.debug(f\"Model parameters: temperature={temp}, top_k={tk}, max_new_tokens={MAX_NEW_TOKENS}\")\n",
    "          logging.debug(f\"Model raw output: {outputs}\")\n",
    "\n",
    "          full_text = self._tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "          prompt_text = self._tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
    "          # Clean up inputs/outputs tensors\n",
    "          del inputs, outputs\n",
    "          clear_cache()\n",
    "\n",
    "          if full_text.startswith(prompt_text):\n",
    "                generated = full_text[len(prompt_text):].strip()\n",
    "          else:\n",
    "                generated = full_text.strip()\n",
    "\n",
    "          # Clean DeepSeek-R1 reasoning tokens\n",
    "          # generated = self._clean_deepseek_output(generated)\n",
    "          return generated\n",
    "\n",
    "        except torch.cuda.OutOfMemoryError as e:\n",
    "            logging.error(f\"CUDA OOM during generation: {e}\")\n",
    "\n",
    "            # Log memory after loading if in debug\n",
    "            log_memory_usage()\n",
    "\n",
    "            clear_cache()\n",
    "            raise RuntimeError(\n",
    "                \"Out of GPU memory during generation. Try:\\n\"\n",
    "                \"1. Reduce max_new_tokens\\n\"\n",
    "                \"2. Process texts in smaller batches\\n\"\n",
    "                \"3. Unload and reload model: agent.model_manager.unload_model()\\n\"\n",
    "                \"4. Enable quantization if not already enabled\"\n",
    "            ) from e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Start with 1.5B model (no quantization needed)\n",
    "print(\"Testing 1.5B model...\")\n",
    "manager = ModelManager(\n",
    "    model_id=CPU_MODEL_ID,\n",
    "    temperature=0.7,\n",
    "    top_k=40\n",
    ")\n",
    "\n",
    "prompt = \"hi?\"\n",
    "response = manager.generate(prompt)\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8ptuwXT9KU4"
   },
   "source": [
    "## OutputValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJoY6k4y7U-5"
   },
   "outputs": [],
   "source": [
    "class OutputValidator:\n",
    "    \"\"\"Validates and parses model outputs.\"\"\"\n",
    "\n",
    "    ALLOWED_CODES = {\"WCT\", \"GT\", \"Other\", \"NONE\"}\n",
    "    REQUIRED_KEYS = {\"CAD-code\", \"rationale\"}\n",
    "\n",
    "    @classmethod\n",
    "    def validate_and_parse(cls, text: str) -> Tuple[bool, Optional[Dict[str, Any]], Optional[str]]:\n",
    "        \"\"\"\n",
    "        Validates model output and parses JSON.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (is_valid, parsed_dict, error_message)\n",
    "        \"\"\"\n",
    "        text = text.strip()\n",
    "\n",
    "        # Try parsing the entire string as JSON\n",
    "        parsed = cls._extract_json(text)\n",
    "        if parsed is None:\n",
    "            return False, None, \"Could not extract valid JSON from response\"\n",
    "\n",
    "        # Validate structure\n",
    "        error = cls._validate_structure(parsed)\n",
    "        if error:\n",
    "            return False, None, error\n",
    "\n",
    "        return True, parsed, None\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_json(text: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Attempts to extract and parse JSON from text.\"\"\"\n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        except json.JSONDecodeError:\n",
    "            # Try to extract first {...} block\n",
    "            match = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
    "            if not match:\n",
    "                return None\n",
    "            try:\n",
    "                return json.loads(match.group(0))\n",
    "            except json.JSONDecodeError:\n",
    "                return None\n",
    "\n",
    "    @classmethod\n",
    "    def _validate_structure(cls, parsed: Dict[str, Any]) -> Optional[str]:\n",
    "        \"\"\"Validates the structure and content of parsed JSON.\"\"\"\n",
    "        # Check keys\n",
    "        if set(parsed.keys()) != cls.REQUIRED_KEYS:\n",
    "            return f\"Unexpected keys: {list(parsed.keys())}\"\n",
    "\n",
    "        # Validate code\n",
    "        code = parsed.get(\"CAD-code\")\n",
    "        if code not in cls.ALLOWED_CODES:\n",
    "            return f\"Invalid CAD-code: {code}\"\n",
    "\n",
    "        # Validate rationale length\n",
    "        rationale = parsed.get(\"rationale\", \"\")\n",
    "\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVWYL1BNN3b0"
   },
   "source": [
    "## Prompt Builder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lj_6we2nDmmz"
   },
   "outputs": [],
   "source": [
    "class PromptBuilder:\n",
    "    \"\"\"Handles all prompt construction logic.\"\"\"\n",
    "\n",
    "    # JSON Schema definition\n",
    "    SCHEMA = {\n",
    "        \"CAD-code\": \"<ONE OF: WCT, GT, Other, NONE>\",\n",
    "        \"rationale\": \"<â‰¤5 sentences, evidence-based>\"\n",
    "    }\n",
    "\n",
    "    # Valid codes for validation\n",
    "    VALID_CODES = {\"WCT\", \"GT\", \"Other\", \"NONE\"}\n",
    "\n",
    "    # Default examples for few-shot learning\n",
    "    DEFAULT_EXAMPLES = [\n",
    "        {\n",
    "            \"input\": \"Everybody please listen.\",\n",
    "            \"output\": {\n",
    "                \"CAD-code\": \"WCT\",\n",
    "                \"rationale\": 'Addresses the whole class using \"Everybody\" to get attention.'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Group 3, read the next paragraph.\",\n",
    "            \"output\": {\n",
    "                \"CAD-code\": \"GT\",\n",
    "                \"rationale\": 'Directs a specific group \"Group 3\" to perform an action.'\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    def __init__(self, name: str, personality: str, role: str, codebook: Dict[str, str], config: Dict[str, Any]):\n",
    "        self.name = name\n",
    "        self.personality = personality\n",
    "        self.role = role\n",
    "        self.codebook = codebook\n",
    "        self.config = config\n",
    "\n",
    "    def build_system_prompt(self, role: str) -> str:\n",
    "        \"\"\"Creates the system prompt with instructions and examples.\"\"\"\n",
    "        return (\n",
    "            f\"You are {self.name}, a {self.personality} qualitative-coding agent.\\n\"\n",
    "            f\"Task: {role}.\\n\\n\"\n",
    "            \"CRITICAL: Output ONLY a single JSON object.\\n\\n\"\n",
    "            \"REQUIREMENTS (follow exactly):\\n\"\n",
    "            \"1) Your ENTIRE response must be ONLY this JSON object and nothing else:\\n\"\n",
    "            f\"   {json.dumps(self.SCHEMA)}\\n\"\n",
    "            \"2) Use double quotes for JSON strings.\\n\"\n",
    "            \"3) CAD-code must be one of: WCT, GT, Other, NONE\\n\"\n",
    "            \"4) Rationale:  grounded in evidence from the text.\\n\"\n",
    "            \"5) If multiple codes could apply, choose the most likely one; if ambiguous, use NONE.\\n\\n\"\n",
    "            \"CORRECT OUTPUT EXAMPLES:\\n\"\n",
    "            'Input: \"Everybody please listen.\"\\n'\n",
    "            'Output: {\"CAD-code\":\"WCT\",\"rationale\":\"Addresses the whole class using \\\\\"Everybody\\\\\" to get attention.\"}\\n\\n'\n",
    "            'Input: \"Group 3, read the next paragraph.\"\\n'\n",
    "            'Output: {\"CAD-code\":\"GT\",\"rationale\":\"Directs a specific group \\\\\"Group 3\\\\\" to perform an action.\"}\\n\\n'\n",
    "            \"Remember: Output ONLY the JSON object. Start your response with { and end with }\"\n",
    "        )\n",
    "    # def build_system_prompt(self, role: str) -> str:\n",
    "    #     \"\"\"Creates the system prompt with instructions and examples.\"\"\"\n",
    "    #     return (\n",
    "    #         f\"You are {self.name}, a {self.personality} qualitative-coding agent.\\n\"\n",
    "    #         f\"Task: {role}.\\n\\n\"\n",
    "    #         \"Remember: Start your response with { and end with } \\n\\n\"\n",
    "    #     )\n",
    "    def build_context_prompt(self) -> str:\n",
    "        \"\"\"Creates the codebook context.\"\"\"\n",
    "        if not self.codebook:\n",
    "            return \"\"\n",
    "        lines = [f\"- {k}: {v}\" for k, v in self.codebook.items()]\n",
    "        return \"Codebook:\\n\" + \"\\n\".join(lines)\n",
    "\n",
    "    def build_user_prompt(self, text: str) -> str:\n",
    "        \"\"\"Creates the user prompt with the text to annotate.\"\"\"\n",
    "        template = self.config.get(\n",
    "            \"user_template\",\n",
    "            'text to code: \\n{text}\\n\\n'\n",
    "        )\n",
    "        return template.format(text=text)\n",
    "\n",
    "    def build_full_prompt(\n",
    "        self,\n",
    "        text: str,\n",
    "        role: str,\n",
    "        extra_context: Optional[str] = None,\n",
    "        previous_turn: Optional[str] = None\n",
    "    ) -> Dict[str, str]:\n",
    "        \"\"\"Builds complete prompt dictionary with all components.\"\"\"\n",
    "        prompt = {\n",
    "            \"system\": self.build_system_prompt(role),\n",
    "            \"context\": self.build_context_prompt(),\n",
    "            \"user\": self.build_user_prompt(text),\n",
    "        }\n",
    "\n",
    "        if extra_context:\n",
    "            prompt[\"extra\"] = extra_context\n",
    "\n",
    "        if previous_turn:\n",
    "            # Append previous turn to context\n",
    "            if prompt.get(\"context\"):\n",
    "                prompt[\"context\"] += f\"\\n\\n###\\nPrevious turn: {previous_turn}\"\n",
    "            else:\n",
    "                prompt[\"user\"] += f\"\\n\\n###\\nPrevious turn: {previous_turn}\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def build_retry_prompt(self, original_prompt: str, failed_output: str) -> str:\n",
    "        \"\"\"Builds a retry prompt when the model fails to produce valid JSON.\"\"\"\n",
    "        return (\n",
    "            f\"{original_prompt}\\n\\n\"\n",
    "            \"--- RETRY REQUEST ---\\n\"\n",
    "            \"Your previous output was invalid or incorrectly formatted.\\n\"\n",
    "            f\"Previous output:\\n{failed_output[:500]}\\n\\n\"\n",
    "            \"Please output ONLY a valid JSON object with this exact structure:\\n\"\n",
    "            f\"{json.dumps(self.SCHEMA)}\\n\\n\"\n",
    "            \"Requirements:\\n\"\n",
    "            \"- Start with { and end with }\\n\"\n",
    "            \"- No markdown, no explanations, no extra text\\n\"\n",
    "            f\"- CAD-code must be exactly one of: {', '.join(sorted(self.VALID_CODES))}\\n\"\n",
    "            \"- Use double quotes for strings\\n\\n\"\n",
    "            \"Return ONLY the JSON object now:\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def to_string(prompt_dict: Dict[str, str]) -> str:\n",
    "        \"\"\"Converts prompt dictionary to a formatted string.\"\"\"\n",
    "        parts = []\n",
    "        for key in [\"system\", \"context\", \"extra\", \"user\"]:\n",
    "            if prompt_dict.get(key):\n",
    "                parts.append(f\"=== {key.upper()} ===\\n{prompt_dict[key]}\")\n",
    "        return \"\\n\\n\".join(parts)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"String representation for debugging.\"\"\"\n",
    "        return (\n",
    "            f\"PromptBuilder(name={self.name}, \"\n",
    "            f\"personality={self.personality}, \"\n",
    "            f\"role={self.role}, \"\n",
    "            f\"codebook_size={len(self.codebook)}, \"\n",
    "            # f\"examples_count={len(self.examples)})\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhbGHJQbQ8MW"
   },
   "source": [
    "## BaseCodingAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wEVWdrGpRAXM"
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Any, Union\n",
    "import torch\n",
    "import logging\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "class BaseCodingAgent:\n",
    "    \"\"\"Base class for coding agents with LLM-based text annotation.\"\"\"\n",
    "\n",
    "    ALLOWED_CODES = {\"WCT\", \"GT\", \"Other\", \"NONE\"}\n",
    "    DEFAULT_TEMPERATURE = 0.4  # Define constant\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        personality: str,\n",
    "        role: str,\n",
    "        model_id: Optional[str] = None,\n",
    "        device: Optional[str] = None,\n",
    "        temperature: float = DEFAULT_TEMPERATURE,\n",
    "        top_k: int = 40,\n",
    "        codebook: Optional[Dict[str, str]] = None,\n",
    "        config: Optional[Dict[str, Any]] = None,\n",
    "        debug: bool = False,\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.personality = personality\n",
    "        self.role = role\n",
    "        self.debug = debug\n",
    "\n",
    "        # Initialize components\n",
    "        self.validator = OutputValidator()\n",
    "        self.prompt_builder = PromptBuilder(\n",
    "            name, personality, role,\n",
    "            codebook or {}, config or {}\n",
    "        )\n",
    "        self.model_manager = ModelManager(\n",
    "            model_id, device, temperature, top_k\n",
    "        )\n",
    "\n",
    "        # Store options\n",
    "        self.options = {\"temperature\": temperature, \"top_k\": top_k}\n",
    "        self.codebook = codebook or {}\n",
    "        self.config = config or {}\n",
    "\n",
    "\n",
    "    # Main chat interface for the agent to interact with the language model\n",
    "    def chat(self, text: str, max_retries = int, role: Optional[str] = None, extra_context: Optional[str] = None, **gen_opts):\n",
    "        # Build prompt\n",
    "        prompt_str = self.get_prompt_str(\n",
    "            text=text,\n",
    "            role=role or self.role,\n",
    "            extra_context=extra_context,\n",
    "            **gen_opts\n",
    "        )\n",
    "\n",
    "        # Log if debug enabled\n",
    "        if self.debug:\n",
    "            logging.info(\"=== PROMPT ===\")\n",
    "            logging.info(prompt_str)\n",
    "\n",
    "        # Generate response\n",
    "        return self._call_and_retry(prompt_str, max_retries, **gen_opts)\n",
    "\n",
    "\n",
    "    def _call_and_retry(\n",
    "        self,\n",
    "        prompt_str: str,\n",
    "        max_retries: int,\n",
    "        **gen_opts\n",
    "    ) -> str:\n",
    "        \"\"\"Internal method to handle generation with retries.\"\"\"\n",
    "        try:\n",
    "            raw = self.model_manager.generate(prompt_str, **gen_opts)\n",
    "            valid, parsed, err = self.validator.validate_and_parse(raw)\n",
    "\n",
    "            if valid:\n",
    "                return raw\n",
    "\n",
    "            logger.debug(\"Initial attempt failed: %s. Raw: %s\", err, raw[:500])\n",
    "\n",
    "            # Retry with structured prompt\n",
    "            for attempt in range(max_retries):\n",
    "                retry_prompt = self.prompt_builder.build_retry_prompt(\n",
    "                    prompt_str, raw\n",
    "                )\n",
    "                logger.debug(\"Retry attempt %d/%d\", attempt + 1, max_retries)\n",
    "\n",
    "                raw = self.model_manager.generate(retry_prompt, **gen_opts)\n",
    "                valid, parsed, err = self.validator.validate_and_parse(raw)\n",
    "\n",
    "                if valid:\n",
    "                    return raw\n",
    "\n",
    "                logger.debug(\n",
    "                    \"Retry %d failed: %s. Raw: %s\",\n",
    "                    attempt + 1, err, raw[:500]\n",
    "                )\n",
    "\n",
    "            # Fallback to heuristic\n",
    "            logger.warning(\n",
    "                \"Model failed after %d retries. Using heuristic fallback.\",\n",
    "                max_retries\n",
    "            )\n",
    "            return json.dumps(self._heuristic_label(prompt_str))\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error during generation: %s\", e, exc_info=True)\n",
    "            return json.dumps({\n",
    "                \"CAD-code\": \"NONE\",\n",
    "                \"rationale\": f\"Error: {str(e)}\"\n",
    "            })\n",
    "\n",
    "    def _heuristic_label(self, text: str) -> Dict[str, str]:\n",
    "        \"\"\"Fallback heuristic labeling when model fails.\"\"\"\n",
    "        lower = (text or \"\").lower().strip()\n",
    "\n",
    "        if not lower:\n",
    "            return {\n",
    "                \"CAD-code\": \"NONE\",\n",
    "                \"rationale\": \"Empty input\"\n",
    "            }\n",
    "\n",
    "        # Check patterns in priority order\n",
    "        patterns = [\n",
    "            (r'\\b(everybody|everyone|class|students|all of you|all)\\b',\n",
    "             \"WCT\", \"whole-class addressing\"),\n",
    "            (r'\\b(group|pair|you two|you three)\\b',\n",
    "             \"GT\", \"group-level addressing\"),\n",
    "            (r'^[A-Z][a-z]+,', \"GT\", \"direct student address\"),\n",
    "        ]\n",
    "\n",
    "        for pattern, code, reason in patterns:\n",
    "            if re.search(pattern, text):\n",
    "                return {\"CAD-code\": code, \"rationale\": reason}\n",
    "\n",
    "        return {\n",
    "            \"CAD-code\": \"Other\",\n",
    "            \"rationale\": \"Non-directed teacher talk\"\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    def get_prompt_str(self, text: str, role: Optional[str] = None,extra_context: Optional[str] = None,**gen_opts):\n",
    "        # Build prompt\n",
    "        prompt_dict = self.prompt_builder.build_full_prompt(\n",
    "            text=text,\n",
    "            role=role or self.role,\n",
    "            extra_context = extra_context,\n",
    "            previous_turn=None,\n",
    "            **gen_opts\n",
    "        )\n",
    "\n",
    "        return self.prompt_builder.to_string(prompt_dict)\n",
    "\n",
    "\n",
    "    def get_agent_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return agent configuration as dictionary.\"\"\"\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"personality\": self.personality,\n",
    "            \"role\": self.role,\n",
    "            \"model\": self.model_manager.model_id,\n",
    "            \"device\": str(self.model_manager.device),\n",
    "            \"options\": self.options,\n",
    "            \"codebook\": self.codebook,\n",
    "            \"config\": self.config,\n",
    "            \"debug\": self.debug,\n",
    "        }\n",
    "\n",
    " # ---------------- Output validation & parsing ----------------\n",
    "    def validate_and_parse(self, text: str) -> Tuple[bool, Optional[Dict[str, Any]], Optional[str]]:\n",
    "        return self.validator.validate_and_parse(text)\n",
    "\n",
    "    def get_parsed_resp(self, text: str):\n",
    "        \"\"\"Parse response.\"\"\"\n",
    "        valid, parsed, err = self.validator.validate_and_parse(text)\n",
    "        return parsed if valid else None\n",
    "\n",
    "class SingleAgentCoding(BaseCodingAgent):\n",
    "    # Assigns a code to a given text based on the codebook and generates a rationale\n",
    "    def assign_code(self, text: str,\n",
    "                    max_retries: int = MAX_RETRIES,\n",
    "                    extra_context: Optional[str] = None,\n",
    "                    **gen_opts) -> str:\n",
    "        logging.debug(f\"Assigning code for text: {text}\") # Log the text being processed\n",
    "        # Call the chat method with a specific role for assigning codes\n",
    "        response = self.chat(\n",
    "                text=text,\n",
    "                role=self.role,\n",
    "                max_retries=max_retries,\n",
    "                extra_context=extra_context,\n",
    "                **gen_opts)\n",
    "        logging.debug(f\"Raw response from agent: {response}\") # Log the raw response from the agent\n",
    "\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gykK-rB9XfEw"
   },
   "source": [
    "## MAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_t6C1biPwZV"
   },
   "source": [
    "\n",
    "**High-level logic (fixed)**\n",
    "\n",
    "1.   Round 0: each agent labels independently.\n",
    "2.   List itemIf all agree â†’ stop.\n",
    "3.   Else iterate discussion rounds:\n",
    "  *   compute majority (strict majority preferred)\n",
    "  *   if majority exists: reprompt only minority agents with majority + arguments\n",
    "  *   else: general discussion (everyone sees everyoneâ€™s arguments)\n",
    "\n",
    "4.  Stop early if: unanimous OR stable majority OR no changes OR reached T.\n",
    "\n",
    "5. Extract final answer via EXT.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFIxm5ZcEWGx"
   },
   "source": [
    "### AgentResponse and DiscussionResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6QJDuOqJEO3j"
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Any, Union, Tuple, List\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass, field\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# from prompt_helper import PromptBuilder\n",
    "# from model_helper import ModelManager\n",
    "# from agent_helper import OutputValidator, SingleAgentCoding\n",
    "\n",
    "@dataclass\n",
    "class AgentResponse:\n",
    "    \"\"\"Represents a single agent's response in a discussion round.\n",
    "\n",
    "    Attributes:\n",
    "        agent: Name/identifier of the agent\n",
    "        code: CAD code assigned (e.g., 'WCT', 'GT', 'Other', 'NONE')\n",
    "        rationale: Agent's reasoning for the code choice\n",
    "        raw: Raw unparsed response from the agent\n",
    "        round: Round number (1-indexed)\n",
    "    \"\"\"\n",
    "    agent: str\n",
    "    code: str\n",
    "    rationale: str\n",
    "    raw: str\n",
    "    round: int\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate response data.\"\"\"\n",
    "        if self.round < 1:\n",
    "            raise ValueError(f\"Round must be >= 1, got {self.round}\")\n",
    "        if not self.agent:\n",
    "            raise ValueError(\"Agent name cannot be empty\")\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"Developer-friendly representation.\"\"\"\n",
    "        return (\n",
    "            f\"AgentResponse(agent={self.agent!r}, code={self.code!r}, \"\n",
    "            f\"round={self.round}, rationale={self.rationale[:50]!r}...)\"\n",
    "        )\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Human-readable representation.\"\"\"\n",
    "        return (\n",
    "            f\"Response from: \"\n",
    "            f\"Agent '{self.agent}' in Round {self.round}:\\n\"\n",
    "            f\"  Code: {self.code}\\n\"\n",
    "            f\"  Rationale: {self.rationale or '(none provided)'}\"\n",
    "            f\"  Raw: {self.raw}\"\n",
    "        )\n",
    "\n",
    "    def convert_to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert response to a dictionary.\"\"\"\n",
    "        return {\n",
    "            \"agent\": self.agent,\n",
    "            # \"text\": self.text,\n",
    "            \"code\": self.code,\n",
    "            \"rationale\": self.rationale,\n",
    "            \"raw\": self.raw,\n",
    "            \"round\": self.round,\n",
    "        }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DiscussionResult:\n",
    "    \"\"\"Results from a multi-agent discussion process.\n",
    "\n",
    "    Attributes:\n",
    "        final_code: Consensus or plurality code\n",
    "        final_rationale: Combined rationale from agents who chose final_code\n",
    "        confidence: Agreement ratio (0.0 to 1.0)\n",
    "        history: List of responses per round: history[round_idx] = [AgentResponse, ...]\n",
    "        tallies: Vote counts per round: tallies[round_idx] = {code: count, ...}\n",
    "        consensus_reached: Whether consensus threshold was met\n",
    "        num_rounds: Total number of rounds conducted\n",
    "    \"\"\"\n",
    "    text_to_code: str\n",
    "    human_code: str\n",
    "    final_code: str\n",
    "    final_rationale: str\n",
    "    confidence: float\n",
    "    history: List[List[AgentResponse]] = field(default_factory=list)\n",
    "    round_dicts: List[Dict[str, Any]] = field(default_factory=list)\n",
    "    tallies: List[Dict[str, int]] = field(default_factory=list)\n",
    "    consensus_reached: bool = False\n",
    "    num_rounds: int = 0\n",
    "    num_agents: int = 0\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate result data.\"\"\"\n",
    "        if not 0.0 <= self.confidence <= 1.0:\n",
    "            raise ValueError(f\"Confidence must be in [0, 1], got {self.confidence}\")\n",
    "        if self.num_rounds < 0:\n",
    "            raise ValueError(f\"num_rounds must be >= 0, got {self.num_rounds}\")\n",
    "        if self.history and len(self.history) != self.num_rounds:\n",
    "            raise ValueError(\n",
    "                f\"History length ({len(self.history)}) doesn't match \"\n",
    "                f\"num_rounds ({self.num_rounds})\"\n",
    "            )\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"Developer-friendly representation.\"\"\"\n",
    "        return (\n",
    "\n",
    "            f\"DiscussionResult(text_to_code={self.text_to_code!r}, \"\n",
    "            f\"human_code={self.human_code!r}, \"\n",
    "            f\"final_code={self.final_code!r}, \"\n",
    "            f\"confidence={self.confidence:.2f}, \"\n",
    "            f\"consensus={self.consensus_reached}, rounds={self.num_rounds})\"\n",
    "            f\"  History: {len(self.history)} rounds\"\n",
    "            f\"  Tallies: {len(self.tallies)} rounds\"\n",
    "        )\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Human-readable summary.\"\"\"\n",
    "        consensus_str = \"âœ“ Consensus\" if self.consensus_reached else \"âœ— Plurality\"\n",
    "        return (\n",
    "            f\"Discussion Result:\\n\"\n",
    "            f\"  Text to Code: {self.text_to_code}\\n\"\n",
    "            f\"  Human Code: {self.human_code}\\n\"\n",
    "            f\"  consensus_str Result ({consensus_str}):\\n\"\n",
    "            f\"  Final Code: {self.final_code}\\n\"\n",
    "            f\"  Confidence: {self.confidence:.1%}\\n\"\n",
    "            f\"  Rounds: {self.num_rounds}\\n\"\n",
    "            f\"  Rationale: {self.final_rationale[:100]}...\"\n",
    "            f\"  History: {len(self.history)} rounds\"\n",
    "            f\"  Tallies: {len(self.tallies)} rounds\"\n",
    "        )\n",
    "    def __dict__(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert result to a dictionary.\"\"\"\n",
    "        return {\n",
    "            \"text_to_code\": self.text_to_code,\n",
    "            \"human_code\": self.human_code,\n",
    "            \"final_code\": self.final_code,\n",
    "            \"final_rationale\": self.final_rationale,\n",
    "            \"confidence\": self.confidence,\n",
    "            \"history\": self.history,\n",
    "            \"tallies\": self.tallies,\n",
    "            \"consensus_reached\": self.consensus_reached,\n",
    "            \"num_rounds\": self.num_rounds,\n",
    "            \"round_dicts\": self.get_round_dicts()\n",
    "        }\n",
    "    def get_num_agents(self) -> int:\n",
    "        \"\"\"Get number of participating agents.\n",
    "\n",
    "        Looks at the first round of discussion history to count agents.\n",
    "        If no history exists, returns 0.\n",
    "        \"\"\"\n",
    "        if self.history:\n",
    "            return len(self.history[0])\n",
    "        return 0\n",
    "\n",
    "    def display(self, verbose: bool = True) -> str:\n",
    "        \"\"\"Formatted display with optional round-by-round details.\n",
    "\n",
    "        Args:\n",
    "            verbose: Show detailed round-by-round breakdown\n",
    "\n",
    "        Returns:\n",
    "            Formatted string representation\n",
    "        \"\"\"\n",
    "        lines = [\n",
    "            f\"\\n{'='*70}\",\n",
    "            f\"DISCUSSION RESULT\",\n",
    "            f\"{'='*70}\",\n",
    "            f\"Final Code: {self.final_code}\",\n",
    "            f\"Confidence: {self.confidence:.1%} ({self.confidence * self.get_num_agents():.0f}/{self.get_num_agents()} agents)\",\n",
    "            f\"Consensus: {'âœ“ Yes' if self.consensus_reached else 'âœ— No (plurality vote)'}\",\n",
    "            f\"Rounds: {self.num_rounds}\",\n",
    "            f\"\\nFinal Rationale:\",\n",
    "            f\"{self.final_rationale or '(none provided)'}\",\n",
    "        ]\n",
    "\n",
    "        if verbose and self.history:\n",
    "            lines.append(f\"\\n{'-'*70}\")\n",
    "            lines.append(\"ROUND-BY-ROUND BREAKDOWN:\")\n",
    "            lines.append('-'*70)\n",
    "\n",
    "            for round_idx, (responses, tally) in enumerate(zip(self.history, self.tallies), 1):\n",
    "                lines.append(f\"\\nðŸ“ Round {round_idx}:\")\n",
    "                lines.append(f\"   Votes: {dict(tally)}\")\n",
    "\n",
    "                for resp in responses:\n",
    "                    lines.append(f\"   â€¢ {resp.agent}: {resp.code}\")\n",
    "                    if resp.rationale:\n",
    "                        lines.append(f\"     â†’ {resp.rationale[:80]}...\")\n",
    "\n",
    "        lines.append('='*70 + '\\n')\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "    def get_agent_journey(self, agent_name: str) -> List[AgentResponse]:\n",
    "        \"\"\"Track how a specific agent voted across rounds.\n",
    "\n",
    "        Args:\n",
    "            agent_name: Name of the agent to track\n",
    "\n",
    "        Returns:\n",
    "            List of AgentResponse objects for this agent, one per round\n",
    "        \"\"\"\n",
    "        journey = []\n",
    "        for round_responses in self.history:\n",
    "            for resp in round_responses:\n",
    "                if resp.agent == agent_name:\n",
    "                    journey.append(resp)\n",
    "                    break\n",
    "        return journey\n",
    "\n",
    "    def get_round_dicts(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Convert history to a list of dictionaries.\"\"\"\n",
    "        res = []\n",
    "        for round_idx, (responses, tally) in enumerate(zip(self.history, self.tallies), 1):\n",
    "            round_dict = {\n",
    "                \"round_num\": round_idx,\n",
    "                \"votes\": dict(tally),\n",
    "                \"responses\": [resp.convert_to_dict() for resp in responses]\n",
    "            }\n",
    "            res.append(round_dict)\n",
    "            self.round_dicts = res\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils for write_row_html_log during code run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import html\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "def _esc(x: Any) -> str:\n",
    "    return html.escape(\"\" if x is None else str(x))\n",
    "\n",
    "def _safe_pre(x: Any, max_len: int = 5000) -> str:\n",
    "    s = \"\" if x is None else str(x)\n",
    "    if len(s) > max_len:\n",
    "        s = s[:max_len] + \"\\n...(truncated)...\"\n",
    "    return html.escape(s)\n",
    "\n",
    "def _flatten_discussion_result(dr: \"DiscussionResult\") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Returns list of rows: one per agent per round.\n",
    "    Uses dr.get_round_dicts() (your JSON-friendly trace).\n",
    "    \"\"\"\n",
    "    flat = []\n",
    "    for rd in (dr.get_round_dicts() or []):\n",
    "        rnum = rd.get(\"round_num\")\n",
    "        votes = rd.get(\"votes\", {})\n",
    "        for resp in rd.get(\"responses\", []):\n",
    "            flat.append({\n",
    "                \"round\": rnum,\n",
    "                \"agent\": resp.get(\"agent\", \"\"),\n",
    "                \"code\": resp.get(\"code\", \"\"),\n",
    "                \"rationale\": resp.get(\"rationale\", \"\"),\n",
    "                \"raw\": resp.get(\"raw\", \"\"),\n",
    "                \"votes\": votes,\n",
    "            })\n",
    "    return flat\n",
    "\n",
    "def write_row_html_log(\n",
    "    out_dir: str,\n",
    "    idx: Any,\n",
    "    transcript: str,\n",
    "    dr: Optional[\"DiscussionResult\"] = None,\n",
    "    human_code: str = \"\",\n",
    "    error: str = \"\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Writes an HTML file for this row. Returns filepath.\n",
    "    \"\"\"\n",
    "    # make a out_dir name folder inside RESULTS_PATH\n",
    "    os.makedirs(os.path.join(RESULTS_PATH, out_dir), exist_ok=True)\n",
    "    fname = f\"row_{idx}.html\"\n",
    "    path = os.path.join(out_dir, fname)\n",
    "    print(f\"Folder created (or already exists) at: {os.path.join(RESULTS_PATH, out_dir)}\")\n",
    "\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    if error or dr is None:\n",
    "        html_doc = f\"\"\"<!doctype html>\n",
    "<html><head><meta charset=\"utf-8\"><title>Row { _esc(idx) } (ERROR)</title>\n",
    "<style>\n",
    "body{{font-family:system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif;margin:24px;line-height:1.35}}\n",
    ".card{{border:1px solid #ddd;border-radius:14px;padding:16px;margin:14px 0}}\n",
    "pre{{white-space:pre-wrap;word-wrap:break-word;background:#fafafa;padding:12px;border-radius:10px;border:1px solid #eee;margin:0}}\n",
    "</style></head><body>\n",
    "<h1>Row { _esc(idx) } (ERROR)</h1>\n",
    "<p style=\"color:#666\">Generated { _esc(now) }</p>\n",
    "<div class=\"card\"><h2>Transcript</h2><pre>{ _safe_pre(transcript) }</pre></div>\n",
    "<div class=\"card\"><h2>Error</h2><pre>{ _safe_pre(error) }</pre></div>\n",
    "</body></html>\"\"\"\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(html_doc)\n",
    "        return path\n",
    "\n",
    "    flat = _flatten_discussion_result(dr)\n",
    "\n",
    "    # Table rows\n",
    "    trs = []\n",
    "    for row in flat:\n",
    "        votes_json = json.dumps(row[\"votes\"], ensure_ascii=False)\n",
    "        trs.append(\n",
    "            \"<tr>\"\n",
    "            f\"<td>{_esc(row['round'])}</td>\"\n",
    "            f\"<td><b>{_esc(row['agent'])}</b></td>\"\n",
    "            f\"<td><b>{_esc(row['code'])}</b></td>\"\n",
    "            f\"<td style='white-space:pre-wrap'>{_esc(row['rationale'])}</td>\"\n",
    "            f\"<td><details><summary>votes</summary><pre style='white-space:pre-wrap'>{_safe_pre(votes_json, max_len=2000)}</pre></details></td>\"\n",
    "            f\"<td><details><summary>raw</summary><pre style='white-space:pre-wrap'>{_safe_pre(row['raw'])}</pre></details></td>\"\n",
    "            \"</tr>\"\n",
    "        )\n",
    "    body_rows = \"\\n\".join(trs) if trs else \"<tr><td colspan='6'>(No trace)</td></tr>\"\n",
    "\n",
    "    # Agreement badge\n",
    "    agree = (human_code.strip() != \"\" and human_code == dr.final_code)\n",
    "    badge = \"âœ…\" if agree else (\"âŒ\" if human_code.strip() else \"â€”\")\n",
    "\n",
    "    html_doc = f\"\"\"<!doctype html>\n",
    "<html><head><meta charset=\"utf-8\"><title>Row { _esc(idx) }</title>\n",
    "<style>\n",
    "body{{font-family:system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif;margin:24px;line-height:1.35}}\n",
    "h1{{margin:0 0 8px 0}}\n",
    ".sub{{color:#666;margin-bottom:18px}}\n",
    ".card{{border:1px solid #ddd;border-radius:14px;padding:16px;margin:14px 0}}\n",
    ".grid{{display:grid;grid-template-columns:180px 1fr;gap:8px 12px}}\n",
    ".label{{color:#555}}\n",
    "pre{{white-space:pre-wrap;word-wrap:break-word;background:#fafafa;padding:12px;border-radius:10px;border:1px solid #eee;margin:0}}\n",
    "table{{border-collapse:collapse;width:100%}}\n",
    "th,td{{border:1px solid #ddd;padding:10px;vertical-align:top}}\n",
    "th{{background:#f5f5f5;text-align:left}}\n",
    ".pill{{display:inline-block;padding:2px 10px;border:1px solid #ddd;border-radius:999px;font-size:12px}}\n",
    "</style></head><body>\n",
    "\n",
    "<h1>{badge} Row {_esc(idx)}</h1>\n",
    "<div class=\"sub\">Generated {_esc(now)}</div>\n",
    "\n",
    "<div class=\"card\">\n",
    "  <h2>Transcript</h2>\n",
    "  <pre>{_safe_pre(transcript)}</pre>\n",
    "</div>\n",
    "\n",
    "<div class=\"card\">\n",
    "  <h2>Final decision</h2>\n",
    "  <div class=\"grid\">\n",
    "    <div class=\"label\">final_code</div><div><span class=\"pill\">{_esc(dr.final_code)}</span></div>\n",
    "    <div class=\"label\">confidence</div><div>{_esc(dr.confidence)}</div>\n",
    "    <div class=\"label\">consensus_reached</div><div>{_esc(dr.consensus_reached)}</div>\n",
    "    <div class=\"label\">num_rounds</div><div>{_esc(dr.num_rounds)}</div>\n",
    "    <div class=\"label\">human_code</div><div>{_esc(human_code)}</div>\n",
    "    <div class=\"label\">final_rationale</div><div style=\"white-space:pre-wrap\">{_esc(dr.final_rationale)}</div>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div class=\"card\">\n",
    "  <h2>Rounds (agent Ã— round)</h2>\n",
    "  <table>\n",
    "    <tr><th>Round</th><th>Agent</th><th>Code</th><th>Rationale</th><th>Votes</th><th>Raw</th></tr>\n",
    "    {body_rows}\n",
    "  </table>\n",
    "</div>\n",
    "\n",
    "</body></html>\"\"\"\n",
    "\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html_doc)\n",
    "\n",
    "    return path\n",
    "\n",
    "def write_index_html(out_dir: str, row_files: List[str], title: str = \"Discussion Logs\") -> str:\n",
    "    \"\"\"\n",
    "    Writes an index.html that links to all row files.\n",
    "    row_files should be file *names* or relative paths within out_dir.\n",
    "    \"\"\"\n",
    "    path = os.path.join(out_dir, \"index.html\")\n",
    "    links = \"\\n\".join([f\"<li><a href='{html.escape(os.path.basename(p))}'>{html.escape(os.path.basename(p))}</a></li>\"\n",
    "                       for p in row_files])\n",
    "    doc = f\"\"\"<!doctype html>\n",
    "<html><head><meta charset=\"utf-8\"><title>{html.escape(title)}</title>\n",
    "<style>\n",
    "body{{font-family:system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif;margin:24px}}\n",
    "li{{margin:6px 0}}\n",
    "</style></head><body>\n",
    "<h1>{html.escape(title)}</h1>\n",
    "<ul>\n",
    "{links}\n",
    "</ul>\n",
    "</body></html>\"\"\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(doc)\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NF3fymO3EPVQ"
   },
   "source": [
    "### MAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55c2M-yjutT_"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DiscussionConfig:\n",
    "    \"\"\"Configuration for multi-agent discussion behavior.\"\"\"\n",
    "    max_rounds: int = 3\n",
    "    consensus_threshold: float = 0.9\n",
    "    max_retries_per_agent: int = 3\n",
    "    allowed_codes: frozenset = field(\n",
    "        default_factory=lambda: frozenset({\"WCT\", \"GT\", \"Other\", \"NONE\"})\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate configuration parameters.\"\"\"\n",
    "        if self.max_rounds < 1:\n",
    "            raise ValueError(\"max_rounds must be at least 1\")\n",
    "        if not 0 < self.consensus_threshold <= 1:\n",
    "            raise ValueError(\"consensus_threshold must be in (0, 1]\")\n",
    "        if self.max_retries_per_agent < 1:\n",
    "            raise ValueError(\"max_retries_per_agent must be at least 1\")\n",
    "        if not self.allowed_codes:\n",
    "            raise ValueError(\"allowed_codes cannot be empty\")\n",
    "\n",
    "class MultiAgentDiscussion:\n",
    "    \"\"\"\n",
    "    Draft multi-agent version\n",
    "\n",
    "    Assumptions (editable):\n",
    "    - N agents.\n",
    "    - Round 1: all agents independently propose a code.\n",
    "    - If unanimous: return immediately.\n",
    "    - Else if a strict majority exists: re-prompt only minority agents (they see majority label + optional rationales).\n",
    "    - Else (tie / no majority): run a discussion round where each agent sees a compact transcript of others.\n",
    "    - Missing-code outputs are re-tried per-agent per-round\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 agents: List[Any],\n",
    "                 config: Optional[DiscussionConfig] = None):\n",
    "\n",
    "        # Validate inputs\n",
    "        if not agents:\n",
    "            raise ValueError(\"At least one agent is required\")\n",
    "\n",
    "        # Initialize configuration\n",
    "        self.config = config or DiscussionConfig()\n",
    "\n",
    "        self.agents = agents\n",
    "        self.round_num = 0\n",
    "        self.prev_responses = None\n",
    "        self.history = []\n",
    "        self.tallies = []\n",
    "        self.consensus_reached = False\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.store_trace = True\n",
    "\n",
    "        self.max_rounds = self.config.max_rounds\n",
    "        self.threshold = self.config.consensus_threshold\n",
    "        self.allowed_codes = self.config.allowed_codes\n",
    "\n",
    "    def _top_non_none(self, items, allowed_codes) -> Tuple[Optional[str], int, int]:\n",
    "        \"\"\"\n",
    "        - validates codes\n",
    "        - ignores \"NONE\"\n",
    "        - returns (top_code, top_count, total_n)\n",
    "        \"\"\"\n",
    "        # Normalize input into a list of codes\n",
    "        if isinstance(items, dict):\n",
    "            codes = []\n",
    "            for code, count in items.items():\n",
    "                codes.extend([code] * int(count))  # expand tally into votes\n",
    "        else:\n",
    "            codes = list(items)\n",
    "\n",
    "        total_n = len(codes)\n",
    "        if total_n == 0:\n",
    "            return None, 0, 0\n",
    "\n",
    "        # Validate codes (fail fast)\n",
    "        invalid = set(codes) - allowed_codes\n",
    "        if invalid:\n",
    "            self.logger.error(f\"Invalid codes encountered: {invalid}\")\n",
    "            return None, 0, total_n\n",
    "\n",
    "        # Drop NONE votes\n",
    "        valid = [c for c in codes if c != \"NONE\"]\n",
    "        if not valid:\n",
    "            return None, 0, total_n\n",
    "\n",
    "        top_code, top_count = Counter(valid).most_common(1)[0]\n",
    "        return top_code, top_count, total_n\n",
    "\n",
    "    def is_consensus_reached(self, tally: Dict[str, int], threshold: float = 0.8) -> Tuple[Optional[str], float]:\n",
    "        \"\"\"\n",
    "        Determines if consensus is reached among agents.\n",
    "\n",
    "        Args:\n",
    "            tally: Dictionary mapping code to count of agents who chose that code\n",
    "            threshold: Minimum agreement ratio required for consensus (default: 0.8 = 80%)\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (code, agreement_ratio) if consensus reached, else (None, 0.0)\n",
    "            - code: The consensus code if agreement >= threshold\n",
    "            - agreement_ratio: Proportion of total agents agreeing on the top code\n",
    "        \"\"\"\n",
    "        top_code, top_count, total_n = self._top_non_none(tally, self.allowed_codes)\n",
    "        if top_code is None or total_n == 0:\n",
    "            return None, 0.0\n",
    "\n",
    "        agreement_ratio = top_count / total_n\n",
    "        if agreement_ratio >= threshold:\n",
    "            self.logger.info(f\"Consensus reached on code '{top_code}' with agreement {agreement_ratio:.2f}\")\n",
    "            return top_code, agreement_ratio\n",
    "\n",
    "        self.logger.info(f\"No consensus: top code '{top_code}' has agreement {agreement_ratio:.2f}\")\n",
    "        return None, agreement_ratio\n",
    "\n",
    "    def get_majority_vote(self, codes: List[str]) -> Union[str, int]:\n",
    "        \"\"\"\n",
    "        Get majority vote from list of codes.\n",
    "\n",
    "        Args:\n",
    "            codes: List of code strings\n",
    "\n",
    "        Returns:\n",
    "            Majority code if strict majority exists (>50%), else -1\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If invalid codes are present\n",
    "        \"\"\"\n",
    "        top_code, top_count, total_n = self._top_non_none(codes, self.config.allowed_codes)\n",
    "        if not top_code or total_n == 0:\n",
    "            return -1\n",
    "\n",
    "        return top_code if top_count > (total_n / 2) else -1\n",
    "\n",
    "    def _finalize(self, last_round_responses: List[AgentResponse]) -> Tuple[str, str, float]:\n",
    "        \"\"\"\n",
    "        Finalize the discussion by plurality vote.\n",
    "        Returns:\n",
    "            - final_code: str\n",
    "            - final_rationale: str\n",
    "            - confidence: float\n",
    "        \"\"\"\n",
    "        codes = [resp.code for resp in last_round_responses]\n",
    "\n",
    "        # sanity check (fail fast instead of silently lying)\n",
    "        invalid = set(codes) - self.allowed_codes\n",
    "        if invalid:\n",
    "            raise ValueError(f\"Invalid codes encountered: {invalid}\")\n",
    "\n",
    "        counts = Counter(codes)\n",
    "        total_n = len(codes)\n",
    "        if not counts:\n",
    "            return \"NONE\", \"\", 0.0\n",
    "\n",
    "        final_code, freq = counts.most_common(1)[0]\n",
    "        confidence = freq / len(self.agents)\n",
    "\n",
    "        # Aggregate rationales for the final code\n",
    "        rationales = [resp.rationale for resp in last_round_responses\n",
    "                      if resp.code == final_code and resp.rationale]\n",
    "        final_rationale = \" | \".join(rationales)\n",
    "\n",
    "        return final_code, final_rationale, confidence\n",
    "\n",
    "    def _build_simple_discussion_context(self, prev_responses: Optional[List[AgentResponse]],\n",
    "                                  round_num: int) -> str:\n",
    "        \"\"\"\n",
    "        Build human-readable discussion context from previous round.\n",
    "\n",
    "        This provides agents with:\n",
    "        - Clear round indicator\n",
    "        - All previous agent positions and rationales (full transparency)\n",
    "        - Vote distribution summary\n",
    "        - Majority position if exists\n",
    "        - Instruction to reconsider\n",
    "\n",
    "        Args:\n",
    "            prev_round: Previous round's agent responses (None for round 1)\n",
    "            round_num: Current round number\n",
    "\n",
    "        Returns:\n",
    "            Formatted string context for agents to consider\n",
    "        \"\"\"\n",
    "        if not prev_responses:\n",
    "            return f\"Round 1: Provide your independent assessment.\"\n",
    "\n",
    "        # Start building context\n",
    "        lines = [f\"Round {round_num}: Previous round responses:\", \"\"]\n",
    "\n",
    "        # Show each agent's position and rationale\n",
    "        for resp in prev_responses:\n",
    "            rationale = resp.rationale if resp.rationale else \"(no rationale provided)\"\n",
    "            lines.append(f\"- {resp.agent} chose '{resp.code}': {rationale}\")\n",
    "\n",
    "        lines.append(\"\")  # Blank line for readability\n",
    "\n",
    "        # Add vote distribution summary\n",
    "        codes = [r.code for r in prev_responses if r.code != \"NONE\"]\n",
    "        if codes:\n",
    "            tally = Counter(codes)\n",
    "            tally_str = \", \".join(f\"{code}: {count}\" for code, count in tally.most_common())\n",
    "            lines.append(f\"Vote distribution: {tally_str}\")\n",
    "\n",
    "            # Highlight majority if exists\n",
    "            majority = self.get_majority_vote([r.code for r in prev_responses])\n",
    "            if majority != -1:\n",
    "                lines.append(f\"Majority position: {majority}\")\n",
    "\n",
    "        # Instruction to reconsider\n",
    "        lines.extend([\n",
    "            \"\",\n",
    "            \"Consider the above responses. You may:\",\n",
    "            \"- Change your assessment if you find others' reasoning convincing\",\n",
    "            \"- Maintain your position if you believe your reasoning is stronger\",\n",
    "            \"- Provide additional rationale to explain your choice\"\n",
    "        ])\n",
    "\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"\n",
    "        Reset discussion state for reuse.\n",
    "\n",
    "        Clears all round history, tallies, and consensus flags.\n",
    "        Useful for running multiple discussions with the same agent pool.\n",
    "        \"\"\"\n",
    "        self.round_num = 0\n",
    "        self.history.clear()\n",
    "        self.tallies.clear()\n",
    "        self.consensus_reached = False\n",
    "        self.logger.debug(\"Discussion state reset\")\n",
    "\n",
    "    def _to_agent_response(self, agent: Any, raw: str, round_num: int) -> AgentResponse:\n",
    "        \"\"\"\n",
    "        Convert raw agent output into AgentResponse using agent.validate_and_parse().\n",
    "        Keeps invalid outputs as NONE with an error rationale.\n",
    "        \"\"\"\n",
    "        valid, parsed, err = agent.validate_and_parse(raw)\n",
    "\n",
    "        if not valid or not isinstance(parsed, dict):\n",
    "            code = \"NONE\"\n",
    "            rationale = f\"Parse error/invalid format: {err}\"\n",
    "        else:\n",
    "            code = parsed.get(\"CAD-code\", \"NONE\")\n",
    "            rationale = parsed.get(\"rationale\", \"\") or \"\"\n",
    "\n",
    "        code = self._validate_code(code)\n",
    "\n",
    "        return AgentResponse(\n",
    "            agent=getattr(agent, \"name\", str(agent)),\n",
    "            code=code,\n",
    "            rationale=rationale,\n",
    "            raw=raw,\n",
    "            round=round_num,\n",
    "        )\n",
    "    def _validate_code(self, code: str) -> str:\n",
    "        if code not in self.allowed_codes:\n",
    "            self.logger.warning(f\"Invalid code received: {code}\")\n",
    "            return \"NONE\"\n",
    "        return code\n",
    "    def _tally_round(self, round_responses: List[AgentResponse]) -> Dict[str, int]:\n",
    "        \"\"\"Count votes for a single round.\"\"\"\n",
    "        return dict(Counter(r.code for r in round_responses))\n",
    "\n",
    "    ############### Discussion funcs ##################\n",
    "    def discuss(self, text: str, human_code: str = \"\", **kwargs) -> DiscussionResult:\n",
    "        \"\"\"\n",
    "        Debate with consensus.\n",
    "        \"\"\"\n",
    "        self.reset()  # avoid leaking history across calls\n",
    "        self.logger.info(\"== Starting MultiAgentDiscussion with %d agents\", len(self.agents))\n",
    "        self.text = text\n",
    "\n",
    "        for round_idx in range(self.config.max_rounds):\n",
    "            self.round_num = round_idx + 1\n",
    "            self.logger.info(\"Round %d/%d\", self.round_num, self.config.max_rounds)\n",
    "\n",
    "            # Agents see previous round responses as context (None on round 1)\n",
    "            prev_resp = self.history[-1] if self.history else None\n",
    "            ctx = self._build_simple_discussion_context(prev_resp, self.round_num)\n",
    "\n",
    "            # Collect responses for this round\n",
    "            round_responses: List[AgentResponse] = []\n",
    "            tally: Dict[str, int] = {}\n",
    "            for agent in self.agents:\n",
    "              max_retries = self.config.max_retries_per_agent if self.config.max_retries_per_agent else 1\n",
    "\n",
    "              raw = agent.assign_code(text, extra_context=ctx, max_retries = max_retries, **kwargs)\n",
    "              # Parse and validate response using the agent's validate_and_parse method\n",
    "              round_responses.append(self._to_agent_response(agent, raw, self.round_num))\n",
    "              # update vote counts for this round\n",
    "\n",
    "            # Save round artifacts\n",
    "            self.history.append(round_responses)\n",
    "            tally = self._tally_round(round_responses)\n",
    "            self.tallies.append(tally)\n",
    "            self.logger.debug(f\"Round {self.round_num} tally: {tally}\")\n",
    "\n",
    "            # stop early if consensus reached\n",
    "            consensus_code, agreement = self.is_consensus_reached(tally, self.config.consensus_threshold)\n",
    "            if consensus_code:\n",
    "                self.consensus_reached = True\n",
    "                rationales = [resp.rationale for resp in round_responses\n",
    "                            if resp.code == consensus_code and resp.rationale]\n",
    "\n",
    "                return DiscussionResult(\n",
    "                    text_to_code=text,\n",
    "                    human_code=human_code,\n",
    "                    final_code=consensus_code,\n",
    "                    final_rationale=\" | \".join(rationales),\n",
    "                    confidence=agreement,\n",
    "                    history=self.history,\n",
    "                    tallies=self.tallies,\n",
    "                    consensus_reached=True,\n",
    "                    num_rounds=self.round_num\n",
    "                )\n",
    "\n",
    "        # Max rounds reached without consensus - finalize by plurality\n",
    "        final_code, final_rationale, confidence = self._finalize(self.history[-1])\n",
    "\n",
    "        return DiscussionResult(\n",
    "            text_to_code=text,\n",
    "            human_code=human_code,\n",
    "            final_code=final_code,\n",
    "            final_rationale=final_rationale,\n",
    "            confidence=confidence,\n",
    "            history=self.history,\n",
    "            tallies=self.tallies,\n",
    "            consensus_reached=False,\n",
    "            num_rounds=self.round_num\n",
    "        )\n",
    "\n",
    "    def run_batch_discussions(\n",
    "        self,\n",
    "        data_df: pd.DataFrame,\n",
    "        text_col: str = \"transcript\",\n",
    "        store_traces: bool = True,\n",
    "        batch_num: int = -1,\n",
    "        log_every: int = 10,\n",
    "        save_name: Optional[str] = None,   # base path without extension\n",
    "        save_every: int = 50,\n",
    "        stop_on_error: bool = False,\n",
    "        **kwargs\n",
    "    ) -> Tuple[List[DiscussionResult], pd.DataFrame]:\n",
    "\n",
    "      if text_col not in data_df.columns:\n",
    "        raise ValueError(f\"Column '{text_col}' not found. Available: {list(data_df.columns)}\")\n",
    "\n",
    "      # How many rows?\n",
    "      total = len(data_df)\n",
    "      n = total if (batch_num is None or int(batch_num) < 0) else min(int(batch_num), total)\n",
    "\n",
    "      idxs = list(data_df.index[:n])\n",
    "      texts = data_df.loc[idxs, text_col].tolist()\n",
    "\n",
    "      results: List[\"DiscussionResult\"] = []\n",
    "      rows: List[Dict[str, Any]] = []  # for output df\n",
    "      row_html_files = []\n",
    "\n",
    "      # FIXED: Added human_code_val argument to signature\n",
    "      def to_row(idx, text, human_code_val, result: Optional[\"DiscussionResult\"], error: str) -> Dict[str, Any]:\n",
    "        \"\"\"One output row (success or failure).\"\"\"\n",
    "        if error:\n",
    "            return {\n",
    "                \"row_index\": idx,\n",
    "                \"text_to_code\": text,\n",
    "                \"human_code\": human_code_val,\n",
    "                \"final_code\": \"NONE\",\n",
    "                \"final_rationale\": \"\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"consensus_reached\": False,\n",
    "                \"num_rounds\": 0,\n",
    "                \"tallies\": [],\n",
    "                \"round_dicts\": [],\n",
    "                \"error\": error,\n",
    "            }\n",
    "\n",
    "        assert result is not None\n",
    "        round_dicts = []\n",
    "        tallies = []\n",
    "        if store_traces:\n",
    "            try:\n",
    "                round_dicts = result.get_round_dicts()\n",
    "            except Exception as e:\n",
    "                self.logger.warning(\"Trace serialization failed at row %s: %s\", idx, e)\n",
    "                round_dicts = []\n",
    "            tallies = result.tallies\n",
    "\n",
    "        return {\n",
    "            \"row_index\": idx,\n",
    "            \"text_to_code\": result.text_to_code,\n",
    "            \"human_code\": human_code_val,\n",
    "            \"final_code\": result.final_code,\n",
    "            \"final_rationale\": result.final_rationale,\n",
    "            \"confidence\": result.confidence,\n",
    "            \"consensus_reached\": result.consensus_reached,\n",
    "            \"num_rounds\": result.num_rounds,\n",
    "            \"tallies\": tallies,\n",
    "            \"round_dicts\": round_dicts,\n",
    "            \"error\": \"\",\n",
    "        }\n",
    "\n",
    "      def checkpoint(reason: str) -> None:\n",
    "          \"\"\"Write a CSV checkpoint (no-op if save_name is None).\"\"\"\n",
    "          if not save_name:\n",
    "              return\n",
    "          save_CSV_path = os.path.join(RESULTS_PATH, f\"{save_name}.csv\")\n",
    "          out_df = pd.DataFrame(rows, index=[r[\"row_index\"] for r in rows])\n",
    "          out_df.to_csv(f\"{save_name}.csv\", index=True)\n",
    "          self.logger.info(\"Checkpoint (%s): wrote %d rows -> %s.csv\", reason, len(out_df), save_name)\n",
    "\n",
    "      self.logger.info(\n",
    "        \"Batch start: %d/%d rows | text_col='%s' | traces=%s | checkpoints=%s\",\n",
    "        n, total, text_col, store_traces, (\"on\" if save_name else \"off\")\n",
    "      )\n",
    "\n",
    "      success = 0\n",
    "      errors = 0\n",
    "\n",
    "      for i, (idx, text) in enumerate(tqdm(list(zip(idxs, texts)), desc=\"Processing\", total=n), start=1):\n",
    "          human_code = str(data_df.loc[idx, \"CAD\"]) if \"CAD\" in data_df.columns else \"\"\n",
    "          try:\n",
    "              res = self.discuss(text, human_code=human_code, **kwargs) # Pass human_code here\n",
    "\n",
    "              if not store_traces:\n",
    "                  res = DiscussionResult(\n",
    "                      text_to_code=res.text_to_code,\n",
    "                      human_code=human_code,\n",
    "                      final_code=res.final_code,\n",
    "                      final_rationale=res.final_rationale,\n",
    "                      confidence=res.confidence,\n",
    "                      history=[],\n",
    "                      tallies=[],\n",
    "                      consensus_reached=res.consensus_reached,\n",
    "                      num_rounds=res.num_rounds,\n",
    "                  )\n",
    "\n",
    "              html_path = write_row_html_log(\n",
    "                  out_dir=\"discussion_html\",\n",
    "                  idx=idx,\n",
    "                  transcript=text,\n",
    "                  dr=res,\n",
    "                  human_code=human_code\n",
    "              )\n",
    "              row_html_files.append(html_path)\n",
    "\n",
    "              results.append(res)\n",
    "              rows.append(to_row(idx, text, human_code, res, error=\"\"))\n",
    "              success += 1\n",
    "\n",
    "          except Exception as e:\n",
    "              err = f\"{type(e).__name__}: {str(e)[:500]}\"\n",
    "              self.logger.exception(\"Row %s failed (%d/%d): %s\", idx, i, n, err)\n",
    "\n",
    "              placeholder = DiscussionResult(\n",
    "                  text_to_code=str(text),\n",
    "                  human_code=human_code,\n",
    "                  final_code=\"NONE\",\n",
    "                  final_rationale=\"\",\n",
    "                  confidence=0.0,\n",
    "                  history=[],\n",
    "                  tallies=[],\n",
    "                  consensus_reached=False,\n",
    "                  num_rounds=0,\n",
    "              )\n",
    "              results.append(placeholder)\n",
    "              rows.append(to_row(idx, text, human_code, None, error=err))\n",
    "              errors += 1\n",
    "\n",
    "              if save_name:\n",
    "                  checkpoint(reason=f\"error_at_{idx}\")\n",
    "\n",
    "              if stop_on_error:\n",
    "                  break\n",
    "\n",
    "          if log_every and (i % log_every == 0 or i == n):\n",
    "              rate = (success / i) * 100 if i else 0.0\n",
    "              self.logger.info(\"Progress: %d/%d | success=%d | errors=%d | rate=%.1f%%\", i, n, success, errors, rate)\n",
    "\n",
    "          if save_every and save_name and (i % save_every == 0):\n",
    "              checkpoint(reason=f\"periodic_{i}\")\n",
    "\n",
    "      write_index_html(\"discussion_html\", row_html_files, title=\"MAD / MultiAgentDiscussion Logs\")\n",
    "      if save_name:\n",
    "          checkpoint(reason=\"final\")\n",
    "\n",
    "      output_df = pd.DataFrame(rows, index=[r[\"row_index\"] for r in rows])\n",
    "      self.logger.info(\"Batch done: success=%d errors=%d total=%d\", success, errors, len(output_df))\n",
    "      return results, output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76raXMBIVBYI"
   },
   "source": [
    "## Experiment run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9k0ulr_KHPL"
   },
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhdMlylh518H"
   },
   "outputs": [],
   "source": [
    "balenced_role = \"Your job is to weigh evidence, reconcile disagreements, and enforce codebook fidelity.\"\n",
    "adversery_role = \"Rigorous prosecutor. Be skeptical. Demand direct textual evidence (quote a short phrase). Actively try to falsify other agentsâ€™ codes. If the text is ambiguous, say so and propose a safe fallback.\"\n",
    "creative_role = \"Creative empathic explorer. Look for subtle intent, context, and edge cases. Propose alternative readings and uncommon-but-plausible codes, but justify with text evidence.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbdjJqKlWvvJ"
   },
   "outputs": [],
   "source": [
    "MAD_config=DiscussionConfig(\n",
    "        max_rounds=3,\n",
    "        consensus_threshold=0.9,\n",
    "        max_retries_per_agent=2,\n",
    "        allowed_codes={\"WCT\", \"GT\", \"Other\", \"NONE\"}\n",
    "      )\n",
    "\n",
    "text = \"So remember you guys are in groups so talk to your partner about the cards you move. Make sure your partner agrees with you.\"\n",
    "\n",
    "# --- Agents ---\n",
    "agents = {\n",
    "    \"a1\": SingleAgentCoding(\"Ava\", \"balanced arbiter\", balenced_role, debug=False, codebook=CAD_CODEBOOK_DICT),\n",
    "    \"a2\": SingleAgentCoding(\"Ben\", \"rigorous and concise\", adversery_role, debug=False, codebook=CAD_CODEBOOK_DICT),\n",
    "    \"a3\": SingleAgentCoding(\"Cam\", \"creative and empathic\", creative_role, debug=False, codebook=CAD_CODEBOOK_DICT)\n",
    "}\n",
    "\n",
    "\n",
    "mad = MultiAgentDiscussion(\n",
    "    list(agents.values()),\n",
    "    config=MAD_config\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOgn1zKHvI8J"
   },
   "source": [
    "### discussion run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HiXKZ2hQ1iRX"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "batch_num = BATCH_NUM or 2\n",
    "output_file_name = f\"{today}-batch_0-{batch_num}_results\"\n",
    "\n",
    "res, new_df = mad.run_batch_discussions(df_2022,\n",
    "                                          text_col=\"transcript\",\n",
    "                                          batch_num= batch_num,\n",
    "                                          log_every=10,\n",
    "                                          save_every=20,\n",
    "                                          stop_on_error=False,\n",
    "                                          store_traces = True,\n",
    "                                          save_name=output_file_name\n",
    "                                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJyh7R3fUHyG"
   },
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6PvapbyTv4i"
   },
   "source": [
    "### Downlaod the output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1768978027951,
     "user": {
      "displayName": "Bahar Shahrokhian",
      "userId": "02701353148338583964"
     },
     "user_tz": 420
    },
    "id": "XoWVBIEZTfoY",
    "outputId": "dea7a4b9-4845-4ed9-9880-cc6f26222e79"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "#  the output FILEs\n",
    "new_df.to_parquet(f\"{output_file_name}.parquet\")\n",
    "new_df.to_csv(f\"{output_file_name}.csv\")\n",
    "\n",
    "\n",
    "files.download(f\"{output_file_name}.parquet\")\n",
    "files.download(f\"{output_file_name}.csv\")\n",
    "\n",
    "# DOWNLOAD discussion_html folder\n",
    "folder_to_download = \"discussion_html\"\n",
    "zip_filename = f\"{folder_to_download}.zip\"\n",
    "\n",
    "# Create a zip archive of the folder\n",
    "shutil.make_archive(folder_to_download, 'zip', folder_to_download)\n",
    "\n",
    "# Download the zip file\n",
    "if os.path.exists(zip_filename):\n",
    "    files.download(zip_filename)\n",
    "    print(f\"Downloaded {zip_filename}\")\n",
    "else:\n",
    "    print(f\"Error: {zip_filename} not created or found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cb6hVobhjxrB"
   },
   "source": [
    "### Write agents infos to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1768977250217,
     "user": {
      "displayName": "Bahar Shahrokhian",
      "userId": "02701353148338583964"
     },
     "user_tz": 420
    },
    "id": "KqzIlWk8i-Yw",
    "outputId": "1e6262b1-7a61-471f-a126-0606bcc3c9f4"
   },
   "outputs": [],
   "source": [
    "import html\n",
    "import json\n",
    "import json\n",
    "\n",
    "# Collect agent info\n",
    "all_agent_info = {}\n",
    "for agent_id, agent in agents.items():\n",
    "    all_agent_info[agent_id] = agent.get_agent_info()\n",
    "\n",
    "# Save to file\n",
    "output_file_name = \"agent_infos\"\n",
    "output_file = f\"{output_file_name}.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(all_agent_info, f, indent=2)\n",
    "\n",
    "print(f\"Agent information saved to {output_file}\")\n",
    "\n",
    "def write_agent_info_to_html(agent_info_dict, filename=\"agent_infos.html\"):\n",
    "    \"\"\"Writes agent configuration dictionary to a styled HTML file.\"\"\"\n",
    "    html_content = [\n",
    "        \"<!doctype html>\",\n",
    "        \"<html><head><meta charset='utf-8'><title>Agent Information</title>\",\n",
    "        \"<style>\",\n",
    "        \"body{font-family:system-ui,-apple-system,sans-serif;margin:20px;line-height:1.5;color:#333;background:#f4f4f9;}\",\n",
    "        \".agent-card{border:1px solid #ddd;margin-bottom:20px;padding:25px;border-radius:10px;box-shadow:0 4px 6px rgba(0,0,0,0.05);background:#fff;}\",\n",
    "        \"h1{color:#2c3e50;text-align:center;margin-bottom:30px;}\",\n",
    "        \"h2{margin-top:0;color:#34495e;border-bottom:2px solid #f0f0f0;padding-bottom:10px;}\",\n",
    "        \".property{margin:10px 0;}\",\n",
    "        \".label{font-weight:600;color:#555;display:inline-block;width:140px;vertical-align:top;}\",\n",
    "        \".value{display:inline-block;width:calc(100% - 150px);}\",\n",
    "        \"pre{background:#f8f9fa;padding:10px;border-radius:4px;overflow-x:auto;margin:0;}\",\n",
    "        \"ul{margin:0;padding-left:20px;}\",\n",
    "        \"</style></head><body>\",\n",
    "        \"<h1>Agent Configuration</h1>\"\n",
    "    ]\n",
    "\n",
    "    for agent_id, info in agent_info_dict.items():\n",
    "        html_content.append(f\"<div class='agent-card'>\")\n",
    "        name = info.get('name', agent_id)\n",
    "        html_content.append(f\"<h2>Agent: {html.escape(str(name))} <small style='color:#777;font-weight:normal'>({html.escape(str(agent_id))})</small></h2>\")\n",
    "\n",
    "        for key, value in info.items():\n",
    "            val_html = \"\"\n",
    "            if key == 'codebook' and isinstance(value, dict):\n",
    "                val_html = \"<ul>\" + \"\".join([f\"<li><b>{html.escape(str(k))}:</b> {html.escape(str(v))}</li>\" for k,v in value.items()]) + \"</ul>\"\n",
    "            elif isinstance(value, (dict, list)):\n",
    "                 val_html = f\"<pre>{html.escape(json.dumps(value, indent=2))}</pre>\"\n",
    "            else:\n",
    "                val_html = html.escape(str(value))\n",
    "\n",
    "            html_content.append(f\"<div class='property'><span class='label'>{html.escape(str(key))}:</span><span class='value'>{val_html}</span></div>\")\n",
    "\n",
    "        html_content.append(\"</div>\")\n",
    "\n",
    "    html_content.append(\"</body></html>\")\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(html_content))\n",
    "    print(f\"Agent info HTML saved to {filename}\")\n",
    "\n",
    "# # Execute using the dictionary from the previous step\n",
    "# if 'all_agent_info' in locals():\n",
    "#     write_agent_info_to_html(all_agent_info)\n",
    "# else:\n",
    "#     # Fallback to regenerate info if variable missing\n",
    "#     temp_info = {aid: ag.get_agent_info() for aid, ag in agents.items()}\n",
    "#     write_agent_info_to_html(temp_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pxe-ixjcyzh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7CJXjXSkPOU"
   },
   "source": [
    "# Test (don't run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tw9KICXj3n8"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "slvtfG3DRrWV"
   },
   "outputs": [],
   "source": [
    "\n",
    "# read parquet\n",
    "df = pd.read_parquet(f\"{output_file_name}.parquet\")\n",
    "df.head()\n",
    "df.to_csv(f\"{output_file_name}.csv\")\n",
    "\n",
    "#  Convert this to list\n",
    "for roud_resp in df[\"round_dicts\"][0]:\n",
    "  print(type(roud_resp))\n",
    "\n",
    "\n",
    "for result in res:\n",
    "\n",
    "\n",
    "  a = result.__dict__()\n",
    "  # print(type(a))\n",
    "  for round in a.get(\"round_dicts\"):\n",
    "    print(f\"round #{round[\"round_num\"]}\")\n",
    "    for agent in round.get(\"responses\"):\n",
    "      print(f\"agent: {agent['agent']}, coded: {agent['code']} with rationale: {agent['rationale']}\")\n",
    "      print(f\"======raw: {agent['raw']}\")\n",
    "\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xUfdBN9NGGaE"
   },
   "outputs": [],
   "source": [
    "def convert_agent_response(agent_resp: str, print_raw = False) -> AgentResponse:\n",
    "  name = agent_resp.split(\"agent=\")[1].split(\",\")[0]\n",
    "  code = agent_resp.split(\"code=\")[1].split(\",\")[0]\n",
    "  rationale = agent_resp.split(\"rationale=\")[1].split(\", raw=\")[0]\n",
    "  round = agent_resp.split(\"round=\")[1].split(\")\")[0]\n",
    "  raw = agent_resp.split(\"raw=\")[1].split(\"round=\")[0]\n",
    "\n",
    "  print(f\" At round {1} - agent {name}, coded this text as {code}, with rational that {rationale}\")\n",
    "  if print_raw:\n",
    "    print(f\"raw agent output was: {raw}\")\n",
    "  agent_resp_obj = AgentResponse(agent=name, code=code, rationale=rationale, raw=raw, round=int(round))\n",
    "  return agent_resp_obj\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# text = df['transcript'][0]\n",
    "# # print(text)\n",
    "# agents_text = df['raw'][0]\n",
    "# discussion_results = agents_text.replace(\"'\", '\"')\n",
    "# # print(discussion_res)\n",
    "# # convert agents_text str to list\n",
    "# b = discussion_results.split(\"], \")\n",
    "# print(b[0].split(\"round=\")[0])\n",
    "# # print(b[1])?\n",
    "# # print(b[2])\n",
    "\n",
    "# # agents_res_list = agents_text.split(\"[AgentResponse(\")\n",
    "# # agents_res_valid = []\n",
    "# # for agent_res in agents_res_list:\n",
    "# #   if \"agent=\" in agent_res:\n",
    "# #     agents_res_valid.append(agent_res)\n",
    "\n",
    "# # # agent_text\n",
    "# # len(agents_res_valid)\n",
    "# # for agent_text in agents_res_valid:\n",
    "# #   print(agent_text)\n",
    "# #   agent_resp_obj = convert_agent_response(agent_text)\n",
    "#   # print(agent_resp_obj)\n",
    "# # # convert str to list\n",
    "# # agent_text = agent_text.replace(\"'\", '\"')\n",
    "# # ares = convert_agent_response(agent_text)\n",
    "# # ares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DanRVWNZj5TV"
   },
   "source": [
    "# **NEW CODE -1/20/2026**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_O71RUFkBht"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# ============================================================\n",
    "# MAD (3 agents x 3 rounds) for CAD coding on MacBook\n",
    "# TIGHT JSON FORCING VERSION:\n",
    "# - Balanced-brace JSON extraction (most reliable)\n",
    "# - Retry-on-invalid JSON (up to 3 tries per agent/round)\n",
    "# - Deterministic retry (temperature=0) + stronger constraints\n",
    "# - Decode ONLY new tokens (prevents prompt echo)\n",
    "# - Normalizes human CAD for accurate agreement\n",
    "# - Saves Excel + HTML to Desktop\n",
    "# ============================================================\n",
    "\n",
    "import os, re, json, time, gc, html\n",
    "from collections import Counter\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# -------------------------\n",
    "# USER SETTINGS\n",
    "# -------------------------\n",
    "FILE_PATH   = \"/Users/elahetajik/Desktop/data -ASU.xlsx\"\n",
    "SHEET_NAME  = None\n",
    "YEAR_FILTER = 2022\n",
    "K           = 200\n",
    "\n",
    "OUT_XLSX = \"/Users/elahetajik/Desktop/mad_outputs_2022_K200.xlsx\"\n",
    "OUT_HTML = \"/Users/elahetajik/Desktop/mad_report_2022_K200.html\"\n",
    "\n",
    "MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "# Generation controls\n",
    "TEMP_ROUND1    = 0.3\n",
    "TEMP_DISCUSS   = 0.0\n",
    "MAX_NEW_TOKENS = 160\n",
    "\n",
    "# JSON retry controls\n",
    "JSON_RETRIES = 3  # attempts per agent per round\n",
    "\n",
    "# -------------------------\n",
    "# CAD CODEBOOK\n",
    "# -------------------------\n",
    "CAD_CODEBOOK_DICT = {\n",
    "    \"WCT\":   \"Teacher is addressing the whole class.\",\n",
    "    \"GT\":    \"Teacher is addressing a group or a student in a group (student-level talk).\",\n",
    "    \"Other\": \"Teacher is not addressing the whole class or groups/students (silent/self talk/visitor/tech).\",\n",
    "    \"NONE\":  \"Ambiguous / cannot determine.\"\n",
    "}\n",
    "ALLOWED_CODES = {\"WCT\", \"GT\", \"Other\", \"NONE\"}\n",
    "\n",
    "# -------------------------\n",
    "# DEVICE\n",
    "# -------------------------\n",
    "device = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"âœ… Device:\", device)\n",
    "\n",
    "# -------------------------\n",
    "# LOAD DATA\n",
    "# -------------------------\n",
    "if not os.path.exists(FILE_PATH):\n",
    "    raise FileNotFoundError(f\"Excel not found: {FILE_PATH}\")\n",
    "\n",
    "df = pd.read_excel(FILE_PATH) if SHEET_NAME is None else pd.read_excel(FILE_PATH, sheet_name=SHEET_NAME)\n",
    "print(\"âœ… Loaded rows:\", len(df))\n",
    "print(\"âœ… Columns:\", df.columns.tolist())\n",
    "\n",
    "if \"transcript\" not in df.columns:\n",
    "    raise ValueError(\"âŒ Your Excel must have a column named exactly: transcript\")\n",
    "\n",
    "df_work = df.copy()\n",
    "\n",
    "if \"year\" in df_work.columns:\n",
    "    df_work = df_work[df_work[\"year\"] == YEAR_FILTER].copy()\n",
    "    print(f\"âœ… After year=={YEAR_FILTER} filter:\", len(df_work))\n",
    "else:\n",
    "    print(\"âš ï¸ No 'year' column found; skipping year filter.\")\n",
    "\n",
    "df_work[\"transcript\"] = df_work[\"transcript\"].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "# -------------------------\n",
    "# NORMALIZE HUMAN CAD\n",
    "# -------------------------\n",
    "def normalize_human_code(x: Any) -> str:\n",
    "    s = \"\" if pd.isna(x) else str(x)\n",
    "    s = s.strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    sl = s.lower()\n",
    "\n",
    "    # exact / common\n",
    "    if sl in {\"wct\", \"whole class\", \"whole-class\", \"wholeclass\"}:\n",
    "        return \"WCT\"\n",
    "    if sl in {\"gt\", \"group\", \"small group\", \"partner\"}:\n",
    "        return \"GT\"\n",
    "    if sl in {\"other\", \"oth\"}:\n",
    "        return \"Other\"\n",
    "    if sl in {\"none\", \"na\", \"n/a\", \"ambiguous\", \"unclear\"}:\n",
    "        return \"NONE\"\n",
    "\n",
    "    # containment for messy labels\n",
    "    if \"wct\" in sl or \"whole\" in sl:\n",
    "        return \"WCT\"\n",
    "    if re.search(r\"\\bgt\\b\", sl) or \"group\" in sl or \"partner\" in sl:\n",
    "        return \"GT\"\n",
    "    if \"other\" in sl:\n",
    "        return \"Other\"\n",
    "    if \"none\" in sl or \"ambig\" in sl or \"unclear\" in sl:\n",
    "        return \"NONE\"\n",
    "\n",
    "    return s\n",
    "\n",
    "if \"CAD\" in df_work.columns:\n",
    "    df_work[\"CAD\"] = df_work[\"CAD\"].fillna(\"\").astype(str)\n",
    "    df_work[\"CAD_norm\"] = df_work[\"CAD\"].apply(normalize_human_code)\n",
    "else:\n",
    "    df_work[\"CAD_norm\"] = \"\"\n",
    "\n",
    "df_work = df_work.head(K).copy()\n",
    "print(\"âœ… Running K =\", len(df_work))\n",
    "\n",
    "# -------------------------\n",
    "# LOAD MODEL\n",
    "# -------------------------\n",
    "print(\"â³ Loading model:\", MODEL_ID)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "\n",
    "torch_dtype = torch.float16 if device in {\"cuda\", \"mps\"} else torch.float32\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device_map=None\n",
    ").to(device)\n",
    "\n",
    "model.eval()\n",
    "print(\"âœ… Model loaded.\")\n",
    "\n",
    "# -------------------------\n",
    "# JSON EXTRACTION (brace-balanced)\n",
    "# -------------------------\n",
    "def extract_first_balanced_json(text: str) -> Optional[dict]:\n",
    "    \"\"\"\n",
    "    Finds the FIRST balanced {...} object in the text by counting braces.\n",
    "    Much more reliable than regex for messy outputs.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    s = text.strip()\n",
    "    # remove fences if any\n",
    "    s = re.sub(r\"^```(?:json)?\\s*\", \"\", s).strip()\n",
    "    s = re.sub(r\"\\s*```$\", \"\", s).strip()\n",
    "\n",
    "    start = s.find(\"{\")\n",
    "    if start == -1:\n",
    "        return None\n",
    "\n",
    "    depth = 0\n",
    "    in_str = False\n",
    "    escape = False\n",
    "\n",
    "    for i in range(start, len(s)):\n",
    "        ch = s[i]\n",
    "\n",
    "        if in_str:\n",
    "            if escape:\n",
    "                escape = False\n",
    "            elif ch == \"\\\\\":\n",
    "                escape = True\n",
    "            elif ch == '\"':\n",
    "                in_str = False\n",
    "            continue\n",
    "\n",
    "        # not in string\n",
    "        if ch == '\"':\n",
    "            in_str = True\n",
    "            continue\n",
    "        if ch == \"{\":\n",
    "            depth += 1\n",
    "        elif ch == \"}\":\n",
    "            depth -= 1\n",
    "            if depth == 0:\n",
    "                cand = s[start:i+1].strip()\n",
    "                try:\n",
    "                    obj = json.loads(cand)\n",
    "                    return obj if isinstance(obj, dict) else None\n",
    "                except Exception:\n",
    "                    return None\n",
    "\n",
    "    return None\n",
    "\n",
    "def normalize_model_json(d: dict) -> dict:\n",
    "    code = (d.get(\"CAD-code\") or d.get(\"code\") or d.get(\"CAD\") or \"\").strip()\n",
    "    rationale = (d.get(\"rationale\") or d.get(\"reasoning\") or \"\").strip()\n",
    "\n",
    "    if code not in ALLOWED_CODES:\n",
    "        code = \"NONE\"\n",
    "\n",
    "    low = rationale.lower()\n",
    "    if (not rationale) or (\"evidence-based\" in low and \"sentences\" in low):\n",
    "        rationale = \"Rationale missing/placeholder.\"\n",
    "\n",
    "    if len(rationale) > 350:\n",
    "        rationale = rationale[:350] + \"...\"\n",
    "\n",
    "    return {\"CAD-code\": code, \"rationale\": rationale}\n",
    "\n",
    "# -------------------------\n",
    "# PROMPTS\n",
    "# -------------------------\n",
    "def agent_system(name: str, personality: str) -> str:\n",
    "    return (\n",
    "        f\"You are {name}, a {personality} qualitative-coding agent.\\n\"\n",
    "        \"Task: Assign ONE CAD code for the teacher transcript using the codebook.\\n\"\n",
    "        \"STRICT OUTPUT RULE:\\n\"\n",
    "        \"- Output ONLY a JSON object. No extra text. No markdown. No thinking.\\n\"\n",
    "        'JSON schema: {\"CAD-code\":\"WCT|GT|Other|NONE\",\"rationale\":\"1-3 short evidence-based sentences\"}\\n'\n",
    "        \"Heuristics:\\n\"\n",
    "        \"- WCT: whole-class address/instructions.\\n\"\n",
    "        \"- GT: talk to a small group/partner(s).\\n\"\n",
    "        \"- Other: teacher tech/self/visitor talk not directing students.\\n\"\n",
    "        \"- NONE: only if truly impossible to infer.\\n\"\n",
    "    )\n",
    "\n",
    "def agent_user(transcript: str, extra_context: str) -> str:\n",
    "    cb = \"\\n\".join([f\"- {k}: {v}\" for k, v in CAD_CODEBOOK_DICT.items()])\n",
    "    return (\n",
    "        f\"Codebook:\\n{cb}\\n\\n\"\n",
    "        f\"{extra_context}\\n\\n\"\n",
    "        f'Text to code:\\n\"\"\"{transcript}\"\"\"\\n\\n'\n",
    "        \"Return ONLY JSON now.\"\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# GENERATION (decode only NEW tokens)\n",
    "# -------------------------\n",
    "@torch.no_grad()\n",
    "def generate_once(system_prompt: str, user_prompt: str, temperature: float, max_new_tokens: int) -> str:\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    else:\n",
    "        prompt = system_prompt + \"\\n\\n\" + user_prompt\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=int(max_new_tokens),\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=(temperature > 0.0),\n",
    "    )\n",
    "    if temperature > 0.0:\n",
    "        gen_kwargs[\"temperature\"] = float(temperature)\n",
    "\n",
    "    out = model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "    new_tokens = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    text = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "    return text\n",
    "\n",
    "def generate_json_with_retries(system_prompt: str, user_prompt: str,\n",
    "                              temperature: float, max_new_tokens: int,\n",
    "                              retries: int = JSON_RETRIES) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Try multiple times to get valid JSON.\n",
    "    - attempt 1: normal (given temperature)\n",
    "    - attempt 2+: temperature=0 + stronger forcing message appended\n",
    "    Returns dict: {\"raw\":..., \"parsed\": dict|None}\n",
    "    \"\"\"\n",
    "    last_raw = \"\"\n",
    "\n",
    "    for attempt in range(1, retries + 1):\n",
    "        if attempt == 1:\n",
    "            temp = temperature\n",
    "            user = user_prompt\n",
    "        else:\n",
    "            temp = 0.0\n",
    "            # strong forcing on retries\n",
    "            user = (\n",
    "                user_prompt\n",
    "                + \"\\n\\nCRITICAL: Your entire output MUST be ONLY the JSON object.\"\n",
    "                + \"\\nIt MUST BEGIN with '{' and END with '}'. No other characters.\"\n",
    "            )\n",
    "\n",
    "        raw = generate_once(system_prompt, user, temperature=temp, max_new_tokens=max_new_tokens)\n",
    "        last_raw = raw\n",
    "\n",
    "        # quick trim: if there's a '{', start there (helps)\n",
    "        if \"{\" in raw:\n",
    "            raw_trim = raw[raw.find(\"{\"):].strip()\n",
    "        else:\n",
    "            raw_trim = raw.strip()\n",
    "\n",
    "        parsed = extract_first_balanced_json(raw_trim)\n",
    "        if parsed and isinstance(parsed, dict):\n",
    "            return {\"raw\": raw_trim, \"parsed\": parsed}\n",
    "\n",
    "    return {\"raw\": last_raw, \"parsed\": None}\n",
    "\n",
    "# -------------------------\n",
    "# MAD\n",
    "# -------------------------\n",
    "AGENTS = [\n",
    "    (\"Ava\", \"balanced (neutral perspective)\"),\n",
    "    (\"Ben\", \"rigorous and concise (strict evidence grounding)\"),\n",
    "    (\"Cam\", \"creative and empathic (attentive to contextual nuance)\")\n",
    "]\n",
    "\n",
    "def summarize_peers(prev: List[dict]) -> str:\n",
    "    lines = [\"Previous round peer responses:\"]\n",
    "    for p in prev:\n",
    "        lines.append(f'- Code: {p[\"CAD-code\"]} | Rationale: {p[\"rationale\"]}')\n",
    "    lines.append(\"Revise if needed, but remain evidence-based. Output ONLY JSON.\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def run_mad_for_text(text: str, rounds_total: int = 3) -> dict:\n",
    "    history: List[List[dict]] = []\n",
    "\n",
    "    # Round 1\n",
    "    r1 = []\n",
    "    for (name, pers) in AGENTS:\n",
    "        sys_p = agent_system(name, pers)\n",
    "        usr_p = agent_user(text, extra_context=\"Round 1: Independent assessment.\")\n",
    "        got = generate_json_with_retries(sys_p, usr_p, temperature=TEMP_ROUND1, max_new_tokens=MAX_NEW_TOKENS)\n",
    "        parsed = got[\"parsed\"] if got[\"parsed\"] else {}\n",
    "        norm = normalize_model_json(parsed)\n",
    "        norm.update({\"agent\": name, \"raw\": got[\"raw\"]})\n",
    "        r1.append(norm)\n",
    "    history.append(r1)\n",
    "\n",
    "    current = r1\n",
    "\n",
    "    # Round 2..3\n",
    "    for r in range(2, rounds_total + 1):\n",
    "        nxt = []\n",
    "        for i, (name, pers) in enumerate(AGENTS):\n",
    "            peers = [c for j, c in enumerate(current) if j != i]\n",
    "            ctx = f\"Round {r}: Revise after seeing peers.\\n\" + summarize_peers(peers)\n",
    "            sys_p = agent_system(name, pers)\n",
    "            usr_p = agent_user(text, extra_context=ctx)\n",
    "            got = generate_json_with_retries(sys_p, usr_p, temperature=TEMP_DISCUSS, max_new_tokens=MAX_NEW_TOKENS)\n",
    "            parsed = got[\"parsed\"] if got[\"parsed\"] else {}\n",
    "            norm = normalize_model_json(parsed)\n",
    "            norm.update({\"agent\": name, \"raw\": got[\"raw\"]})\n",
    "            nxt.append(norm)\n",
    "        history.append(nxt)\n",
    "        current = nxt\n",
    "\n",
    "    final_codes = [a[\"CAD-code\"] for a in history[-1]]\n",
    "    counts = Counter([c for c in final_codes if c in ALLOWED_CODES])\n",
    "    label, freq = counts.most_common(1)[0] if counts else (\"NONE\", 0)\n",
    "    conf = freq / len(final_codes) if final_codes else 0.0\n",
    "\n",
    "    return {\"history\": history, \"majority\": label, \"conf\": conf}\n",
    "\n",
    "# -------------------------\n",
    "# RUN + SAVE\n",
    "# -------------------------\n",
    "rows_out = []\n",
    "html_blocks = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for n, (idx, row) in enumerate(df_work.iterrows(), start=1):\n",
    "    transcript = row[\"transcript\"]\n",
    "    human = row[\"CAD_norm\"] if \"CAD_norm\" in df_work.columns else \"\"\n",
    "\n",
    "    res = run_mad_for_text(transcript, rounds_total=3)\n",
    "    hist = res[\"history\"]\n",
    "    maj = res[\"majority\"]\n",
    "    conf = res[\"conf\"]\n",
    "\n",
    "    agree = int(human == maj) if human in ALLOWED_CODES else 0\n",
    "\n",
    "    outrow = dict(row)\n",
    "    outrow[\"mad_final_code\"] = maj\n",
    "    outrow[\"mad_final_conf\"] = float(conf)\n",
    "    outrow[\"agree_with_human_CAD\"] = agree\n",
    "\n",
    "    # Flatten per-agent per-round columns\n",
    "    for r_i, round_list in enumerate(hist, start=1):\n",
    "        for a in round_list:\n",
    "            agent = a[\"agent\"]\n",
    "            outrow[f\"R{r_i}_{agent}_code\"] = a[\"CAD-code\"]\n",
    "            outrow[f\"R{r_i}_{agent}_rationale\"] = a[\"rationale\"]\n",
    "            outrow[f\"R{r_i}_{agent}_raw\"] = a[\"raw\"]\n",
    "\n",
    "    # Final rationale = rationales of majority voters in final round\n",
    "    final_round = hist[-1]\n",
    "    rats = [a[\"rationale\"] for a in final_round if a[\"CAD-code\"] == maj and a[\"rationale\"]]\n",
    "    outrow[\"mad_final_rationale\"] = \" | \".join(rats) if rats else \"\"\n",
    "\n",
    "    rows_out.append(outrow)\n",
    "\n",
    "    # HTML report block\n",
    "    badge = \"âœ…\" if agree == 1 else \"âŒ\"\n",
    "    esc_t = html.escape(transcript)\n",
    "    esc_h = html.escape(human)\n",
    "    esc_m = html.escape(maj)\n",
    "\n",
    "    block = []\n",
    "    block.append(f\"<h2>{badge} Row {idx} | Final: {esc_m} (conf={conf:.2f}) | Human: {esc_h} | Agree={agree}</h2>\")\n",
    "    block.append(f\"<p><b>Transcript:</b> {esc_t}</p>\")\n",
    "\n",
    "    for r_i, round_list in enumerate(hist, start=1):\n",
    "        block.append(f\"<h3>Round {r_i}</h3>\")\n",
    "        block.append(\"<table border='1' cellpadding='6' cellspacing='0' style='border-collapse:collapse;width:100%'>\")\n",
    "        block.append(\"<tr><th>Agent</th><th>Code</th><th>Rationale</th><th>Raw</th></tr>\")\n",
    "        for a in round_list:\n",
    "            raw = html.escape(a[\"raw\"] or \"\")\n",
    "            rat = html.escape(a[\"rationale\"] or \"\")\n",
    "            code = html.escape(a[\"CAD-code\"] or \"\")\n",
    "            agent = html.escape(a[\"agent\"] or \"\")\n",
    "            block.append(\n",
    "                f\"<tr><td><b>{agent}</b></td><td><b>{code}</b></td><td>{rat}</td>\"\n",
    "                f\"<td><details><summary>show</summary><pre style='white-space:pre-wrap'>{raw}</pre></details></td></tr>\"\n",
    "            )\n",
    "        block.append(\"</table>\")\n",
    "\n",
    "    html_blocks.append(\"\\n\".join(block))\n",
    "\n",
    "    # cleanup\n",
    "    gc.collect()\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if n % 10 == 0:\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"Progress: {n}/{len(df_work)} rows | {elapsed/60:.1f} min elapsed\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"âœ… Done. Processed {len(rows_out)} rows in {elapsed/60:.2f} min\")\n",
    "\n",
    "out_df = pd.DataFrame(rows_out)\n",
    "out_df.to_excel(OUT_XLSX, index=False)\n",
    "\n",
    "with open(OUT_HTML, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"<html><head><meta charset='utf-8'><title>MAD Report</title></head><body>\")\n",
    "    f.write(f\"<h1>MAD Results | YEAR={YEAR_FILTER} | K={len(df_work)} | Model={MODEL_ID}</h1>\")\n",
    "    f.write(\"<p>Report includes: transcript, human CAD, final MAD code, and all agents/rounds with raw.</p>\")\n",
    "    f.write(\"<hr>\")\n",
    "    f.write(\"\\n<hr>\\n\".join(html_blocks))\n",
    "    f.write(\"</body></html>\")\n",
    "\n",
    "print(\"âœ… Saved Excel:\", OUT_XLSX)\n",
    "print(\"âœ… Saved HTML :\", OUT_HTML)\n",
    "print(\"Open HTML with:\")\n",
    "print(f\"open '{OUT_HTML}'\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "cb6hVobhjxrB",
    "X7CJXjXSkPOU"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "whisperx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
