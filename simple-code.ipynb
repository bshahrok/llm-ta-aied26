{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "3fDOzpVnIuY-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: C:\\Users\\notin\\Bahar\\llm\\AIED\\data\\trans_df.csv\n"
     ]
    }
   ],
   "source": [
    "# change data path as you need\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"C:\\\\Users\\\\notin\\\\Bahar\\\\llm\\\\AIED\\\\data\"\n",
    "RESULTS_PATH = \"C:\\\\Users\\\\notin\\\\Bahar\\\\llm\\\\AIED\\\\results\"\n",
    "\n",
    "data_file_name = \"trans_df.csv\"\n",
    "data_path = os.path.join(DATA_PATH, data_file_name)\n",
    "print(f\"Loading data from: {data_path}\")\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in 2022: 1185\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>start_time</th>\n",
       "      <th>stop_time</th>\n",
       "      <th>CAD</th>\n",
       "      <th>audio_file_path</th>\n",
       "      <th>duration</th>\n",
       "      <th>audio_file_name</th>\n",
       "      <th>year</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>teacher_id</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>376733</td>\n",
       "      <td>387533</td>\n",
       "      <td>WCT</td>\n",
       "      <td>/content/drive/MyDrive/Projects/2025 papers/IS...</td>\n",
       "      <td>10800</td>\n",
       "      <td>p1_segment0_WCT.wav</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019_p1_s0_T2</td>\n",
       "      <td>T2</td>\n",
       "      <td>So fact, he's writing it here, dot, a-s-u, Ari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>401551</td>\n",
       "      <td>404239</td>\n",
       "      <td>WCT</td>\n",
       "      <td>/content/drive/MyDrive/Projects/2025 papers/IS...</td>\n",
       "      <td>2688</td>\n",
       "      <td>p1_segment1_WCT.wav</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019_p1_s1_T2</td>\n",
       "      <td>T2</td>\n",
       "      <td>27 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>406801</td>\n",
       "      <td>410489</td>\n",
       "      <td>WCT</td>\n",
       "      <td>/content/drive/MyDrive/Projects/2025 papers/IS...</td>\n",
       "      <td>3688</td>\n",
       "      <td>p1_segment2_WCT.wav</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019_p1_s2_T2</td>\n",
       "      <td>T2</td>\n",
       "      <td>Just put your name, yeah.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>413489</td>\n",
       "      <td>414989</td>\n",
       "      <td>WCT</td>\n",
       "      <td>/content/drive/MyDrive/Projects/2025 papers/IS...</td>\n",
       "      <td>1500</td>\n",
       "      <td>p1_segment3_WCT.wav</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019_p1_s3_T2</td>\n",
       "      <td>T2</td>\n",
       "      <td>There shouldn't be any duplicates.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>415801</td>\n",
       "      <td>419614</td>\n",
       "      <td>WCT</td>\n",
       "      <td>/content/drive/MyDrive/Projects/2025 papers/IS...</td>\n",
       "      <td>3813</td>\n",
       "      <td>p1_segment4_WCT.wav</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019_p1_s4_T2</td>\n",
       "      <td>T2</td>\n",
       "      <td>There shouldn't be any duplicates. Everybody h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  session  start_time  stop_time  CAD  \\\n",
       "0       1      376733     387533  WCT   \n",
       "1       1      401551     404239  WCT   \n",
       "2       1      406801     410489  WCT   \n",
       "3       1      413489     414989  WCT   \n",
       "4       1      415801     419614  WCT   \n",
       "\n",
       "                                     audio_file_path  duration  \\\n",
       "0  /content/drive/MyDrive/Projects/2025 papers/IS...     10800   \n",
       "1  /content/drive/MyDrive/Projects/2025 papers/IS...      2688   \n",
       "2  /content/drive/MyDrive/Projects/2025 papers/IS...      3688   \n",
       "3  /content/drive/MyDrive/Projects/2025 papers/IS...      1500   \n",
       "4  /content/drive/MyDrive/Projects/2025 papers/IS...      3813   \n",
       "\n",
       "       audio_file_name  year      unique_id teacher_id  \\\n",
       "0  p1_segment0_WCT.wav  2019  2019_p1_s0_T2         T2   \n",
       "1  p1_segment1_WCT.wav  2019  2019_p1_s1_T2         T2   \n",
       "2  p1_segment2_WCT.wav  2019  2019_p1_s2_T2         T2   \n",
       "3  p1_segment3_WCT.wav  2019  2019_p1_s3_T2         T2   \n",
       "4  p1_segment4_WCT.wav  2019  2019_p1_s4_T2         T2   \n",
       "\n",
       "                                          transcript  \n",
       "0  So fact, he's writing it here, dot, a-s-u, Ari...  \n",
       "1                                              27 21  \n",
       "2                          Just put your name, yeah.  \n",
       "3                 There shouldn't be any duplicates.  \n",
       "4  There shouldn't be any duplicates. Everybody h...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_2022 = df[df['year'] == 2022]\n",
    "df_2022.drop(columns=['year', \"audio_file_path\", \"session\", \"audio_file_name\"], inplace=True)\n",
    "print(\"Number of records in 2022:\", len(df_2022))\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RETRIES = 2\n",
    "BATCH_NUM = 10\n",
    "MAX_NEW_TOKENS = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4w-rod8UV-u"
   },
   "source": [
    "# Run this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3otIZ5BaCZla"
   },
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1768977189693,
     "user": {
      "displayName": "Bahar Shahrokhian",
      "userId": "02701353148338583964"
     },
     "user_tz": 420
    },
    "id": "ti7iJ7XODP5i",
    "outputId": "7fc22386-1066-4092-f3fd-12b79241718c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os, re, json, time, ast\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "l7QVvwPoH6NC"
   },
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "# del model   # if available\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "XDZJAvhvs9Pb"
   },
   "outputs": [],
   "source": [
    "CAD_CODEBOOK_DICT = {\n",
    "    \"WCT\": \"The teacher is addressing the whole class.\",\n",
    "    \"GT\":  \"The teacher is addressing a group or a student in a group. It also includes any talk: student level\",\n",
    "    \"Other\": \"The teacher isn’t talking to the whole class or any groups or students. Either she’s silent or talking to herself or a visitor in a non-distracting way: \"\n",
    "}\n",
    "output_example = { \"code\": \"WCT\", \"reasoning\": \"it is more likly that the teacher is addressing the whole class\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1768977190198,
     "user": {
      "displayName": "Bahar Shahrokhian",
      "userId": "02701353148338583964"
     },
     "user_tz": 420
    },
    "id": "aXvcOzC0PzEH",
    "outputId": "60248857-fdc5-4c00-eb33-8b0aaf38de54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-21 15:09:14,985 - INFO - This is an info message from the explicitly configured logger.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Get the root logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Remove any existing handlers to prevent duplicate messages or conflicting configurations\n",
    "# This is important if basicConfig was called before or if the cell is run multiple times\n",
    "for handler in logger.handlers[:]: # Iterate over a copy of the list\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "# Create a StreamHandler that prints to standard output\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "\n",
    "# Define a formatter for the log messages\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "# Add the handler to the logger\n",
    "logger.addHandler(handler)\n",
    "\n",
    "logger.info(\"This is an info message from the explicitly configured logger.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "BvwholfAQ3Xb"
   },
   "outputs": [],
   "source": [
    "class AnnotationResult:\n",
    "    \"\"\"Result from annotating a single text.\"\"\"\n",
    "    success: bool\n",
    "    code: Optional[str] = None\n",
    "    rationale: Optional[str] = None\n",
    "    raw_output: Optional[str] = None\n",
    "    error: Optional[str] = None\n",
    "    text: Optional[str] = None\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        if self.success:\n",
    "            return f\"✓ {self.code}: {self.rationale}\"\n",
    "        return f\"✗ Error: {self.error}\"\n",
    "\n",
    "    @property\n",
    "    def parsed(self) -> Dict[str, str]:\n",
    "        \"\"\"Get parsed result in legacy format.\"\"\"\n",
    "        return {\n",
    "            \"CAD-code\": self.code or \"NONE\",\n",
    "            \"rationale\": self.rationale or \"\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lbo5AF4GJLmf"
   },
   "source": [
    "## Model/gpu manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    AX_NEW_TOKENS = 512\n",
    "    TEMPERATURE = 0.4\n",
    "    TOP_K = 40\n",
    "    CPU_MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "    GPU_MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "\n",
    "    # ============================================================================\n",
    "    # MEMORY UTILITIES\n",
    "    # ============================================================================\n",
    "\n",
    "    @staticmethod\n",
    "    def clear_cache():\n",
    "        \"\"\"Clears GPU cache and runs garbage collection.\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "        gc.collect()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_memory_info() -> Dict[str, float]:\n",
    "        \"\"\"Returns current GPU memory usage in GB.\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return {}\n",
    "\n",
    "        device = torch.cuda.current_device()\n",
    "        allocated = torch.cuda.memory_allocated(device) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(device) / 1024**3\n",
    "        total = torch.cuda.get_device_properties(device).total_memory / 1024**3\n",
    "        free = total - allocated\n",
    "\n",
    "        return {\n",
    "            \"allocated_gb\": allocated,\n",
    "            \"reserved_gb\": reserved,\n",
    "            \"total_gb\": total,\n",
    "            \"free_gb\": free\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def log_memory_usage():\n",
    "        \"\"\"Logs current memory usage.\"\"\"\n",
    "        info = get_memory_info()\n",
    "        if info:\n",
    "            logger.debug(\n",
    "                f\"GPU Memory - Allocated: {info['allocated_gb']:.2f}GB, \"\n",
    "                f\"Free: {info['free_gb']:.2f}GB, Total: {info['total_gb']:.2f}GB\"\n",
    "            )\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_memory_optimization():\n",
    "        \"\"\"Sets up environment variables for better memory management.\"\"\"\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFyS2ftqJLX6"
   },
   "outputs": [],
   "source": [
    "class ModelManager:\n",
    "    \"\"\"Handles model loading and inference.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: Optional[str] = None,\n",
    "        device: Optional[str] = None,\n",
    "        temperature: float = 0.7,\n",
    "        top_k: int = 40,\n",
    "        use_8bit: bool = False,\n",
    "        use_4bit: bool = True, # Essential for 7B on 8GB GPU\n",
    "        max_memory_gb: Optional[float] = 7.5 # Leave some headroom\n",
    "    ):\n",
    "        self.device = self._resolve_device(device)\n",
    "        self.model_id = model_id or self._get_default_model_id()\n",
    "        self.temperature = temperature\n",
    "        self.top_k = top_k\n",
    "        self.use_8bit = use_8bit\n",
    "        self.use_4bit = use_4bit\n",
    "        self.max_memory_gb = max_memory_gb\n",
    "\n",
    "\n",
    "        self._tokenizer = None\n",
    "        self._model = None\n",
    "        # Setup memory optimization\n",
    "        setup_memory_optimization()\n",
    "\n",
    "\n",
    "    def _resolve_device(self, device: Optional[str]) -> torch.device:\n",
    "        \"\"\"Determines the appropriate device for model execution.\"\"\"\n",
    "        if device:\n",
    "            return torch.device(device)\n",
    "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def _get_default_model_id(self) -> str:\n",
    "        \"\"\"Selects default model based on available hardware.\"\"\"\n",
    "        if self.device.type == \"cuda\":\n",
    "            return CPU_MODEL_ID\n",
    "        return CPU_MODEL_ID\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Loads tokenizer and model if not already loaded.\"\"\"\n",
    "        if self._model and self._tokenizer:\n",
    "            return\n",
    "        logging.info(f\"Loading model: {self.model_id}\")\n",
    "        clear_cache()\n",
    "        try:\n",
    "          self._tokenizer = AutoTokenizer.from_pretrained(self.model_id, use_fast=True)\n",
    "          \n",
    "          # Build model loading kwargs\n",
    "          model_kwargs = {\"device_map\": \"auto\"}\n",
    "\n",
    "          # ########## Add quantization for GPU ##########\n",
    "          #  Apply 4-bit or 8-bit quantization if specified\n",
    "          if self.use_4bit:\n",
    "              from transformers import BitsAndBytesConfig\n",
    "              model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "                  load_in_4bit=True,\n",
    "                  bnb_4bit_compute_dtype=torch.float16,\n",
    "                  bnb_4bit_quant_type=\"nf4\",\n",
    "                  bnb_4bit_use_double_quant=True,\n",
    "              )\n",
    "              logging.info(\"Loading with 4-bit quantization\")\n",
    "          elif self.use_8bit:\n",
    "                from transformers import BitsAndBytesConfig\n",
    "                model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "                    load_in_8bit=True\n",
    "                )\n",
    "                logging.info(\"Loading with 8-bit quantization\")\n",
    "          else:\n",
    "              # Only set dtype if NOT using quantization\n",
    "              model_kwargs[\"torch_dtype\"] = torch.float16 if self.device.type == \"cuda\" else torch.float32\n",
    "            \n",
    "          if self.max_memory_gb and self.device.type == \"cuda\":\n",
    "            device_index = self.device.index if self.device.index is not None else 0\n",
    "            model_kwargs[\"max_memory\"] = {device_index: f\"{self.max_memory_gb}GB\"}\n",
    "          # ########## quantization ##########\n",
    "\n",
    "          self._model = AutoModelForCausalLM.from_pretrained(\n",
    "              self.model_id,\n",
    "              **model_kwargs\n",
    "          )\n",
    "          self._model.eval()\n",
    "\n",
    "          # Log memory after loading if in debug\n",
    "          logging.info(\"Model loaded successfully\")\n",
    "          log_memory_usage()\n",
    "        except torch.cuda.OutOfMemoryError as e:\n",
    "            logging.error(f\"CUDA OOM while loading model: {e}\")\n",
    "            clear_cache()\n",
    "            raise RuntimeError(\n",
    "                \"Out of GPU memory. Try: \\n\"\n",
    "                \"1. Use smaller model (1.5B instead of 7B)\\n\"\n",
    "                \"2. Enable quantization: use_8bit=True or use_4bit=True\\n\"\n",
    "                \"3. Set max_memory_gb to limit memory per GPU\\n\"\n",
    "                \"4. Close other GPU processes\"\n",
    "            ) from e\n",
    "\n",
    "\n",
    "    # def _clean_deepseek_output(self, text: str) -> str:\n",
    "    #     \"\"\"\n",
    "    #     Cleans DeepSeek-R1 model output by removing reasoning tokens.\n",
    "    #     DeepSeek-R1 models wrap reasoning in <think></think> or similar tags.\n",
    "    #     \"\"\"\n",
    "    #     # Remove content between think tags\n",
    "    #     text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n",
    "    #     text = re.sub(r'<sub>.*?</sub>', '', text, flags=re.DOTALL)\n",
    "\n",
    "    #     # Try to extract JSON object if present\n",
    "    #     json_match = re.search(r'\\{[^{}]*\"CAD-code\"[^{}]*\\}', text, flags=re.DOTALL)\n",
    "    #     if json_match:\n",
    "    #         return json_match.group(0)\n",
    "\n",
    "    #     return text.strip()\n",
    "\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        temperature: Optional[float] = None,\n",
    "        top_k: Optional[int] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Generates text from the model given a prompt.\"\"\"\n",
    "        self.load_model()\n",
    "        temp = temperature if temperature is not None else self.temperature\n",
    "        tk = top_k if top_k is not None else self.top_k\n",
    "\n",
    "        try:\n",
    "          # Clear cache before generation\n",
    "          clear_cache()\n",
    "\n",
    "          inputs = self._tokenizer(\n",
    "              prompt,\n",
    "              return_tensors=\"pt\",\n",
    "              truncation=True\n",
    "          ).to(self._model.device)\n",
    "          logging.debug(f\" Calling model with these inputs: {inputs}\")\n",
    "\n",
    "          generate_kwargs = {\n",
    "            \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "            \"pad_token_id\": self._tokenizer.eos_token_id,\n",
    "            \"eos_token_id\": self._tokenizer.eos_token_id,\n",
    "          }\n",
    "          generate_kwargs.update({\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": float(temp),\n",
    "            \"top_k\": int(tk),\n",
    "        })\n",
    "\n",
    "\n",
    "          with torch.no_grad():\n",
    "              outputs = self._model.generate(**inputs, **generate_kwargs)\n",
    "              \n",
    "          logging.debug(f\"Model parameters: temperature={temp}, top_k={tk}, max_new_tokens={MAX_NEW_TOKENS}\")\n",
    "          logging.debug(f\"Model raw output: {outputs}\")\n",
    "\n",
    "          full_text = self._tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "          prompt_text = self._tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
    "          # Clean up inputs/outputs tensors\n",
    "          del inputs, outputs\n",
    "          clear_cache()\n",
    "\n",
    "          if full_text.startswith(prompt_text):\n",
    "                generated = full_text[len(prompt_text):].strip()\n",
    "          else:\n",
    "                generated = full_text.strip()\n",
    "\n",
    "          # Clean DeepSeek-R1 reasoning tokens\n",
    "          # generated = self._clean_deepseek_output(generated)\n",
    "          return generated\n",
    "\n",
    "        except torch.cuda.OutOfMemoryError as e:\n",
    "            logging.error(f\"CUDA OOM during generation: {e}\")\n",
    "\n",
    "            # Log memory after loading if in debug\n",
    "            log_memory_usage()\n",
    "\n",
    "            clear_cache()\n",
    "            raise RuntimeError(\n",
    "                \"Out of GPU memory during generation. Try:\\n\"\n",
    "                \"1. Reduce max_new_tokens\\n\"\n",
    "                \"2. Process texts in smaller batches\\n\"\n",
    "                \"3. Unload and reload model: agent.model_manager.unload_model()\\n\"\n",
    "                \"4. Enable quantization if not already enabled\"\n",
    "            ) from e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 1.5B model...\n",
      "2026-01-21 15:17:08,447 - INFO - Loading model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "2026-01-21 15:17:10,024 - INFO - Loading with 4-bit quantization\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Device None is not recognized, available devices are integers(for GPU/XPU), 'mps', 'cpu' and 'disk'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      6\u001b[39m manager = ModelManager(\n\u001b[32m      7\u001b[39m     model_id=CPU_MODEL_ID,\n\u001b[32m      8\u001b[39m     temperature=\u001b[32m0.7\u001b[39m,\n\u001b[32m      9\u001b[39m     top_k=\u001b[32m40\u001b[39m\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m prompt = \u001b[33m\"\u001b[39m\u001b[33mhi?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m response = \u001b[43mmanager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResponse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 123\u001b[39m, in \u001b[36mModelManager.generate\u001b[39m\u001b[34m(self, prompt, temperature, top_k)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\n\u001b[32m    117\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    118\u001b[39m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    119\u001b[39m     temperature: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    120\u001b[39m     top_k: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    121\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    122\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generates text from the model given a prompt.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m     temp = temperature \u001b[38;5;28;01mif\u001b[39;00m temperature \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.temperature\n\u001b[32m    125\u001b[39m     tk = top_k \u001b[38;5;28;01mif\u001b[39;00m top_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.top_k\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mModelManager.load_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     75\u001b[39m     model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mmax_memory\u001b[39m\u001b[33m\"\u001b[39m] = {\u001b[38;5;28mself\u001b[39m.device.index: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.max_memory_gb\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mGB\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# ########## quantization ##########\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[38;5;28mself\u001b[39m._model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m._model.eval()\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Log memory after loading if in debug\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:5029\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5027\u001b[39m \u001b[38;5;66;03m# Prepare the full device map\u001b[39;00m\n\u001b[32m   5028\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5029\u001b[39m     device_map = \u001b[43m_get_device_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5031\u001b[39m \u001b[38;5;66;03m# Finalize model weight initialization\u001b[39;00m\n\u001b[32m   5032\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m from_tf:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:1336\u001b[39m, in \u001b[36m_get_device_map\u001b[39m\u001b[34m(model, device_map, max_memory, hf_quantizer, dtype, keep_in_fp32_regex)\u001b[39m\n\u001b[32m   1330\u001b[39m     logger.warning(\n\u001b[32m   1331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis model has some weights that should be kept in higher precision, you need to upgrade \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`accelerate` to properly deal with them (`pip install --upgrade accelerate`).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1333\u001b[39m     )\n\u001b[32m   1335\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_map != \u001b[33m\"\u001b[39m\u001b[33msequential\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     inferred_max_memory = \u001b[43mget_balanced_memory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1338\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlow_zero\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbalanced_low_0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1340\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1341\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdevice_map_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1342\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1344\u001b[39m     inferred_max_memory = get_max_memory(max_memory)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:956\u001b[39m, in \u001b[36mget_balanced_memory\u001b[39m\u001b[34m(model, max_memory, no_split_module_classes, dtype, special_dtypes, low_zero)\u001b[39m\n\u001b[32m    954\u001b[39m \u001b[38;5;66;03m# Get default / clean up max_memory\u001b[39;00m\n\u001b[32m    955\u001b[39m user_not_set_max_memory = max_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m956\u001b[39m max_memory = \u001b[43mget_max_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_npu_available():\n\u001b[32m    959\u001b[39m     expected_device_type = \u001b[33m\"\u001b[39m\u001b[33mnpu\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:847\u001b[39m, in \u001b[36mget_max_memory\u001b[39m\u001b[34m(max_memory)\u001b[39m\n\u001b[32m    845\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m max_memory.keys():\n\u001b[32m    846\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_devices:\n\u001b[32m--> \u001b[39m\u001b[32m847\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    848\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDevice \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not recognized, available devices are integers(for GPU/XPU), \u001b[39m\u001b[33m'\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    849\u001b[39m         )\n\u001b[32m    850\u001b[39m max_memory = {k: max_memory[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m all_devices}\n\u001b[32m    852\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m max_memory\n",
      "\u001b[31mValueError\u001b[39m: Device None is not recognized, available devices are integers(for GPU/XPU), 'mps', 'cpu' and 'disk'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Start with 1.5B model (no quantization needed)\n",
    "print(\"Testing 1.5B model...\")\n",
    "manager = ModelManager(\n",
    "    model_id=CPU_MODEL_ID,\n",
    "    temperature=0.7,\n",
    "    top_k=40\n",
    ")\n",
    "\n",
    "prompt = \"hi?\"\n",
    "response = manager.generate(prompt)\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8ptuwXT9KU4"
   },
   "source": [
    "## OutputValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "JJoY6k4y7U-5"
   },
   "outputs": [],
   "source": [
    "class OutputValidator:\n",
    "    \"\"\"Validates and parses model outputs.\"\"\"\n",
    "\n",
    "    ALLOWED_CODES = {\"WCT\", \"GT\", \"Other\", \"NONE\"}\n",
    "    REQUIRED_KEYS = {\"CAD-code\", \"rationale\"}\n",
    "\n",
    "    @classmethod\n",
    "    def validate_and_parse(cls, text: str) -> Tuple[bool, Optional[Dict[str, Any]], Optional[str]]:\n",
    "        \"\"\"\n",
    "        Validates model output and parses JSON.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (is_valid, parsed_dict, error_message)\n",
    "        \"\"\"\n",
    "        text = text.strip()\n",
    "\n",
    "        # Try parsing the entire string as JSON\n",
    "        parsed = cls._extract_json(text)\n",
    "        if parsed is None:\n",
    "            return False, None, \"Could not extract valid JSON from response\"\n",
    "\n",
    "        # Validate structure\n",
    "        error = cls._validate_structure(parsed)\n",
    "        if error:\n",
    "            return False, None, error\n",
    "\n",
    "        return True, parsed, None\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_json(text: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Attempts to extract and parse JSON from text.\"\"\"\n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        except json.JSONDecodeError:\n",
    "            # Try to extract first {...} block\n",
    "            match = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
    "            if not match:\n",
    "                return None\n",
    "            try:\n",
    "                return json.loads(match.group(0))\n",
    "            except json.JSONDecodeError:\n",
    "                return None\n",
    "\n",
    "    @classmethod\n",
    "    def _validate_structure(cls, parsed: Dict[str, Any]) -> Optional[str]:\n",
    "        \"\"\"Validates the structure and content of parsed JSON.\"\"\"\n",
    "        # Check keys\n",
    "        if set(parsed.keys()) != cls.REQUIRED_KEYS:\n",
    "            return f\"Unexpected keys: {list(parsed.keys())}\"\n",
    "\n",
    "        # Validate code\n",
    "        code = parsed.get(\"CAD-code\")\n",
    "        if code not in cls.ALLOWED_CODES:\n",
    "            return f\"Invalid CAD-code: {code}\"\n",
    "\n",
    "        # Validate rationale length\n",
    "        rationale = parsed.get(\"rationale\", \"\")\n",
    "\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVWYL1BNN3b0"
   },
   "source": [
    "## Prompt Builder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "lj_6we2nDmmz"
   },
   "outputs": [],
   "source": [
    "class PromptBuilder:\n",
    "    \"\"\"Handles all prompt construction logic.\"\"\"\n",
    "\n",
    "    # JSON Schema definition\n",
    "    SCHEMA = {\n",
    "        \"CAD-code\": \"<ONE OF: WCT, GT, Other, NONE>\",\n",
    "        \"rationale\": \"<≤5 sentences, evidence-based>\"\n",
    "    }\n",
    "\n",
    "    # Valid codes for validation\n",
    "    VALID_CODES = {\"WCT\", \"GT\", \"Other\", \"NONE\"}\n",
    "\n",
    "    # Default examples for few-shot learning\n",
    "    DEFAULT_EXAMPLES = [\n",
    "        {\n",
    "            \"input\": \"Everybody please listen.\",\n",
    "            \"output\": {\n",
    "                \"CAD-code\": \"WCT\",\n",
    "                \"rationale\": 'Addresses the whole class using \"Everybody\" to get attention.'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Group 3, read the next paragraph.\",\n",
    "            \"output\": {\n",
    "                \"CAD-code\": \"GT\",\n",
    "                \"rationale\": 'Directs a specific group \"Group 3\" to perform an action.'\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    def __init__(self, name: str, personality: str, role: str, codebook: Dict[str, str], config: Dict[str, Any]):\n",
    "        self.name = name\n",
    "        self.personality = personality\n",
    "        self.role = role\n",
    "        self.codebook = codebook\n",
    "        self.config = config\n",
    "\n",
    "    def build_system_prompt(self, role: str) -> str:\n",
    "        \"\"\"Creates the system prompt with instructions and examples.\"\"\"\n",
    "        return (\n",
    "            f\"You are {self.name}, a {self.personality} qualitative-coding agent.\\n\"\n",
    "            f\"Task: {role}.\\n\\n\"\n",
    "            \"CRITICAL: Output ONLY a single JSON object.\\n\\n\"\n",
    "            \"REQUIREMENTS (follow exactly):\\n\"\n",
    "            \"1) Your ENTIRE response must be ONLY this JSON object and nothing else:\\n\"\n",
    "            f\"   {json.dumps(self.SCHEMA)}\\n\"\n",
    "            \"2) Use double quotes for JSON strings.\\n\"\n",
    "            \"3) CAD-code must be one of: WCT, GT, Other, NONE\\n\"\n",
    "            \"4) Rationale:  grounded in evidence from the text.\\n\"\n",
    "            \"5) If multiple codes could apply, choose the most likely one; if ambiguous, use NONE.\\n\\n\"\n",
    "            \"CORRECT OUTPUT EXAMPLES:\\n\"\n",
    "            'Input: \"Everybody please listen.\"\\n'\n",
    "            'Output: {\"CAD-code\":\"WCT\",\"rationale\":\"Addresses the whole class using \\\\\"Everybody\\\\\" to get attention.\"}\\n\\n'\n",
    "            'Input: \"Group 3, read the next paragraph.\"\\n'\n",
    "            'Output: {\"CAD-code\":\"GT\",\"rationale\":\"Directs a specific group \\\\\"Group 3\\\\\" to perform an action.\"}\\n\\n'\n",
    "            \"Remember: Output ONLY the JSON object. Start your response with { and end with }\"\n",
    "        )\n",
    "    # def build_system_prompt(self, role: str) -> str:\n",
    "    #     \"\"\"Creates the system prompt with instructions and examples.\"\"\"\n",
    "    #     return (\n",
    "    #         f\"You are {self.name}, a {self.personality} qualitative-coding agent.\\n\"\n",
    "    #         f\"Task: {role}.\\n\\n\"\n",
    "    #         \"Remember: Start your response with { and end with } \\n\\n\"\n",
    "    #     )\n",
    "    def build_context_prompt(self) -> str:\n",
    "        \"\"\"Creates the codebook context.\"\"\"\n",
    "        if not self.codebook:\n",
    "            return \"\"\n",
    "        lines = [f\"- {k}: {v}\" for k, v in self.codebook.items()]\n",
    "        return \"Codebook:\\n\" + \"\\n\".join(lines)\n",
    "\n",
    "    def build_user_prompt(self, text: str) -> str:\n",
    "        \"\"\"Creates the user prompt with the text to annotate.\"\"\"\n",
    "        template = self.config.get(\n",
    "            \"user_template\",\n",
    "            'text to code: \\n{text}\\n\\n'\n",
    "        )\n",
    "        return template.format(text=text)\n",
    "\n",
    "    def build_full_prompt(\n",
    "        self,\n",
    "        text: str,\n",
    "        role: str,\n",
    "        extra_context: Optional[str] = None,\n",
    "        previous_turn: Optional[str] = None\n",
    "    ) -> Dict[str, str]:\n",
    "        \"\"\"Builds complete prompt dictionary with all components.\"\"\"\n",
    "        prompt = {\n",
    "            \"system\": self.build_system_prompt(role),\n",
    "            \"context\": self.build_context_prompt(),\n",
    "            \"user\": self.build_user_prompt(text),\n",
    "        }\n",
    "\n",
    "        if extra_context:\n",
    "            prompt[\"extra\"] = extra_context\n",
    "\n",
    "        if previous_turn:\n",
    "            # Append previous turn to context\n",
    "            if prompt.get(\"context\"):\n",
    "                prompt[\"context\"] += f\"\\n\\n###\\nPrevious turn: {previous_turn}\"\n",
    "            else:\n",
    "                prompt[\"user\"] += f\"\\n\\n###\\nPrevious turn: {previous_turn}\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def build_retry_prompt(self, original_prompt: str, failed_output: str) -> str:\n",
    "        \"\"\"Builds a retry prompt when the model fails to produce valid JSON.\"\"\"\n",
    "        return (\n",
    "            f\"{original_prompt}\\n\\n\"\n",
    "            \"--- RETRY REQUEST ---\\n\"\n",
    "            \"Your previous output was invalid or incorrectly formatted.\\n\"\n",
    "            f\"Previous output:\\n{failed_output[:500]}\\n\\n\"\n",
    "            \"Please output ONLY a valid JSON object with this exact structure:\\n\"\n",
    "            f\"{json.dumps(self.SCHEMA)}\\n\\n\"\n",
    "            \"Requirements:\\n\"\n",
    "            \"- Start with { and end with }\\n\"\n",
    "            \"- No markdown, no explanations, no extra text\\n\"\n",
    "            f\"- CAD-code must be exactly one of: {', '.join(sorted(self.VALID_CODES))}\\n\"\n",
    "            \"- Use double quotes for strings\\n\\n\"\n",
    "            \"Return ONLY the JSON object now:\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def to_string(prompt_dict: Dict[str, str]) -> str:\n",
    "        \"\"\"Converts prompt dictionary to a formatted string.\"\"\"\n",
    "        parts = []\n",
    "        for key in [\"system\", \"context\", \"extra\", \"user\"]:\n",
    "            if prompt_dict.get(key):\n",
    "                parts.append(f\"=== {key.upper()} ===\\n{prompt_dict[key]}\")\n",
    "        return \"\\n\\n\".join(parts)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"String representation for debugging.\"\"\"\n",
    "        return (\n",
    "            f\"PromptBuilder(name={self.name}, \"\n",
    "            f\"personality={self.personality}, \"\n",
    "            f\"role={self.role}, \"\n",
    "            f\"codebook_size={len(self.codebook)}, \"\n",
    "            # f\"examples_count={len(self.examples)})\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhbGHJQbQ8MW"
   },
   "source": [
    "## BaseCodingAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "wEVWdrGpRAXM"
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Any, Union\n",
    "import torch\n",
    "import logging\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "class BaseCodingAgent:\n",
    "    \"\"\"Base class for coding agents with LLM-based text annotation.\"\"\"\n",
    "\n",
    "    ALLOWED_CODES = {\"WCT\", \"GT\", \"Other\", \"NONE\"}\n",
    "    DEFAULT_TEMPERATURE = 0.4  # Define constant\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        personality: str,\n",
    "        role: str,\n",
    "        model_id: Optional[str] = None,\n",
    "        device: Optional[str] = None,\n",
    "        temperature: float = DEFAULT_TEMPERATURE,\n",
    "        top_k: int = 40,\n",
    "        codebook: Optional[Dict[str, str]] = None,\n",
    "        config: Optional[Dict[str, Any]] = None,\n",
    "        debug: bool = False,\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.personality = personality\n",
    "        self.role = role\n",
    "        self.debug = debug\n",
    "\n",
    "        # Initialize components\n",
    "        self.validator = OutputValidator()\n",
    "        self.prompt_builder = PromptBuilder(\n",
    "            name, personality, role,\n",
    "            codebook or {}, config or {}\n",
    "        )\n",
    "        self.model_manager = ModelManager(\n",
    "            model_id, device, temperature, top_k\n",
    "        )\n",
    "\n",
    "        # Store options\n",
    "        self.options = {\"temperature\": temperature, \"top_k\": top_k}\n",
    "        self.codebook = codebook or {}\n",
    "        self.config = config or {}\n",
    "\n",
    "\n",
    "    # Main chat interface for the agent to interact with the language model\n",
    "    def chat(self, text: str, max_retries = int, role: Optional[str] = None, extra_context: Optional[str] = None, **gen_opts):\n",
    "        # Build prompt\n",
    "        prompt_str = self.get_prompt_str(\n",
    "            text=text,\n",
    "            role=role or self.role,\n",
    "            extra_context=extra_context,\n",
    "            **gen_opts\n",
    "        )\n",
    "\n",
    "        # Log if debug enabled\n",
    "        if self.debug:\n",
    "            logging.info(\"=== PROMPT ===\")\n",
    "            logging.info(prompt_str)\n",
    "\n",
    "        # Generate response\n",
    "        return self._call_and_retry(prompt_str, max_retries, **gen_opts)\n",
    "\n",
    "\n",
    "    def _call_and_retry(\n",
    "        self,\n",
    "        prompt_str: str,\n",
    "        max_retries: int,\n",
    "        **gen_opts\n",
    "    ) -> str:\n",
    "        \"\"\"Internal method to handle generation with retries.\"\"\"\n",
    "        try:\n",
    "            raw = self.model_manager.generate(prompt_str, **gen_opts)\n",
    "            valid, parsed, err = self.validator.validate_and_parse(raw)\n",
    "\n",
    "            if valid:\n",
    "                return raw\n",
    "\n",
    "            logger.debug(\"Initial attempt failed: %s. Raw: %s\", err, raw[:500])\n",
    "\n",
    "            # Retry with structured prompt\n",
    "            for attempt in range(max_retries):\n",
    "                retry_prompt = self.prompt_builder.build_retry_prompt(\n",
    "                    prompt_str, raw\n",
    "                )\n",
    "                logger.debug(\"Retry attempt %d/%d\", attempt + 1, max_retries)\n",
    "\n",
    "                raw = self.model_manager.generate(retry_prompt, **gen_opts)\n",
    "                valid, parsed, err = self.validator.validate_and_parse(raw)\n",
    "\n",
    "                if valid:\n",
    "                    return raw\n",
    "\n",
    "                logger.debug(\n",
    "                    \"Retry %d failed: %s. Raw: %s\",\n",
    "                    attempt + 1, err, raw[:500]\n",
    "                )\n",
    "\n",
    "            # Fallback to heuristic\n",
    "            logger.warning(\n",
    "                \"Model failed after %d retries. Using heuristic fallback.\",\n",
    "                max_retries\n",
    "            )\n",
    "            return json.dumps(self._heuristic_label(prompt_str))\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error during generation: %s\", e, exc_info=True)\n",
    "            return json.dumps({\n",
    "                \"CAD-code\": \"NONE\",\n",
    "                \"rationale\": f\"Error: {str(e)}\"\n",
    "            })\n",
    "\n",
    "    def _heuristic_label(self, text: str) -> Dict[str, str]:\n",
    "        \"\"\"Fallback heuristic labeling when model fails.\"\"\"\n",
    "        lower = (text or \"\").lower().strip()\n",
    "\n",
    "        if not lower:\n",
    "            return {\n",
    "                \"CAD-code\": \"NONE\",\n",
    "                \"rationale\": \"Empty input\"\n",
    "            }\n",
    "\n",
    "        # Check patterns in priority order\n",
    "        patterns = [\n",
    "            (r'\\b(everybody|everyone|class|students|all of you|all)\\b',\n",
    "             \"WCT\", \"whole-class addressing\"),\n",
    "            (r'\\b(group|pair|you two|you three)\\b',\n",
    "             \"GT\", \"group-level addressing\"),\n",
    "            (r'^[A-Z][a-z]+,', \"GT\", \"direct student address\"),\n",
    "        ]\n",
    "\n",
    "        for pattern, code, reason in patterns:\n",
    "            if re.search(pattern, text):\n",
    "                return {\"CAD-code\": code, \"rationale\": reason}\n",
    "\n",
    "        return {\n",
    "            \"CAD-code\": \"Other\",\n",
    "            \"rationale\": \"Non-directed teacher talk\"\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    def get_prompt_str(self, text: str, role: Optional[str] = None,extra_context: Optional[str] = None,**gen_opts):\n",
    "        # Build prompt\n",
    "        prompt_dict = self.prompt_builder.build_full_prompt(\n",
    "            text=text,\n",
    "            role=role or self.role,\n",
    "            extra_context = extra_context,\n",
    "            previous_turn=None,\n",
    "            **gen_opts\n",
    "        )\n",
    "\n",
    "        return self.prompt_builder.to_string(prompt_dict)\n",
    "\n",
    "\n",
    "    def get_agent_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return agent configuration as dictionary.\"\"\"\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"personality\": self.personality,\n",
    "            \"role\": self.role,\n",
    "            \"model\": self.model_manager.model_id,\n",
    "            \"device\": str(self.model_manager.device),\n",
    "            \"options\": self.options,\n",
    "            \"codebook\": self.codebook,\n",
    "            \"config\": self.config,\n",
    "            \"debug\": self.debug,\n",
    "        }\n",
    "\n",
    " # ---------------- Output validation & parsing ----------------\n",
    "    def validate_and_parse(self, text: str) -> Tuple[bool, Optional[Dict[str, Any]], Optional[str]]:\n",
    "        return self.validator.validate_and_parse(text)\n",
    "\n",
    "    def get_parsed_resp(self, text: str):\n",
    "        \"\"\"Parse response.\"\"\"\n",
    "        valid, parsed, err = self.validator.validate_and_parse(text)\n",
    "        return parsed if valid else None\n",
    "\n",
    "class SingleAgentCoding(BaseCodingAgent):\n",
    "    # Assigns a code to a given text based on the codebook and generates a rationale\n",
    "    def assign_code(self, text: str,\n",
    "                    max_retries: int = MAX_RETRIES,\n",
    "                    extra_context: Optional[str] = None,\n",
    "                    **gen_opts) -> str:\n",
    "        logging.debug(f\"Assigning code for text: {text}\") # Log the text being processed\n",
    "        # Call the chat method with a specific role for assigning codes\n",
    "        response = self.chat(\n",
    "                text=text,\n",
    "                role=self.role,\n",
    "                max_retries=max_retries,\n",
    "                extra_context=extra_context,\n",
    "                **gen_opts)\n",
    "        logging.debug(f\"Raw response from agent: {response}\") # Log the raw response from the agent\n",
    "\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gykK-rB9XfEw"
   },
   "source": [
    "## MAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_t6C1biPwZV"
   },
   "source": [
    "\n",
    "**High-level logic (fixed)**\n",
    "\n",
    "1.   Round 0: each agent labels independently.\n",
    "2.   List itemIf all agree → stop.\n",
    "3.   Else iterate discussion rounds:\n",
    "  *   compute majority (strict majority preferred)\n",
    "  *   if majority exists: reprompt only minority agents with majority + arguments\n",
    "  *   else: general discussion (everyone sees everyone’s arguments)\n",
    "\n",
    "4.  Stop early if: unanimous OR stable majority OR no changes OR reached T.\n",
    "\n",
    "5. Extract final answer via EXT.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFIxm5ZcEWGx"
   },
   "source": [
    "### AgentResponse and DiscussionResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "6QJDuOqJEO3j"
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Any, Union, Tuple, List\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass, field\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# from prompt_helper import PromptBuilder\n",
    "# from model_helper import ModelManager\n",
    "# from agent_helper import OutputValidator, SingleAgentCoding\n",
    "\n",
    "@dataclass\n",
    "class AgentResponse:\n",
    "    \"\"\"Represents a single agent's response in a discussion round.\n",
    "\n",
    "    Attributes:\n",
    "        agent: Name/identifier of the agent\n",
    "        code: CAD code assigned (e.g., 'WCT', 'GT', 'Other', 'NONE')\n",
    "        rationale: Agent's reasoning for the code choice\n",
    "        raw: Raw unparsed response from the agent\n",
    "        round: Round number (1-indexed)\n",
    "    \"\"\"\n",
    "    agent: str\n",
    "    code: str\n",
    "    rationale: str\n",
    "    raw: str\n",
    "    round: int\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate response data.\"\"\"\n",
    "        if self.round < 1:\n",
    "            raise ValueError(f\"Round must be >= 1, got {self.round}\")\n",
    "        if not self.agent:\n",
    "            raise ValueError(\"Agent name cannot be empty\")\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"Developer-friendly representation.\"\"\"\n",
    "        return (\n",
    "            f\"AgentResponse(agent={self.agent!r}, code={self.code!r}, \"\n",
    "            f\"round={self.round}, rationale={self.rationale[:50]!r}...)\"\n",
    "        )\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Human-readable representation.\"\"\"\n",
    "        return (\n",
    "            f\"Response from: \"\n",
    "            f\"Agent '{self.agent}' in Round {self.round}:\\n\"\n",
    "            f\"  Code: {self.code}\\n\"\n",
    "            f\"  Rationale: {self.rationale or '(none provided)'}\"\n",
    "            f\"  Raw: {self.raw}\"\n",
    "        )\n",
    "\n",
    "    def convert_to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert response to a dictionary.\"\"\"\n",
    "        return {\n",
    "            \"agent\": self.agent,\n",
    "            # \"text\": self.text,\n",
    "            \"code\": self.code,\n",
    "            \"rationale\": self.rationale,\n",
    "            \"raw\": self.raw,\n",
    "            \"round\": self.round,\n",
    "        }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DiscussionResult:\n",
    "    \"\"\"Results from a multi-agent discussion process.\n",
    "\n",
    "    Attributes:\n",
    "        final_code: Consensus or plurality code\n",
    "        final_rationale: Combined rationale from agents who chose final_code\n",
    "        confidence: Agreement ratio (0.0 to 1.0)\n",
    "        history: List of responses per round: history[round_idx] = [AgentResponse, ...]\n",
    "        tallies: Vote counts per round: tallies[round_idx] = {code: count, ...}\n",
    "        consensus_reached: Whether consensus threshold was met\n",
    "        num_rounds: Total number of rounds conducted\n",
    "    \"\"\"\n",
    "    text_to_code: str\n",
    "    human_code: str\n",
    "    final_code: str\n",
    "    final_rationale: str\n",
    "    confidence: float\n",
    "    history: List[List[AgentResponse]] = field(default_factory=list)\n",
    "    round_dicts: List[Dict[str, Any]] = field(default_factory=list)\n",
    "    tallies: List[Dict[str, int]] = field(default_factory=list)\n",
    "    consensus_reached: bool = False\n",
    "    num_rounds: int = 0\n",
    "    num_agents: int = 0\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate result data.\"\"\"\n",
    "        if not 0.0 <= self.confidence <= 1.0:\n",
    "            raise ValueError(f\"Confidence must be in [0, 1], got {self.confidence}\")\n",
    "        if self.num_rounds < 0:\n",
    "            raise ValueError(f\"num_rounds must be >= 0, got {self.num_rounds}\")\n",
    "        if self.history and len(self.history) != self.num_rounds:\n",
    "            raise ValueError(\n",
    "                f\"History length ({len(self.history)}) doesn't match \"\n",
    "                f\"num_rounds ({self.num_rounds})\"\n",
    "            )\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"Developer-friendly representation.\"\"\"\n",
    "        return (\n",
    "\n",
    "            f\"DiscussionResult(text_to_code={self.text_to_code!r}, \"\n",
    "            f\"human_code={self.human_code!r}, \"\n",
    "            f\"final_code={self.final_code!r}, \"\n",
    "            f\"confidence={self.confidence:.2f}, \"\n",
    "            f\"consensus={self.consensus_reached}, rounds={self.num_rounds})\"\n",
    "            f\"  History: {len(self.history)} rounds\"\n",
    "            f\"  Tallies: {len(self.tallies)} rounds\"\n",
    "        )\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Human-readable summary.\"\"\"\n",
    "        consensus_str = \"✓ Consensus\" if self.consensus_reached else \"✗ Plurality\"\n",
    "        return (\n",
    "            f\"Discussion Result:\\n\"\n",
    "            f\"  Text to Code: {self.text_to_code}\\n\"\n",
    "            f\"  Human Code: {self.human_code}\\n\"\n",
    "            f\"  consensus_str Result ({consensus_str}):\\n\"\n",
    "            f\"  Final Code: {self.final_code}\\n\"\n",
    "            f\"  Confidence: {self.confidence:.1%}\\n\"\n",
    "            f\"  Rounds: {self.num_rounds}\\n\"\n",
    "            f\"  Rationale: {self.final_rationale[:100]}...\"\n",
    "            f\"  History: {len(self.history)} rounds\"\n",
    "            f\"  Tallies: {len(self.tallies)} rounds\"\n",
    "        )\n",
    "    def __dict__(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert result to a dictionary.\"\"\"\n",
    "        return {\n",
    "            \"text_to_code\": self.text_to_code,\n",
    "            \"human_code\": self.human_code,\n",
    "            \"final_code\": self.final_code,\n",
    "            \"final_rationale\": self.final_rationale,\n",
    "            \"confidence\": self.confidence,\n",
    "            \"history\": self.history,\n",
    "            \"tallies\": self.tallies,\n",
    "            \"consensus_reached\": self.consensus_reached,\n",
    "            \"num_rounds\": self.num_rounds,\n",
    "            \"round_dicts\": self.get_round_dicts()\n",
    "        }\n",
    "    def get_num_agents(self) -> int:\n",
    "        \"\"\"Get number of participating agents.\n",
    "\n",
    "        Looks at the first round of discussion history to count agents.\n",
    "        If no history exists, returns 0.\n",
    "        \"\"\"\n",
    "        if self.history:\n",
    "            return len(self.history[0])\n",
    "        return 0\n",
    "\n",
    "    def display(self, verbose: bool = True) -> str:\n",
    "        \"\"\"Formatted display with optional round-by-round details.\n",
    "\n",
    "        Args:\n",
    "            verbose: Show detailed round-by-round breakdown\n",
    "\n",
    "        Returns:\n",
    "            Formatted string representation\n",
    "        \"\"\"\n",
    "        lines = [\n",
    "            f\"\\n{'='*70}\",\n",
    "            f\"DISCUSSION RESULT\",\n",
    "            f\"{'='*70}\",\n",
    "            f\"Final Code: {self.final_code}\",\n",
    "            f\"Confidence: {self.confidence:.1%} ({self.confidence * self.get_num_agents():.0f}/{self.get_num_agents()} agents)\",\n",
    "            f\"Consensus: {'✓ Yes' if self.consensus_reached else '✗ No (plurality vote)'}\",\n",
    "            f\"Rounds: {self.num_rounds}\",\n",
    "            f\"\\nFinal Rationale:\",\n",
    "            f\"{self.final_rationale or '(none provided)'}\",\n",
    "        ]\n",
    "\n",
    "        if verbose and self.history:\n",
    "            lines.append(f\"\\n{'-'*70}\")\n",
    "            lines.append(\"ROUND-BY-ROUND BREAKDOWN:\")\n",
    "            lines.append('-'*70)\n",
    "\n",
    "            for round_idx, (responses, tally) in enumerate(zip(self.history, self.tallies), 1):\n",
    "                lines.append(f\"\\n📍 Round {round_idx}:\")\n",
    "                lines.append(f\"   Votes: {dict(tally)}\")\n",
    "\n",
    "                for resp in responses:\n",
    "                    lines.append(f\"   • {resp.agent}: {resp.code}\")\n",
    "                    if resp.rationale:\n",
    "                        lines.append(f\"     → {resp.rationale[:80]}...\")\n",
    "\n",
    "        lines.append('='*70 + '\\n')\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "    def get_agent_journey(self, agent_name: str) -> List[AgentResponse]:\n",
    "        \"\"\"Track how a specific agent voted across rounds.\n",
    "\n",
    "        Args:\n",
    "            agent_name: Name of the agent to track\n",
    "\n",
    "        Returns:\n",
    "            List of AgentResponse objects for this agent, one per round\n",
    "        \"\"\"\n",
    "        journey = []\n",
    "        for round_responses in self.history:\n",
    "            for resp in round_responses:\n",
    "                if resp.agent == agent_name:\n",
    "                    journey.append(resp)\n",
    "                    break\n",
    "        return journey\n",
    "\n",
    "    def get_round_dicts(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Convert history to a list of dictionaries.\"\"\"\n",
    "        res = []\n",
    "        for round_idx, (responses, tally) in enumerate(zip(self.history, self.tallies), 1):\n",
    "            round_dict = {\n",
    "                \"round_num\": round_idx,\n",
    "                \"votes\": dict(tally),\n",
    "                \"responses\": [resp.convert_to_dict() for resp in responses]\n",
    "            }\n",
    "            res.append(round_dict)\n",
    "            self.round_dicts = res\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils for write_row_html_log during code run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import html\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "def _esc(x: Any) -> str:\n",
    "    return html.escape(\"\" if x is None else str(x))\n",
    "\n",
    "def _safe_pre(x: Any, max_len: int = 5000) -> str:\n",
    "    s = \"\" if x is None else str(x)\n",
    "    if len(s) > max_len:\n",
    "        s = s[:max_len] + \"\\n...(truncated)...\"\n",
    "    return html.escape(s)\n",
    "\n",
    "def _flatten_discussion_result(dr: \"DiscussionResult\") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Returns list of rows: one per agent per round.\n",
    "    Uses dr.get_round_dicts() (your JSON-friendly trace).\n",
    "    \"\"\"\n",
    "    flat = []\n",
    "    for rd in (dr.get_round_dicts() or []):\n",
    "        rnum = rd.get(\"round_num\")\n",
    "        votes = rd.get(\"votes\", {})\n",
    "        for resp in rd.get(\"responses\", []):\n",
    "            flat.append({\n",
    "                \"round\": rnum,\n",
    "                \"agent\": resp.get(\"agent\", \"\"),\n",
    "                \"code\": resp.get(\"code\", \"\"),\n",
    "                \"rationale\": resp.get(\"rationale\", \"\"),\n",
    "                \"raw\": resp.get(\"raw\", \"\"),\n",
    "                \"votes\": votes,\n",
    "            })\n",
    "    return flat\n",
    "\n",
    "def write_row_html_log(\n",
    "    out_dir: str,\n",
    "    idx: Any,\n",
    "    transcript: str,\n",
    "    dr: Optional[\"DiscussionResult\"] = None,\n",
    "    human_code: str = \"\",\n",
    "    error: str = \"\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Writes an HTML file for this row. Returns filepath.\n",
    "    \"\"\"\n",
    "    # make a out_dir name folder inside RESULTS_PATH\n",
    "    os.makedirs(os.path.join(RESULTS_PATH, out_dir), exist_ok=True)\n",
    "    fname = f\"row_{idx}.html\"\n",
    "    path = os.path.join(out_dir, fname)\n",
    "    print(f\"Folder created (or already exists) at: {os.path.join(RESULTS_PATH, out_dir)}\")\n",
    "\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    if error or dr is None:\n",
    "        html_doc = f\"\"\"<!doctype html>\n",
    "<html><head><meta charset=\"utf-8\"><title>Row { _esc(idx) } (ERROR)</title>\n",
    "<style>\n",
    "body{{font-family:system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif;margin:24px;line-height:1.35}}\n",
    ".card{{border:1px solid #ddd;border-radius:14px;padding:16px;margin:14px 0}}\n",
    "pre{{white-space:pre-wrap;word-wrap:break-word;background:#fafafa;padding:12px;border-radius:10px;border:1px solid #eee;margin:0}}\n",
    "</style></head><body>\n",
    "<h1>Row { _esc(idx) } (ERROR)</h1>\n",
    "<p style=\"color:#666\">Generated { _esc(now) }</p>\n",
    "<div class=\"card\"><h2>Transcript</h2><pre>{ _safe_pre(transcript) }</pre></div>\n",
    "<div class=\"card\"><h2>Error</h2><pre>{ _safe_pre(error) }</pre></div>\n",
    "</body></html>\"\"\"\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(html_doc)\n",
    "        return path\n",
    "\n",
    "    flat = _flatten_discussion_result(dr)\n",
    "\n",
    "    # Table rows\n",
    "    trs = []\n",
    "    for row in flat:\n",
    "        votes_json = json.dumps(row[\"votes\"], ensure_ascii=False)\n",
    "        trs.append(\n",
    "            \"<tr>\"\n",
    "            f\"<td>{_esc(row['round'])}</td>\"\n",
    "            f\"<td><b>{_esc(row['agent'])}</b></td>\"\n",
    "            f\"<td><b>{_esc(row['code'])}</b></td>\"\n",
    "            f\"<td style='white-space:pre-wrap'>{_esc(row['rationale'])}</td>\"\n",
    "            f\"<td><details><summary>votes</summary><pre style='white-space:pre-wrap'>{_safe_pre(votes_json, max_len=2000)}</pre></details></td>\"\n",
    "            f\"<td><details><summary>raw</summary><pre style='white-space:pre-wrap'>{_safe_pre(row['raw'])}</pre></details></td>\"\n",
    "            \"</tr>\"\n",
    "        )\n",
    "    body_rows = \"\\n\".join(trs) if trs else \"<tr><td colspan='6'>(No trace)</td></tr>\"\n",
    "\n",
    "    # Agreement badge\n",
    "    agree = (human_code.strip() != \"\" and human_code == dr.final_code)\n",
    "    badge = \"✅\" if agree else (\"❌\" if human_code.strip() else \"—\")\n",
    "\n",
    "    html_doc = f\"\"\"<!doctype html>\n",
    "<html><head><meta charset=\"utf-8\"><title>Row { _esc(idx) }</title>\n",
    "<style>\n",
    "body{{font-family:system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif;margin:24px;line-height:1.35}}\n",
    "h1{{margin:0 0 8px 0}}\n",
    ".sub{{color:#666;margin-bottom:18px}}\n",
    ".card{{border:1px solid #ddd;border-radius:14px;padding:16px;margin:14px 0}}\n",
    ".grid{{display:grid;grid-template-columns:180px 1fr;gap:8px 12px}}\n",
    ".label{{color:#555}}\n",
    "pre{{white-space:pre-wrap;word-wrap:break-word;background:#fafafa;padding:12px;border-radius:10px;border:1px solid #eee;margin:0}}\n",
    "table{{border-collapse:collapse;width:100%}}\n",
    "th,td{{border:1px solid #ddd;padding:10px;vertical-align:top}}\n",
    "th{{background:#f5f5f5;text-align:left}}\n",
    ".pill{{display:inline-block;padding:2px 10px;border:1px solid #ddd;border-radius:999px;font-size:12px}}\n",
    "</style></head><body>\n",
    "\n",
    "<h1>{badge} Row {_esc(idx)}</h1>\n",
    "<div class=\"sub\">Generated {_esc(now)}</div>\n",
    "\n",
    "<div class=\"card\">\n",
    "  <h2>Transcript</h2>\n",
    "  <pre>{_safe_pre(transcript)}</pre>\n",
    "</div>\n",
    "\n",
    "<div class=\"card\">\n",
    "  <h2>Final decision</h2>\n",
    "  <div class=\"grid\">\n",
    "    <div class=\"label\">final_code</div><div><span class=\"pill\">{_esc(dr.final_code)}</span></div>\n",
    "    <div class=\"label\">confidence</div><div>{_esc(dr.confidence)}</div>\n",
    "    <div class=\"label\">consensus_reached</div><div>{_esc(dr.consensus_reached)}</div>\n",
    "    <div class=\"label\">num_rounds</div><div>{_esc(dr.num_rounds)}</div>\n",
    "    <div class=\"label\">human_code</div><div>{_esc(human_code)}</div>\n",
    "    <div class=\"label\">final_rationale</div><div style=\"white-space:pre-wrap\">{_esc(dr.final_rationale)}</div>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div class=\"card\">\n",
    "  <h2>Rounds (agent × round)</h2>\n",
    "  <table>\n",
    "    <tr><th>Round</th><th>Agent</th><th>Code</th><th>Rationale</th><th>Votes</th><th>Raw</th></tr>\n",
    "    {body_rows}\n",
    "  </table>\n",
    "</div>\n",
    "\n",
    "</body></html>\"\"\"\n",
    "\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html_doc)\n",
    "\n",
    "    return path\n",
    "\n",
    "def write_index_html(out_dir: str, row_files: List[str], title: str = \"Discussion Logs\") -> str:\n",
    "    \"\"\"\n",
    "    Writes an index.html that links to all row files.\n",
    "    row_files should be file *names* or relative paths within out_dir.\n",
    "    \"\"\"\n",
    "    path = os.path.join(out_dir, \"index.html\")\n",
    "    links = \"\\n\".join([f\"<li><a href='{html.escape(os.path.basename(p))}'>{html.escape(os.path.basename(p))}</a></li>\"\n",
    "                       for p in row_files])\n",
    "    doc = f\"\"\"<!doctype html>\n",
    "<html><head><meta charset=\"utf-8\"><title>{html.escape(title)}</title>\n",
    "<style>\n",
    "body{{font-family:system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif;margin:24px}}\n",
    "li{{margin:6px 0}}\n",
    "</style></head><body>\n",
    "<h1>{html.escape(title)}</h1>\n",
    "<ul>\n",
    "{links}\n",
    "</ul>\n",
    "</body></html>\"\"\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(doc)\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NF3fymO3EPVQ"
   },
   "source": [
    "### MAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "55c2M-yjutT_"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DiscussionConfig:\n",
    "    \"\"\"Configuration for multi-agent discussion behavior.\"\"\"\n",
    "    max_rounds: int = 3\n",
    "    consensus_threshold: float = 0.9\n",
    "    max_retries_per_agent: int = 3\n",
    "    allowed_codes: frozenset = field(\n",
    "        default_factory=lambda: frozenset({\"WCT\", \"GT\", \"Other\", \"NONE\"})\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate configuration parameters.\"\"\"\n",
    "        if self.max_rounds < 1:\n",
    "            raise ValueError(\"max_rounds must be at least 1\")\n",
    "        if not 0 < self.consensus_threshold <= 1:\n",
    "            raise ValueError(\"consensus_threshold must be in (0, 1]\")\n",
    "        if self.max_retries_per_agent < 1:\n",
    "            raise ValueError(\"max_retries_per_agent must be at least 1\")\n",
    "        if not self.allowed_codes:\n",
    "            raise ValueError(\"allowed_codes cannot be empty\")\n",
    "\n",
    "class MultiAgentDiscussion:\n",
    "    \"\"\"\n",
    "    Draft multi-agent version\n",
    "\n",
    "    Assumptions (editable):\n",
    "    - N agents.\n",
    "    - Round 1: all agents independently propose a code.\n",
    "    - If unanimous: return immediately.\n",
    "    - Else if a strict majority exists: re-prompt only minority agents (they see majority label + optional rationales).\n",
    "    - Else (tie / no majority): run a discussion round where each agent sees a compact transcript of others.\n",
    "    - Missing-code outputs are re-tried per-agent per-round\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 agents: List[Any],\n",
    "                 config: Optional[DiscussionConfig] = None):\n",
    "\n",
    "        # Validate inputs\n",
    "        if not agents:\n",
    "            raise ValueError(\"At least one agent is required\")\n",
    "\n",
    "        # Initialize configuration\n",
    "        self.config = config or DiscussionConfig()\n",
    "\n",
    "        self.agents = agents\n",
    "        self.round_num = 0\n",
    "        self.prev_responses = None\n",
    "        self.history = []\n",
    "        self.tallies = []\n",
    "        self.consensus_reached = False\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.store_trace = True\n",
    "\n",
    "        self.max_rounds = self.config.max_rounds\n",
    "        self.threshold = self.config.consensus_threshold\n",
    "        self.allowed_codes = self.config.allowed_codes\n",
    "\n",
    "    def _top_non_none(self, items, allowed_codes) -> Tuple[Optional[str], int, int]:\n",
    "        \"\"\"\n",
    "        - validates codes\n",
    "        - ignores \"NONE\"\n",
    "        - returns (top_code, top_count, total_n)\n",
    "        \"\"\"\n",
    "        # Normalize input into a list of codes\n",
    "        if isinstance(items, dict):\n",
    "            codes = []\n",
    "            for code, count in items.items():\n",
    "                codes.extend([code] * int(count))  # expand tally into votes\n",
    "        else:\n",
    "            codes = list(items)\n",
    "\n",
    "        total_n = len(codes)\n",
    "        if total_n == 0:\n",
    "            return None, 0, 0\n",
    "\n",
    "        # Validate codes (fail fast)\n",
    "        invalid = set(codes) - allowed_codes\n",
    "        if invalid:\n",
    "            self.logger.error(f\"Invalid codes encountered: {invalid}\")\n",
    "            return None, 0, total_n\n",
    "\n",
    "        # Drop NONE votes\n",
    "        valid = [c for c in codes if c != \"NONE\"]\n",
    "        if not valid:\n",
    "            return None, 0, total_n\n",
    "\n",
    "        top_code, top_count = Counter(valid).most_common(1)[0]\n",
    "        return top_code, top_count, total_n\n",
    "\n",
    "    def is_consensus_reached(self, tally: Dict[str, int], threshold: float = 0.8) -> Tuple[Optional[str], float]:\n",
    "        \"\"\"\n",
    "        Determines if consensus is reached among agents.\n",
    "\n",
    "        Args:\n",
    "            tally: Dictionary mapping code to count of agents who chose that code\n",
    "            threshold: Minimum agreement ratio required for consensus (default: 0.8 = 80%)\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (code, agreement_ratio) if consensus reached, else (None, 0.0)\n",
    "            - code: The consensus code if agreement >= threshold\n",
    "            - agreement_ratio: Proportion of total agents agreeing on the top code\n",
    "        \"\"\"\n",
    "        top_code, top_count, total_n = self._top_non_none(tally, self.allowed_codes)\n",
    "        if top_code is None or total_n == 0:\n",
    "            return None, 0.0\n",
    "\n",
    "        agreement_ratio = top_count / total_n\n",
    "        if agreement_ratio >= threshold:\n",
    "            self.logger.info(f\"Consensus reached on code '{top_code}' with agreement {agreement_ratio:.2f}\")\n",
    "            return top_code, agreement_ratio\n",
    "\n",
    "        self.logger.info(f\"No consensus: top code '{top_code}' has agreement {agreement_ratio:.2f}\")\n",
    "        return None, agreement_ratio\n",
    "\n",
    "    def get_majority_vote(self, codes: List[str]) -> Union[str, int]:\n",
    "        \"\"\"\n",
    "        Get majority vote from list of codes.\n",
    "\n",
    "        Args:\n",
    "            codes: List of code strings\n",
    "\n",
    "        Returns:\n",
    "            Majority code if strict majority exists (>50%), else -1\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If invalid codes are present\n",
    "        \"\"\"\n",
    "        top_code, top_count, total_n = self._top_non_none(codes, self.config.allowed_codes)\n",
    "        if not top_code or total_n == 0:\n",
    "            return -1\n",
    "\n",
    "        return top_code if top_count > (total_n / 2) else -1\n",
    "\n",
    "    def _finalize(self, last_round_responses: List[AgentResponse]) -> Tuple[str, str, float]:\n",
    "        \"\"\"\n",
    "        Finalize the discussion by plurality vote.\n",
    "        Returns:\n",
    "            - final_code: str\n",
    "            - final_rationale: str\n",
    "            - confidence: float\n",
    "        \"\"\"\n",
    "        codes = [resp.code for resp in last_round_responses]\n",
    "\n",
    "        # sanity check (fail fast instead of silently lying)\n",
    "        invalid = set(codes) - self.allowed_codes\n",
    "        if invalid:\n",
    "            raise ValueError(f\"Invalid codes encountered: {invalid}\")\n",
    "\n",
    "        counts = Counter(codes)\n",
    "        total_n = len(codes)\n",
    "        if not counts:\n",
    "            return \"NONE\", \"\", 0.0\n",
    "\n",
    "        final_code, freq = counts.most_common(1)[0]\n",
    "        confidence = freq / len(self.agents)\n",
    "\n",
    "        # Aggregate rationales for the final code\n",
    "        rationales = [resp.rationale for resp in last_round_responses\n",
    "                      if resp.code == final_code and resp.rationale]\n",
    "        final_rationale = \" | \".join(rationales)\n",
    "\n",
    "        return final_code, final_rationale, confidence\n",
    "\n",
    "    def _build_simple_discussion_context(self, prev_responses: Optional[List[AgentResponse]],\n",
    "                                  round_num: int) -> str:\n",
    "        \"\"\"\n",
    "        Build human-readable discussion context from previous round.\n",
    "\n",
    "        This provides agents with:\n",
    "        - Clear round indicator\n",
    "        - All previous agent positions and rationales (full transparency)\n",
    "        - Vote distribution summary\n",
    "        - Majority position if exists\n",
    "        - Instruction to reconsider\n",
    "\n",
    "        Args:\n",
    "            prev_round: Previous round's agent responses (None for round 1)\n",
    "            round_num: Current round number\n",
    "\n",
    "        Returns:\n",
    "            Formatted string context for agents to consider\n",
    "        \"\"\"\n",
    "        if not prev_responses:\n",
    "            return f\"Round 1: Provide your independent assessment.\"\n",
    "\n",
    "        # Start building context\n",
    "        lines = [f\"Round {round_num}: Previous round responses:\", \"\"]\n",
    "\n",
    "        # Show each agent's position and rationale\n",
    "        for resp in prev_responses:\n",
    "            rationale = resp.rationale if resp.rationale else \"(no rationale provided)\"\n",
    "            lines.append(f\"- {resp.agent} chose '{resp.code}': {rationale}\")\n",
    "\n",
    "        lines.append(\"\")  # Blank line for readability\n",
    "\n",
    "        # Add vote distribution summary\n",
    "        codes = [r.code for r in prev_responses if r.code != \"NONE\"]\n",
    "        if codes:\n",
    "            tally = Counter(codes)\n",
    "            tally_str = \", \".join(f\"{code}: {count}\" for code, count in tally.most_common())\n",
    "            lines.append(f\"Vote distribution: {tally_str}\")\n",
    "\n",
    "            # Highlight majority if exists\n",
    "            majority = self.get_majority_vote([r.code for r in prev_responses])\n",
    "            if majority != -1:\n",
    "                lines.append(f\"Majority position: {majority}\")\n",
    "\n",
    "        # Instruction to reconsider\n",
    "        lines.extend([\n",
    "            \"\",\n",
    "            \"Consider the above responses. You may:\",\n",
    "            \"- Change your assessment if you find others' reasoning convincing\",\n",
    "            \"- Maintain your position if you believe your reasoning is stronger\",\n",
    "            \"- Provide additional rationale to explain your choice\"\n",
    "        ])\n",
    "\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"\n",
    "        Reset discussion state for reuse.\n",
    "\n",
    "        Clears all round history, tallies, and consensus flags.\n",
    "        Useful for running multiple discussions with the same agent pool.\n",
    "        \"\"\"\n",
    "        self.round_num = 0\n",
    "        self.history.clear()\n",
    "        self.tallies.clear()\n",
    "        self.consensus_reached = False\n",
    "        self.logger.debug(\"Discussion state reset\")\n",
    "\n",
    "    def _to_agent_response(self, agent: Any, raw: str, round_num: int) -> AgentResponse:\n",
    "        \"\"\"\n",
    "        Convert raw agent output into AgentResponse using agent.validate_and_parse().\n",
    "        Keeps invalid outputs as NONE with an error rationale.\n",
    "        \"\"\"\n",
    "        valid, parsed, err = agent.validate_and_parse(raw)\n",
    "\n",
    "        if not valid or not isinstance(parsed, dict):\n",
    "            code = \"NONE\"\n",
    "            rationale = f\"Parse error/invalid format: {err}\"\n",
    "        else:\n",
    "            code = parsed.get(\"CAD-code\", \"NONE\")\n",
    "            rationale = parsed.get(\"rationale\", \"\") or \"\"\n",
    "\n",
    "        code = self._validate_code(code)\n",
    "\n",
    "        return AgentResponse(\n",
    "            agent=getattr(agent, \"name\", str(agent)),\n",
    "            code=code,\n",
    "            rationale=rationale,\n",
    "            raw=raw,\n",
    "            round=round_num,\n",
    "        )\n",
    "    def _validate_code(self, code: str) -> str:\n",
    "        if code not in self.allowed_codes:\n",
    "            self.logger.warning(f\"Invalid code received: {code}\")\n",
    "            return \"NONE\"\n",
    "        return code\n",
    "    def _tally_round(self, round_responses: List[AgentResponse]) -> Dict[str, int]:\n",
    "        \"\"\"Count votes for a single round.\"\"\"\n",
    "        return dict(Counter(r.code for r in round_responses))\n",
    "\n",
    "    ############### Discussion funcs ##################\n",
    "    def discuss(self, text: str, human_code: str = \"\", **kwargs) -> DiscussionResult:\n",
    "        \"\"\"\n",
    "        Debate with consensus.\n",
    "        \"\"\"\n",
    "        self.reset()  # avoid leaking history across calls\n",
    "        self.logger.info(\"== Starting MultiAgentDiscussion with %d agents\", len(self.agents))\n",
    "        self.text = text\n",
    "\n",
    "        for round_idx in range(self.config.max_rounds):\n",
    "            self.round_num = round_idx + 1\n",
    "            self.logger.info(\"Round %d/%d\", self.round_num, self.config.max_rounds)\n",
    "\n",
    "            # Agents see previous round responses as context (None on round 1)\n",
    "            prev_resp = self.history[-1] if self.history else None\n",
    "            ctx = self._build_simple_discussion_context(prev_resp, self.round_num)\n",
    "\n",
    "            # Collect responses for this round\n",
    "            round_responses: List[AgentResponse] = []\n",
    "            tally: Dict[str, int] = {}\n",
    "            for agent in self.agents:\n",
    "              max_retries = self.config.max_retries_per_agent if self.config.max_retries_per_agent else 1\n",
    "\n",
    "              raw = agent.assign_code(text, extra_context=ctx, max_retries = max_retries, **kwargs)\n",
    "              # Parse and validate response using the agent's validate_and_parse method\n",
    "              round_responses.append(self._to_agent_response(agent, raw, self.round_num))\n",
    "              # update vote counts for this round\n",
    "\n",
    "            # Save round artifacts\n",
    "            self.history.append(round_responses)\n",
    "            tally = self._tally_round(round_responses)\n",
    "            self.tallies.append(tally)\n",
    "            self.logger.debug(f\"Round {self.round_num} tally: {tally}\")\n",
    "\n",
    "            # stop early if consensus reached\n",
    "            consensus_code, agreement = self.is_consensus_reached(tally, self.config.consensus_threshold)\n",
    "            if consensus_code:\n",
    "                self.consensus_reached = True\n",
    "                rationales = [resp.rationale for resp in round_responses\n",
    "                            if resp.code == consensus_code and resp.rationale]\n",
    "\n",
    "                return DiscussionResult(\n",
    "                    text_to_code=text,\n",
    "                    human_code=human_code,\n",
    "                    final_code=consensus_code,\n",
    "                    final_rationale=\" | \".join(rationales),\n",
    "                    confidence=agreement,\n",
    "                    history=self.history,\n",
    "                    tallies=self.tallies,\n",
    "                    consensus_reached=True,\n",
    "                    num_rounds=self.round_num\n",
    "                )\n",
    "\n",
    "        # Max rounds reached without consensus - finalize by plurality\n",
    "        final_code, final_rationale, confidence = self._finalize(self.history[-1])\n",
    "\n",
    "        return DiscussionResult(\n",
    "            text_to_code=text,\n",
    "            human_code=human_code,\n",
    "            final_code=final_code,\n",
    "            final_rationale=final_rationale,\n",
    "            confidence=confidence,\n",
    "            history=self.history,\n",
    "            tallies=self.tallies,\n",
    "            consensus_reached=False,\n",
    "            num_rounds=self.round_num\n",
    "        )\n",
    "\n",
    "    def run_batch_discussions(\n",
    "        self,\n",
    "        data_df: pd.DataFrame,\n",
    "        text_col: str = \"transcript\",\n",
    "        store_traces: bool = True,\n",
    "        batch_num: int = -1,\n",
    "        log_every: int = 10,\n",
    "        save_name: Optional[str] = None,   # base path without extension\n",
    "        save_every: int = 50,\n",
    "        stop_on_error: bool = False,\n",
    "        **kwargs\n",
    "    ) -> Tuple[List[DiscussionResult], pd.DataFrame]:\n",
    "\n",
    "      if text_col not in data_df.columns:\n",
    "        raise ValueError(f\"Column '{text_col}' not found. Available: {list(data_df.columns)}\")\n",
    "\n",
    "      # How many rows?\n",
    "      total = len(data_df)\n",
    "      n = total if (batch_num is None or int(batch_num) < 0) else min(int(batch_num), total)\n",
    "\n",
    "      idxs = list(data_df.index[:n])\n",
    "      texts = data_df.loc[idxs, text_col].tolist()\n",
    "\n",
    "      results: List[\"DiscussionResult\"] = []\n",
    "      rows: List[Dict[str, Any]] = []  # for output df\n",
    "      row_html_files = []\n",
    "\n",
    "      # FIXED: Added human_code_val argument to signature\n",
    "      def to_row(idx, text, human_code_val, result: Optional[\"DiscussionResult\"], error: str) -> Dict[str, Any]:\n",
    "        \"\"\"One output row (success or failure).\"\"\"\n",
    "        if error:\n",
    "            return {\n",
    "                \"row_index\": idx,\n",
    "                \"text_to_code\": text,\n",
    "                \"human_code\": human_code_val,\n",
    "                \"final_code\": \"NONE\",\n",
    "                \"final_rationale\": \"\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"consensus_reached\": False,\n",
    "                \"num_rounds\": 0,\n",
    "                \"tallies\": [],\n",
    "                \"round_dicts\": [],\n",
    "                \"error\": error,\n",
    "            }\n",
    "\n",
    "        assert result is not None\n",
    "        round_dicts = []\n",
    "        tallies = []\n",
    "        if store_traces:\n",
    "            try:\n",
    "                round_dicts = result.get_round_dicts()\n",
    "            except Exception as e:\n",
    "                self.logger.warning(\"Trace serialization failed at row %s: %s\", idx, e)\n",
    "                round_dicts = []\n",
    "            tallies = result.tallies\n",
    "\n",
    "        return {\n",
    "            \"row_index\": idx,\n",
    "            \"text_to_code\": result.text_to_code,\n",
    "            \"human_code\": human_code_val,\n",
    "            \"final_code\": result.final_code,\n",
    "            \"final_rationale\": result.final_rationale,\n",
    "            \"confidence\": result.confidence,\n",
    "            \"consensus_reached\": result.consensus_reached,\n",
    "            \"num_rounds\": result.num_rounds,\n",
    "            \"tallies\": tallies,\n",
    "            \"round_dicts\": round_dicts,\n",
    "            \"error\": \"\",\n",
    "        }\n",
    "\n",
    "      def checkpoint(reason: str) -> None:\n",
    "          \"\"\"Write a CSV checkpoint (no-op if save_name is None).\"\"\"\n",
    "          if not save_name:\n",
    "              return\n",
    "          save_CSV_path = os.path.join(RESULTS_PATH, f\"{save_name}.csv\")\n",
    "          out_df = pd.DataFrame(rows, index=[r[\"row_index\"] for r in rows])\n",
    "          out_df.to_csv(f\"{save_name}.csv\", index=True)\n",
    "          self.logger.info(\"Checkpoint (%s): wrote %d rows -> %s.csv\", reason, len(out_df), save_name)\n",
    "\n",
    "      self.logger.info(\n",
    "        \"Batch start: %d/%d rows | text_col='%s' | traces=%s | checkpoints=%s\",\n",
    "        n, total, text_col, store_traces, (\"on\" if save_name else \"off\")\n",
    "      )\n",
    "\n",
    "      success = 0\n",
    "      errors = 0\n",
    "\n",
    "      for i, (idx, text) in enumerate(tqdm(list(zip(idxs, texts)), desc=\"Processing\", total=n), start=1):\n",
    "          human_code = str(data_df.loc[idx, \"CAD\"]) if \"CAD\" in data_df.columns else \"\"\n",
    "          try:\n",
    "              res = self.discuss(text, human_code=human_code, **kwargs) # Pass human_code here\n",
    "\n",
    "              if not store_traces:\n",
    "                  res = DiscussionResult(\n",
    "                      text_to_code=res.text_to_code,\n",
    "                      human_code=human_code,\n",
    "                      final_code=res.final_code,\n",
    "                      final_rationale=res.final_rationale,\n",
    "                      confidence=res.confidence,\n",
    "                      history=[],\n",
    "                      tallies=[],\n",
    "                      consensus_reached=res.consensus_reached,\n",
    "                      num_rounds=res.num_rounds,\n",
    "                  )\n",
    "\n",
    "              html_path = write_row_html_log(\n",
    "                  out_dir=\"discussion_html\",\n",
    "                  idx=idx,\n",
    "                  transcript=text,\n",
    "                  dr=res,\n",
    "                  human_code=human_code\n",
    "              )\n",
    "              row_html_files.append(html_path)\n",
    "\n",
    "              results.append(res)\n",
    "              rows.append(to_row(idx, text, human_code, res, error=\"\"))\n",
    "              success += 1\n",
    "\n",
    "          except Exception as e:\n",
    "              err = f\"{type(e).__name__}: {str(e)[:500]}\"\n",
    "              self.logger.exception(\"Row %s failed (%d/%d): %s\", idx, i, n, err)\n",
    "\n",
    "              placeholder = DiscussionResult(\n",
    "                  text_to_code=str(text),\n",
    "                  human_code=human_code,\n",
    "                  final_code=\"NONE\",\n",
    "                  final_rationale=\"\",\n",
    "                  confidence=0.0,\n",
    "                  history=[],\n",
    "                  tallies=[],\n",
    "                  consensus_reached=False,\n",
    "                  num_rounds=0,\n",
    "              )\n",
    "              results.append(placeholder)\n",
    "              rows.append(to_row(idx, text, human_code, None, error=err))\n",
    "              errors += 1\n",
    "\n",
    "              if save_name:\n",
    "                  checkpoint(reason=f\"error_at_{idx}\")\n",
    "\n",
    "              if stop_on_error:\n",
    "                  break\n",
    "\n",
    "          if log_every and (i % log_every == 0 or i == n):\n",
    "              rate = (success / i) * 100 if i else 0.0\n",
    "              self.logger.info(\"Progress: %d/%d | success=%d | errors=%d | rate=%.1f%%\", i, n, success, errors, rate)\n",
    "\n",
    "          if save_every and save_name and (i % save_every == 0):\n",
    "              checkpoint(reason=f\"periodic_{i}\")\n",
    "\n",
    "      write_index_html(\"discussion_html\", row_html_files, title=\"MAD / MultiAgentDiscussion Logs\")\n",
    "      if save_name:\n",
    "          checkpoint(reason=\"final\")\n",
    "\n",
    "      output_df = pd.DataFrame(rows, index=[r[\"row_index\"] for r in rows])\n",
    "      self.logger.info(\"Batch done: success=%d errors=%d total=%d\", success, errors, len(output_df))\n",
    "      return results, output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76raXMBIVBYI"
   },
   "source": [
    "## Experiment run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9k0ulr_KHPL"
   },
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "OhdMlylh518H"
   },
   "outputs": [],
   "source": [
    "balenced_role = \"Your job is to weigh evidence, reconcile disagreements, and enforce codebook fidelity.\"\n",
    "adversery_role = \"Rigorous prosecutor. Be skeptical. Demand direct textual evidence (quote a short phrase). Actively try to falsify other agents’ codes. If the text is ambiguous, say so and propose a safe fallback.\"\n",
    "creative_role = \"Creative empathic explorer. Look for subtle intent, context, and edge cases. Propose alternative readings and uncommon-but-plausible codes, but justify with text evidence.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ZbdjJqKlWvvJ"
   },
   "outputs": [],
   "source": [
    "MAD_config=DiscussionConfig(\n",
    "        max_rounds=3,\n",
    "        consensus_threshold=0.9,\n",
    "        max_retries_per_agent=2,\n",
    "        allowed_codes={\"WCT\", \"GT\", \"Other\", \"NONE\"}\n",
    "      )\n",
    "\n",
    "text = \"So remember you guys are in groups so talk to your partner about the cards you move. Make sure your partner agrees with you.\"\n",
    "\n",
    "# --- Agents ---\n",
    "agents = {\n",
    "    \"a1\": SingleAgentCoding(\"Ava\", \"balanced arbiter\", balenced_role, debug=False, codebook=CAD_CODEBOOK_DICT),\n",
    "    \"a2\": SingleAgentCoding(\"Ben\", \"rigorous and concise\", adversery_role, debug=False, codebook=CAD_CODEBOOK_DICT),\n",
    "    \"a3\": SingleAgentCoding(\"Cam\", \"creative and empathic\", creative_role, debug=False, codebook=CAD_CODEBOOK_DICT)\n",
    "}\n",
    "\n",
    "\n",
    "mad = MultiAgentDiscussion(\n",
    "    list(agents.values()),\n",
    "    config=MAD_config\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 1.5B model...\n",
      "2026-01-21 15:09:15,113 - INFO - Loading model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "2026-01-21 15:09:16,569 - INFO - Loading with 4-bit quantization\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Device None is not recognized, available devices are integers(for GPU/XPU), 'mps', 'cpu' and 'disk'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      6\u001b[39m manager = ModelManager(\n\u001b[32m      7\u001b[39m     model_id=CPU_MODEL_ID,\n\u001b[32m      8\u001b[39m     temperature=\u001b[32m0.7\u001b[39m,\n\u001b[32m      9\u001b[39m     top_k=\u001b[32m40\u001b[39m\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m prompt = \u001b[33m\"\u001b[39m\u001b[33mWhat is Python?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m response = \u001b[43mmanager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResponse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 123\u001b[39m, in \u001b[36mModelManager.generate\u001b[39m\u001b[34m(self, prompt, temperature, top_k)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\n\u001b[32m    117\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    118\u001b[39m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    119\u001b[39m     temperature: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    120\u001b[39m     top_k: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    121\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    122\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generates text from the model given a prompt.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m     temp = temperature \u001b[38;5;28;01mif\u001b[39;00m temperature \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.temperature\n\u001b[32m    125\u001b[39m     tk = top_k \u001b[38;5;28;01mif\u001b[39;00m top_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.top_k\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mModelManager.load_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     75\u001b[39m     model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mmax_memory\u001b[39m\u001b[33m\"\u001b[39m] = {\u001b[38;5;28mself\u001b[39m.device.index: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.max_memory_gb\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mGB\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# ########## quantization ##########\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[38;5;28mself\u001b[39m._model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m._model.eval()\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Log memory after loading if in debug\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:5029\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5027\u001b[39m \u001b[38;5;66;03m# Prepare the full device map\u001b[39;00m\n\u001b[32m   5028\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5029\u001b[39m     device_map = \u001b[43m_get_device_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5031\u001b[39m \u001b[38;5;66;03m# Finalize model weight initialization\u001b[39;00m\n\u001b[32m   5032\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m from_tf:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:1336\u001b[39m, in \u001b[36m_get_device_map\u001b[39m\u001b[34m(model, device_map, max_memory, hf_quantizer, dtype, keep_in_fp32_regex)\u001b[39m\n\u001b[32m   1330\u001b[39m     logger.warning(\n\u001b[32m   1331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis model has some weights that should be kept in higher precision, you need to upgrade \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`accelerate` to properly deal with them (`pip install --upgrade accelerate`).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1333\u001b[39m     )\n\u001b[32m   1335\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_map != \u001b[33m\"\u001b[39m\u001b[33msequential\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     inferred_max_memory = \u001b[43mget_balanced_memory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1338\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlow_zero\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbalanced_low_0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1340\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1341\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdevice_map_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1342\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1344\u001b[39m     inferred_max_memory = get_max_memory(max_memory)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:956\u001b[39m, in \u001b[36mget_balanced_memory\u001b[39m\u001b[34m(model, max_memory, no_split_module_classes, dtype, special_dtypes, low_zero)\u001b[39m\n\u001b[32m    954\u001b[39m \u001b[38;5;66;03m# Get default / clean up max_memory\u001b[39;00m\n\u001b[32m    955\u001b[39m user_not_set_max_memory = max_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m956\u001b[39m max_memory = \u001b[43mget_max_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_npu_available():\n\u001b[32m    959\u001b[39m     expected_device_type = \u001b[33m\"\u001b[39m\u001b[33mnpu\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:847\u001b[39m, in \u001b[36mget_max_memory\u001b[39m\u001b[34m(max_memory)\u001b[39m\n\u001b[32m    845\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m max_memory.keys():\n\u001b[32m    846\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_devices:\n\u001b[32m--> \u001b[39m\u001b[32m847\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    848\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDevice \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not recognized, available devices are integers(for GPU/XPU), \u001b[39m\u001b[33m'\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    849\u001b[39m         )\n\u001b[32m    850\u001b[39m max_memory = {k: max_memory[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m all_devices}\n\u001b[32m    852\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m max_memory\n",
      "\u001b[31mValueError\u001b[39m: Device None is not recognized, available devices are integers(for GPU/XPU), 'mps', 'cpu' and 'disk'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOgn1zKHvI8J"
   },
   "source": [
    "### discussion run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HiXKZ2hQ1iRX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-21 13:04:28,090 - INFO - Batch start: 10/1185 rows | text_col='transcript' | traces=True | checkpoints=on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-21 13:04:28,092 - INFO - == Starting MultiAgentDiscussion with 3 agents\n",
      "2026-01-21 13:04:28,092 - INFO - Round 1/3\n",
      "2026-01-21 13:04:29,494 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2026-01-21 13:04:29,502 - INFO - Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:\n",
      "  - 0: 1866989568 bytes required\n",
      "These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.\n",
      "2026-01-21 13:07:23,629 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2026-01-21 13:07:23,639 - INFO - Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:\n",
      "  - 0: 1866989568 bytes required\n",
      "These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.\n",
      "2026-01-21 13:07:30,761 - WARNING - Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/10 [04:26<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m batch_num = BATCH_NUM \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m2\u001b[39m\n\u001b[32m      5\u001b[39m output_file_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoday\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-batch_0-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_results\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m res, new_df = \u001b[43mmad\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_batch_discussions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_2022\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                                          \u001b[49m\u001b[43mtext_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtranscript\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m                                          \u001b[49m\u001b[43mbatch_num\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m                                          \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m                                          \u001b[49m\u001b[43msave_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m                                          \u001b[49m\u001b[43mstop_on_error\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m                                          \u001b[49m\u001b[43mstore_traces\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m                                          \u001b[49m\u001b[43msave_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_name\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m                                          \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 422\u001b[39m, in \u001b[36mMultiAgentDiscussion.run_batch_discussions\u001b[39m\u001b[34m(self, data_df, text_col, store_traces, batch_num, log_every, save_name, save_every, stop_on_error, **kwargs)\u001b[39m\n\u001b[32m    420\u001b[39m human_code = \u001b[38;5;28mstr\u001b[39m(data_df.loc[idx, \u001b[33m\"\u001b[39m\u001b[33mCAD\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCAD\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data_df.columns \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdiscuss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhuman_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhuman_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Pass human_code here\u001b[39;00m\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m store_traces:\n\u001b[32m    425\u001b[39m         res = DiscussionResult(\n\u001b[32m    426\u001b[39m             text_to_code=res.text_to_code,\n\u001b[32m    427\u001b[39m             human_code=human_code,\n\u001b[32m   (...)\u001b[39m\u001b[32m    434\u001b[39m             num_rounds=res.num_rounds,\n\u001b[32m    435\u001b[39m         )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 288\u001b[39m, in \u001b[36mMultiAgentDiscussion.discuss\u001b[39m\u001b[34m(self, text, human_code, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agents:\n\u001b[32m    286\u001b[39m   max_retries = \u001b[38;5;28mself\u001b[39m.config.max_retries_per_agent \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.max_retries_per_agent \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m   raw = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43massign_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m   \u001b[38;5;66;03m# Parse and validate response using the agent's validate_and_parse method\u001b[39;00m\n\u001b[32m    290\u001b[39m   round_responses.append(\u001b[38;5;28mself\u001b[39m._to_agent_response(agent, raw, \u001b[38;5;28mself\u001b[39m.round_num))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 189\u001b[39m, in \u001b[36mSingleAgentCoding.assign_code\u001b[39m\u001b[34m(self, text, max_retries, extra_context, **gen_opts)\u001b[39m\n\u001b[32m    187\u001b[39m logging.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAssigning code for text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m# Log the text being processed\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# Call the chat method with a specific role for assigning codes\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrole\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrole\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgen_opts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m logging.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRaw response from agent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m# Log the raw response from the agent\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mBaseCodingAgent.chat\u001b[39m\u001b[34m(self, text, max_retries, role, extra_context, **gen_opts)\u001b[39m\n\u001b[32m     61\u001b[39m     logging.info(prompt_str)\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Generate response\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_and_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgen_opts\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mBaseCodingAgent._call_and_retry\u001b[39m\u001b[34m(self, prompt_str, max_retries, **gen_opts)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Internal method to handle generation with retries.\"\"\"\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     raw = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgen_opts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     valid, parsed, err = \u001b[38;5;28mself\u001b[39m.validator.validate_and_parse(raw)\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m valid:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 109\u001b[39m, in \u001b[36mModelManager.generate\u001b[39m\u001b[34m(self, prompt, temperature, top_k)\u001b[39m\n\u001b[32m    106\u001b[39m logging.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Calling model with these inputs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_NEW_TOKENS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m logging.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel parameters: temperature=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, top_k=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, max_new_tokens=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_NEW_TOKENS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    119\u001b[39m logging.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel raw output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2566\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2563\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2565\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2566\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2576\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2578\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2579\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2580\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2581\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2789\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2787\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2788\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2789\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2791\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2792\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2793\u001b[39m     outputs,\n\u001b[32m   2794\u001b[39m     model_kwargs,\n\u001b[32m   2795\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2796\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:449\u001b[39m, in \u001b[36mQwen2ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    418\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    430\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    431\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    432\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    433\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    434\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    447\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    448\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    461\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:1072\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1069\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1074\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1075\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1076\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1077\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:384\u001b[39m, in \u001b[36mQwen2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    381\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    397\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    398\u001b[39m     past_key_values=past_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    399\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:249\u001b[39m, in \u001b[36mQwen2DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    247\u001b[39m residual = hidden_states\n\u001b[32m    248\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:46\u001b[39m, in \u001b[36mQwen2MLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     down_proj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\notin\\Bahar\\llm\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "batch_num = BATCH_NUM or 2\n",
    "output_file_name = f\"{today}-batch_0-{batch_num}_results\"\n",
    "\n",
    "res, new_df = mad.run_batch_discussions(df_2022,\n",
    "                                          text_col=\"transcript\",\n",
    "                                          batch_num= batch_num,\n",
    "                                          log_every=10,\n",
    "                                          save_every=20,\n",
    "                                          stop_on_error=False,\n",
    "                                          store_traces = True,\n",
    "                                          save_name=output_file_name\n",
    "                                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJyh7R3fUHyG"
   },
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6PvapbyTv4i"
   },
   "source": [
    "### Downlaod the output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1768978027951,
     "user": {
      "displayName": "Bahar Shahrokhian",
      "userId": "02701353148338583964"
     },
     "user_tz": 420
    },
    "id": "XoWVBIEZTfoY",
    "outputId": "dea7a4b9-4845-4ed9-9880-cc6f26222e79"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "download(\"download_11cd2a56-f1b9-4a2c-9e58-c85e4028ce4e\", \"discussion_html.zip\", 4384)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded discussion_html.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "#  the output FILEs\n",
    "new_df.to_parquet(f\"{output_file_name}.parquet\")\n",
    "new_df.to_csv(f\"{output_file_name}.csv\")\n",
    "\n",
    "\n",
    "files.download(f\"{output_file_name}.parquet\")\n",
    "files.download(f\"{output_file_name}.csv\")\n",
    "\n",
    "# DOWNLOAD discussion_html folder\n",
    "folder_to_download = \"discussion_html\"\n",
    "zip_filename = f\"{folder_to_download}.zip\"\n",
    "\n",
    "# Create a zip archive of the folder\n",
    "shutil.make_archive(folder_to_download, 'zip', folder_to_download)\n",
    "\n",
    "# Download the zip file\n",
    "if os.path.exists(zip_filename):\n",
    "    files.download(zip_filename)\n",
    "    print(f\"Downloaded {zip_filename}\")\n",
    "else:\n",
    "    print(f\"Error: {zip_filename} not created or found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cb6hVobhjxrB"
   },
   "source": [
    "### Write agents infos to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1768977250217,
     "user": {
      "displayName": "Bahar Shahrokhian",
      "userId": "02701353148338583964"
     },
     "user_tz": 420
    },
    "id": "KqzIlWk8i-Yw",
    "outputId": "1e6262b1-7a61-471f-a126-0606bcc3c9f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent information saved to agent_infos.json\n",
      "Agent info HTML saved to agent_infos.html\n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "import json\n",
    "import json\n",
    "\n",
    "# Collect agent info\n",
    "all_agent_info = {}\n",
    "for agent_id, agent in agents.items():\n",
    "    all_agent_info[agent_id] = agent.get_agent_info()\n",
    "\n",
    "# Save to file\n",
    "output_file_name = \"agent_infos\"\n",
    "output_file = f\"{output_file_name}.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(all_agent_info, f, indent=2)\n",
    "\n",
    "print(f\"Agent information saved to {output_file}\")\n",
    "\n",
    "def write_agent_info_to_html(agent_info_dict, filename=\"agent_infos.html\"):\n",
    "    \"\"\"Writes agent configuration dictionary to a styled HTML file.\"\"\"\n",
    "    html_content = [\n",
    "        \"<!doctype html>\",\n",
    "        \"<html><head><meta charset='utf-8'><title>Agent Information</title>\",\n",
    "        \"<style>\",\n",
    "        \"body{font-family:system-ui,-apple-system,sans-serif;margin:20px;line-height:1.5;color:#333;background:#f4f4f9;}\",\n",
    "        \".agent-card{border:1px solid #ddd;margin-bottom:20px;padding:25px;border-radius:10px;box-shadow:0 4px 6px rgba(0,0,0,0.05);background:#fff;}\",\n",
    "        \"h1{color:#2c3e50;text-align:center;margin-bottom:30px;}\",\n",
    "        \"h2{margin-top:0;color:#34495e;border-bottom:2px solid #f0f0f0;padding-bottom:10px;}\",\n",
    "        \".property{margin:10px 0;}\",\n",
    "        \".label{font-weight:600;color:#555;display:inline-block;width:140px;vertical-align:top;}\",\n",
    "        \".value{display:inline-block;width:calc(100% - 150px);}\",\n",
    "        \"pre{background:#f8f9fa;padding:10px;border-radius:4px;overflow-x:auto;margin:0;}\",\n",
    "        \"ul{margin:0;padding-left:20px;}\",\n",
    "        \"</style></head><body>\",\n",
    "        \"<h1>Agent Configuration</h1>\"\n",
    "    ]\n",
    "\n",
    "    for agent_id, info in agent_info_dict.items():\n",
    "        html_content.append(f\"<div class='agent-card'>\")\n",
    "        name = info.get('name', agent_id)\n",
    "        html_content.append(f\"<h2>Agent: {html.escape(str(name))} <small style='color:#777;font-weight:normal'>({html.escape(str(agent_id))})</small></h2>\")\n",
    "\n",
    "        for key, value in info.items():\n",
    "            val_html = \"\"\n",
    "            if key == 'codebook' and isinstance(value, dict):\n",
    "                val_html = \"<ul>\" + \"\".join([f\"<li><b>{html.escape(str(k))}:</b> {html.escape(str(v))}</li>\" for k,v in value.items()]) + \"</ul>\"\n",
    "            elif isinstance(value, (dict, list)):\n",
    "                 val_html = f\"<pre>{html.escape(json.dumps(value, indent=2))}</pre>\"\n",
    "            else:\n",
    "                val_html = html.escape(str(value))\n",
    "\n",
    "            html_content.append(f\"<div class='property'><span class='label'>{html.escape(str(key))}:</span><span class='value'>{val_html}</span></div>\")\n",
    "\n",
    "        html_content.append(\"</div>\")\n",
    "\n",
    "    html_content.append(\"</body></html>\")\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(html_content))\n",
    "    print(f\"Agent info HTML saved to {filename}\")\n",
    "\n",
    "# # Execute using the dictionary from the previous step\n",
    "# if 'all_agent_info' in locals():\n",
    "#     write_agent_info_to_html(all_agent_info)\n",
    "# else:\n",
    "#     # Fallback to regenerate info if variable missing\n",
    "#     temp_info = {aid: ag.get_agent_info() for aid, ag in agents.items()}\n",
    "#     write_agent_info_to_html(temp_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pxe-ixjcyzh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7CJXjXSkPOU"
   },
   "source": [
    "# Test (don't run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tw9KICXj3n8"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "slvtfG3DRrWV"
   },
   "outputs": [],
   "source": [
    "\n",
    "# read parquet\n",
    "df = pd.read_parquet(f\"{output_file_name}.parquet\")\n",
    "df.head()\n",
    "df.to_csv(f\"{output_file_name}.csv\")\n",
    "\n",
    "#  Convert this to list\n",
    "for roud_resp in df[\"round_dicts\"][0]:\n",
    "  print(type(roud_resp))\n",
    "\n",
    "\n",
    "for result in res:\n",
    "\n",
    "\n",
    "  a = result.__dict__()\n",
    "  # print(type(a))\n",
    "  for round in a.get(\"round_dicts\"):\n",
    "    print(f\"round #{round[\"round_num\"]}\")\n",
    "    for agent in round.get(\"responses\"):\n",
    "      print(f\"agent: {agent['agent']}, coded: {agent['code']} with rationale: {agent['rationale']}\")\n",
    "      print(f\"======raw: {agent['raw']}\")\n",
    "\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xUfdBN9NGGaE"
   },
   "outputs": [],
   "source": [
    "def convert_agent_response(agent_resp: str, print_raw = False) -> AgentResponse:\n",
    "  name = agent_resp.split(\"agent=\")[1].split(\",\")[0]\n",
    "  code = agent_resp.split(\"code=\")[1].split(\",\")[0]\n",
    "  rationale = agent_resp.split(\"rationale=\")[1].split(\", raw=\")[0]\n",
    "  round = agent_resp.split(\"round=\")[1].split(\")\")[0]\n",
    "  raw = agent_resp.split(\"raw=\")[1].split(\"round=\")[0]\n",
    "\n",
    "  print(f\" At round {1} - agent {name}, coded this text as {code}, with rational that {rationale}\")\n",
    "  if print_raw:\n",
    "    print(f\"raw agent output was: {raw}\")\n",
    "  agent_resp_obj = AgentResponse(agent=name, code=code, rationale=rationale, raw=raw, round=int(round))\n",
    "  return agent_resp_obj\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# text = df['transcript'][0]\n",
    "# # print(text)\n",
    "# agents_text = df['raw'][0]\n",
    "# discussion_results = agents_text.replace(\"'\", '\"')\n",
    "# # print(discussion_res)\n",
    "# # convert agents_text str to list\n",
    "# b = discussion_results.split(\"], \")\n",
    "# print(b[0].split(\"round=\")[0])\n",
    "# # print(b[1])?\n",
    "# # print(b[2])\n",
    "\n",
    "# # agents_res_list = agents_text.split(\"[AgentResponse(\")\n",
    "# # agents_res_valid = []\n",
    "# # for agent_res in agents_res_list:\n",
    "# #   if \"agent=\" in agent_res:\n",
    "# #     agents_res_valid.append(agent_res)\n",
    "\n",
    "# # # agent_text\n",
    "# # len(agents_res_valid)\n",
    "# # for agent_text in agents_res_valid:\n",
    "# #   print(agent_text)\n",
    "# #   agent_resp_obj = convert_agent_response(agent_text)\n",
    "#   # print(agent_resp_obj)\n",
    "# # # convert str to list\n",
    "# # agent_text = agent_text.replace(\"'\", '\"')\n",
    "# # ares = convert_agent_response(agent_text)\n",
    "# # ares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DanRVWNZj5TV"
   },
   "source": [
    "# **NEW CODE -1/20/2026**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_O71RUFkBht"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# ============================================================\n",
    "# MAD (3 agents x 3 rounds) for CAD coding on MacBook\n",
    "# TIGHT JSON FORCING VERSION:\n",
    "# - Balanced-brace JSON extraction (most reliable)\n",
    "# - Retry-on-invalid JSON (up to 3 tries per agent/round)\n",
    "# - Deterministic retry (temperature=0) + stronger constraints\n",
    "# - Decode ONLY new tokens (prevents prompt echo)\n",
    "# - Normalizes human CAD for accurate agreement\n",
    "# - Saves Excel + HTML to Desktop\n",
    "# ============================================================\n",
    "\n",
    "import os, re, json, time, gc, html\n",
    "from collections import Counter\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# -------------------------\n",
    "# USER SETTINGS\n",
    "# -------------------------\n",
    "FILE_PATH   = \"/Users/elahetajik/Desktop/data -ASU.xlsx\"\n",
    "SHEET_NAME  = None\n",
    "YEAR_FILTER = 2022\n",
    "K           = 200\n",
    "\n",
    "OUT_XLSX = \"/Users/elahetajik/Desktop/mad_outputs_2022_K200.xlsx\"\n",
    "OUT_HTML = \"/Users/elahetajik/Desktop/mad_report_2022_K200.html\"\n",
    "\n",
    "MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "# Generation controls\n",
    "TEMP_ROUND1    = 0.3\n",
    "TEMP_DISCUSS   = 0.0\n",
    "MAX_NEW_TOKENS = 160\n",
    "\n",
    "# JSON retry controls\n",
    "JSON_RETRIES = 3  # attempts per agent per round\n",
    "\n",
    "# -------------------------\n",
    "# CAD CODEBOOK\n",
    "# -------------------------\n",
    "CAD_CODEBOOK_DICT = {\n",
    "    \"WCT\":   \"Teacher is addressing the whole class.\",\n",
    "    \"GT\":    \"Teacher is addressing a group or a student in a group (student-level talk).\",\n",
    "    \"Other\": \"Teacher is not addressing the whole class or groups/students (silent/self talk/visitor/tech).\",\n",
    "    \"NONE\":  \"Ambiguous / cannot determine.\"\n",
    "}\n",
    "ALLOWED_CODES = {\"WCT\", \"GT\", \"Other\", \"NONE\"}\n",
    "\n",
    "# -------------------------\n",
    "# DEVICE\n",
    "# -------------------------\n",
    "device = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"✅ Device:\", device)\n",
    "\n",
    "# -------------------------\n",
    "# LOAD DATA\n",
    "# -------------------------\n",
    "if not os.path.exists(FILE_PATH):\n",
    "    raise FileNotFoundError(f\"Excel not found: {FILE_PATH}\")\n",
    "\n",
    "df = pd.read_excel(FILE_PATH) if SHEET_NAME is None else pd.read_excel(FILE_PATH, sheet_name=SHEET_NAME)\n",
    "print(\"✅ Loaded rows:\", len(df))\n",
    "print(\"✅ Columns:\", df.columns.tolist())\n",
    "\n",
    "if \"transcript\" not in df.columns:\n",
    "    raise ValueError(\"❌ Your Excel must have a column named exactly: transcript\")\n",
    "\n",
    "df_work = df.copy()\n",
    "\n",
    "if \"year\" in df_work.columns:\n",
    "    df_work = df_work[df_work[\"year\"] == YEAR_FILTER].copy()\n",
    "    print(f\"✅ After year=={YEAR_FILTER} filter:\", len(df_work))\n",
    "else:\n",
    "    print(\"⚠️ No 'year' column found; skipping year filter.\")\n",
    "\n",
    "df_work[\"transcript\"] = df_work[\"transcript\"].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "# -------------------------\n",
    "# NORMALIZE HUMAN CAD\n",
    "# -------------------------\n",
    "def normalize_human_code(x: Any) -> str:\n",
    "    s = \"\" if pd.isna(x) else str(x)\n",
    "    s = s.strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    sl = s.lower()\n",
    "\n",
    "    # exact / common\n",
    "    if sl in {\"wct\", \"whole class\", \"whole-class\", \"wholeclass\"}:\n",
    "        return \"WCT\"\n",
    "    if sl in {\"gt\", \"group\", \"small group\", \"partner\"}:\n",
    "        return \"GT\"\n",
    "    if sl in {\"other\", \"oth\"}:\n",
    "        return \"Other\"\n",
    "    if sl in {\"none\", \"na\", \"n/a\", \"ambiguous\", \"unclear\"}:\n",
    "        return \"NONE\"\n",
    "\n",
    "    # containment for messy labels\n",
    "    if \"wct\" in sl or \"whole\" in sl:\n",
    "        return \"WCT\"\n",
    "    if re.search(r\"\\bgt\\b\", sl) or \"group\" in sl or \"partner\" in sl:\n",
    "        return \"GT\"\n",
    "    if \"other\" in sl:\n",
    "        return \"Other\"\n",
    "    if \"none\" in sl or \"ambig\" in sl or \"unclear\" in sl:\n",
    "        return \"NONE\"\n",
    "\n",
    "    return s\n",
    "\n",
    "if \"CAD\" in df_work.columns:\n",
    "    df_work[\"CAD\"] = df_work[\"CAD\"].fillna(\"\").astype(str)\n",
    "    df_work[\"CAD_norm\"] = df_work[\"CAD\"].apply(normalize_human_code)\n",
    "else:\n",
    "    df_work[\"CAD_norm\"] = \"\"\n",
    "\n",
    "df_work = df_work.head(K).copy()\n",
    "print(\"✅ Running K =\", len(df_work))\n",
    "\n",
    "# -------------------------\n",
    "# LOAD MODEL\n",
    "# -------------------------\n",
    "print(\"⏳ Loading model:\", MODEL_ID)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "\n",
    "torch_dtype = torch.float16 if device in {\"cuda\", \"mps\"} else torch.float32\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device_map=None\n",
    ").to(device)\n",
    "\n",
    "model.eval()\n",
    "print(\"✅ Model loaded.\")\n",
    "\n",
    "# -------------------------\n",
    "# JSON EXTRACTION (brace-balanced)\n",
    "# -------------------------\n",
    "def extract_first_balanced_json(text: str) -> Optional[dict]:\n",
    "    \"\"\"\n",
    "    Finds the FIRST balanced {...} object in the text by counting braces.\n",
    "    Much more reliable than regex for messy outputs.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    s = text.strip()\n",
    "    # remove fences if any\n",
    "    s = re.sub(r\"^```(?:json)?\\s*\", \"\", s).strip()\n",
    "    s = re.sub(r\"\\s*```$\", \"\", s).strip()\n",
    "\n",
    "    start = s.find(\"{\")\n",
    "    if start == -1:\n",
    "        return None\n",
    "\n",
    "    depth = 0\n",
    "    in_str = False\n",
    "    escape = False\n",
    "\n",
    "    for i in range(start, len(s)):\n",
    "        ch = s[i]\n",
    "\n",
    "        if in_str:\n",
    "            if escape:\n",
    "                escape = False\n",
    "            elif ch == \"\\\\\":\n",
    "                escape = True\n",
    "            elif ch == '\"':\n",
    "                in_str = False\n",
    "            continue\n",
    "\n",
    "        # not in string\n",
    "        if ch == '\"':\n",
    "            in_str = True\n",
    "            continue\n",
    "        if ch == \"{\":\n",
    "            depth += 1\n",
    "        elif ch == \"}\":\n",
    "            depth -= 1\n",
    "            if depth == 0:\n",
    "                cand = s[start:i+1].strip()\n",
    "                try:\n",
    "                    obj = json.loads(cand)\n",
    "                    return obj if isinstance(obj, dict) else None\n",
    "                except Exception:\n",
    "                    return None\n",
    "\n",
    "    return None\n",
    "\n",
    "def normalize_model_json(d: dict) -> dict:\n",
    "    code = (d.get(\"CAD-code\") or d.get(\"code\") or d.get(\"CAD\") or \"\").strip()\n",
    "    rationale = (d.get(\"rationale\") or d.get(\"reasoning\") or \"\").strip()\n",
    "\n",
    "    if code not in ALLOWED_CODES:\n",
    "        code = \"NONE\"\n",
    "\n",
    "    low = rationale.lower()\n",
    "    if (not rationale) or (\"evidence-based\" in low and \"sentences\" in low):\n",
    "        rationale = \"Rationale missing/placeholder.\"\n",
    "\n",
    "    if len(rationale) > 350:\n",
    "        rationale = rationale[:350] + \"...\"\n",
    "\n",
    "    return {\"CAD-code\": code, \"rationale\": rationale}\n",
    "\n",
    "# -------------------------\n",
    "# PROMPTS\n",
    "# -------------------------\n",
    "def agent_system(name: str, personality: str) -> str:\n",
    "    return (\n",
    "        f\"You are {name}, a {personality} qualitative-coding agent.\\n\"\n",
    "        \"Task: Assign ONE CAD code for the teacher transcript using the codebook.\\n\"\n",
    "        \"STRICT OUTPUT RULE:\\n\"\n",
    "        \"- Output ONLY a JSON object. No extra text. No markdown. No thinking.\\n\"\n",
    "        'JSON schema: {\"CAD-code\":\"WCT|GT|Other|NONE\",\"rationale\":\"1-3 short evidence-based sentences\"}\\n'\n",
    "        \"Heuristics:\\n\"\n",
    "        \"- WCT: whole-class address/instructions.\\n\"\n",
    "        \"- GT: talk to a small group/partner(s).\\n\"\n",
    "        \"- Other: teacher tech/self/visitor talk not directing students.\\n\"\n",
    "        \"- NONE: only if truly impossible to infer.\\n\"\n",
    "    )\n",
    "\n",
    "def agent_user(transcript: str, extra_context: str) -> str:\n",
    "    cb = \"\\n\".join([f\"- {k}: {v}\" for k, v in CAD_CODEBOOK_DICT.items()])\n",
    "    return (\n",
    "        f\"Codebook:\\n{cb}\\n\\n\"\n",
    "        f\"{extra_context}\\n\\n\"\n",
    "        f'Text to code:\\n\"\"\"{transcript}\"\"\"\\n\\n'\n",
    "        \"Return ONLY JSON now.\"\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# GENERATION (decode only NEW tokens)\n",
    "# -------------------------\n",
    "@torch.no_grad()\n",
    "def generate_once(system_prompt: str, user_prompt: str, temperature: float, max_new_tokens: int) -> str:\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    else:\n",
    "        prompt = system_prompt + \"\\n\\n\" + user_prompt\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=int(max_new_tokens),\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=(temperature > 0.0),\n",
    "    )\n",
    "    if temperature > 0.0:\n",
    "        gen_kwargs[\"temperature\"] = float(temperature)\n",
    "\n",
    "    out = model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "    new_tokens = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    text = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "    return text\n",
    "\n",
    "def generate_json_with_retries(system_prompt: str, user_prompt: str,\n",
    "                              temperature: float, max_new_tokens: int,\n",
    "                              retries: int = JSON_RETRIES) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Try multiple times to get valid JSON.\n",
    "    - attempt 1: normal (given temperature)\n",
    "    - attempt 2+: temperature=0 + stronger forcing message appended\n",
    "    Returns dict: {\"raw\":..., \"parsed\": dict|None}\n",
    "    \"\"\"\n",
    "    last_raw = \"\"\n",
    "\n",
    "    for attempt in range(1, retries + 1):\n",
    "        if attempt == 1:\n",
    "            temp = temperature\n",
    "            user = user_prompt\n",
    "        else:\n",
    "            temp = 0.0\n",
    "            # strong forcing on retries\n",
    "            user = (\n",
    "                user_prompt\n",
    "                + \"\\n\\nCRITICAL: Your entire output MUST be ONLY the JSON object.\"\n",
    "                + \"\\nIt MUST BEGIN with '{' and END with '}'. No other characters.\"\n",
    "            )\n",
    "\n",
    "        raw = generate_once(system_prompt, user, temperature=temp, max_new_tokens=max_new_tokens)\n",
    "        last_raw = raw\n",
    "\n",
    "        # quick trim: if there's a '{', start there (helps)\n",
    "        if \"{\" in raw:\n",
    "            raw_trim = raw[raw.find(\"{\"):].strip()\n",
    "        else:\n",
    "            raw_trim = raw.strip()\n",
    "\n",
    "        parsed = extract_first_balanced_json(raw_trim)\n",
    "        if parsed and isinstance(parsed, dict):\n",
    "            return {\"raw\": raw_trim, \"parsed\": parsed}\n",
    "\n",
    "    return {\"raw\": last_raw, \"parsed\": None}\n",
    "\n",
    "# -------------------------\n",
    "# MAD\n",
    "# -------------------------\n",
    "AGENTS = [\n",
    "    (\"Ava\", \"balanced (neutral perspective)\"),\n",
    "    (\"Ben\", \"rigorous and concise (strict evidence grounding)\"),\n",
    "    (\"Cam\", \"creative and empathic (attentive to contextual nuance)\")\n",
    "]\n",
    "\n",
    "def summarize_peers(prev: List[dict]) -> str:\n",
    "    lines = [\"Previous round peer responses:\"]\n",
    "    for p in prev:\n",
    "        lines.append(f'- Code: {p[\"CAD-code\"]} | Rationale: {p[\"rationale\"]}')\n",
    "    lines.append(\"Revise if needed, but remain evidence-based. Output ONLY JSON.\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def run_mad_for_text(text: str, rounds_total: int = 3) -> dict:\n",
    "    history: List[List[dict]] = []\n",
    "\n",
    "    # Round 1\n",
    "    r1 = []\n",
    "    for (name, pers) in AGENTS:\n",
    "        sys_p = agent_system(name, pers)\n",
    "        usr_p = agent_user(text, extra_context=\"Round 1: Independent assessment.\")\n",
    "        got = generate_json_with_retries(sys_p, usr_p, temperature=TEMP_ROUND1, max_new_tokens=MAX_NEW_TOKENS)\n",
    "        parsed = got[\"parsed\"] if got[\"parsed\"] else {}\n",
    "        norm = normalize_model_json(parsed)\n",
    "        norm.update({\"agent\": name, \"raw\": got[\"raw\"]})\n",
    "        r1.append(norm)\n",
    "    history.append(r1)\n",
    "\n",
    "    current = r1\n",
    "\n",
    "    # Round 2..3\n",
    "    for r in range(2, rounds_total + 1):\n",
    "        nxt = []\n",
    "        for i, (name, pers) in enumerate(AGENTS):\n",
    "            peers = [c for j, c in enumerate(current) if j != i]\n",
    "            ctx = f\"Round {r}: Revise after seeing peers.\\n\" + summarize_peers(peers)\n",
    "            sys_p = agent_system(name, pers)\n",
    "            usr_p = agent_user(text, extra_context=ctx)\n",
    "            got = generate_json_with_retries(sys_p, usr_p, temperature=TEMP_DISCUSS, max_new_tokens=MAX_NEW_TOKENS)\n",
    "            parsed = got[\"parsed\"] if got[\"parsed\"] else {}\n",
    "            norm = normalize_model_json(parsed)\n",
    "            norm.update({\"agent\": name, \"raw\": got[\"raw\"]})\n",
    "            nxt.append(norm)\n",
    "        history.append(nxt)\n",
    "        current = nxt\n",
    "\n",
    "    final_codes = [a[\"CAD-code\"] for a in history[-1]]\n",
    "    counts = Counter([c for c in final_codes if c in ALLOWED_CODES])\n",
    "    label, freq = counts.most_common(1)[0] if counts else (\"NONE\", 0)\n",
    "    conf = freq / len(final_codes) if final_codes else 0.0\n",
    "\n",
    "    return {\"history\": history, \"majority\": label, \"conf\": conf}\n",
    "\n",
    "# -------------------------\n",
    "# RUN + SAVE\n",
    "# -------------------------\n",
    "rows_out = []\n",
    "html_blocks = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for n, (idx, row) in enumerate(df_work.iterrows(), start=1):\n",
    "    transcript = row[\"transcript\"]\n",
    "    human = row[\"CAD_norm\"] if \"CAD_norm\" in df_work.columns else \"\"\n",
    "\n",
    "    res = run_mad_for_text(transcript, rounds_total=3)\n",
    "    hist = res[\"history\"]\n",
    "    maj = res[\"majority\"]\n",
    "    conf = res[\"conf\"]\n",
    "\n",
    "    agree = int(human == maj) if human in ALLOWED_CODES else 0\n",
    "\n",
    "    outrow = dict(row)\n",
    "    outrow[\"mad_final_code\"] = maj\n",
    "    outrow[\"mad_final_conf\"] = float(conf)\n",
    "    outrow[\"agree_with_human_CAD\"] = agree\n",
    "\n",
    "    # Flatten per-agent per-round columns\n",
    "    for r_i, round_list in enumerate(hist, start=1):\n",
    "        for a in round_list:\n",
    "            agent = a[\"agent\"]\n",
    "            outrow[f\"R{r_i}_{agent}_code\"] = a[\"CAD-code\"]\n",
    "            outrow[f\"R{r_i}_{agent}_rationale\"] = a[\"rationale\"]\n",
    "            outrow[f\"R{r_i}_{agent}_raw\"] = a[\"raw\"]\n",
    "\n",
    "    # Final rationale = rationales of majority voters in final round\n",
    "    final_round = hist[-1]\n",
    "    rats = [a[\"rationale\"] for a in final_round if a[\"CAD-code\"] == maj and a[\"rationale\"]]\n",
    "    outrow[\"mad_final_rationale\"] = \" | \".join(rats) if rats else \"\"\n",
    "\n",
    "    rows_out.append(outrow)\n",
    "\n",
    "    # HTML report block\n",
    "    badge = \"✅\" if agree == 1 else \"❌\"\n",
    "    esc_t = html.escape(transcript)\n",
    "    esc_h = html.escape(human)\n",
    "    esc_m = html.escape(maj)\n",
    "\n",
    "    block = []\n",
    "    block.append(f\"<h2>{badge} Row {idx} | Final: {esc_m} (conf={conf:.2f}) | Human: {esc_h} | Agree={agree}</h2>\")\n",
    "    block.append(f\"<p><b>Transcript:</b> {esc_t}</p>\")\n",
    "\n",
    "    for r_i, round_list in enumerate(hist, start=1):\n",
    "        block.append(f\"<h3>Round {r_i}</h3>\")\n",
    "        block.append(\"<table border='1' cellpadding='6' cellspacing='0' style='border-collapse:collapse;width:100%'>\")\n",
    "        block.append(\"<tr><th>Agent</th><th>Code</th><th>Rationale</th><th>Raw</th></tr>\")\n",
    "        for a in round_list:\n",
    "            raw = html.escape(a[\"raw\"] or \"\")\n",
    "            rat = html.escape(a[\"rationale\"] or \"\")\n",
    "            code = html.escape(a[\"CAD-code\"] or \"\")\n",
    "            agent = html.escape(a[\"agent\"] or \"\")\n",
    "            block.append(\n",
    "                f\"<tr><td><b>{agent}</b></td><td><b>{code}</b></td><td>{rat}</td>\"\n",
    "                f\"<td><details><summary>show</summary><pre style='white-space:pre-wrap'>{raw}</pre></details></td></tr>\"\n",
    "            )\n",
    "        block.append(\"</table>\")\n",
    "\n",
    "    html_blocks.append(\"\\n\".join(block))\n",
    "\n",
    "    # cleanup\n",
    "    gc.collect()\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if n % 10 == 0:\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"Progress: {n}/{len(df_work)} rows | {elapsed/60:.1f} min elapsed\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"✅ Done. Processed {len(rows_out)} rows in {elapsed/60:.2f} min\")\n",
    "\n",
    "out_df = pd.DataFrame(rows_out)\n",
    "out_df.to_excel(OUT_XLSX, index=False)\n",
    "\n",
    "with open(OUT_HTML, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"<html><head><meta charset='utf-8'><title>MAD Report</title></head><body>\")\n",
    "    f.write(f\"<h1>MAD Results | YEAR={YEAR_FILTER} | K={len(df_work)} | Model={MODEL_ID}</h1>\")\n",
    "    f.write(\"<p>Report includes: transcript, human CAD, final MAD code, and all agents/rounds with raw.</p>\")\n",
    "    f.write(\"<hr>\")\n",
    "    f.write(\"\\n<hr>\\n\".join(html_blocks))\n",
    "    f.write(\"</body></html>\")\n",
    "\n",
    "print(\"✅ Saved Excel:\", OUT_XLSX)\n",
    "print(\"✅ Saved HTML :\", OUT_HTML)\n",
    "print(\"Open HTML with:\")\n",
    "print(f\"open '{OUT_HTML}'\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "cb6hVobhjxrB",
    "X7CJXjXSkPOU"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
